{
  "generation_metadata": {
    "timestamp": "2025-07-14T14:16:12.225866+00:00",
    "total_scheduled_articles": 30,
    "blog_articles_generated": 30,
    "full_content_fetched": 24,
    "failed_content_fetch": 6,
    "dry_run": false
  },
  "generated_at": "2025-07-14T14:16:23.050228+00:00",
  "total_articles": 30,
  "articles": [
    {
      "id": "8b28f2e790f90210111f77f13d241c71",
      "title": "AI 'Nudify' Websites Are Raking in Millions of Dollars",
      "url": "https://www.wired.com/story/ai-nudify-websites-are-raking-in-millions-of-dollars/",
      "source": "Wired AI",
      "published": "2025-07-14T11:00:00Z",
      "summary": "Nudify apps and websites allow people to create nonconsensual and abusive images of women and girls. Despite some lawmakers and tech companies taking steps to limit the harmful services, millions of people are still accessing the websites. Most of the sites rely on tech services from Google, Amazon, and Cloudflare to operate.",
      "full_content": "For years, so-called “nudify” apps and websites have mushroomed online, allowing people to create nonconsensual and abusive images of women and girls, including child sexual abuse material.\n\nDespite some lawmakers and tech companies taking steps to limit the harmful services, every month, millions of people are still accessing the websites, and the sites’ creators may be making millions of dollars each year, new research suggests.An analysis of 85 nudify and “undress” websites—which allow people to upload photos and use AI to generate “nude” pictures of the subjects with just a few clicks—has found that most of the sites rely on tech services from Google, Amazon, and Cloudflare to operate and stay online.\n\nThe findings, revealed by Indicator, a publication investigating digital deception, say that the websites had a combined average of 18.5 million visitors for each of the past six months and collectively may be making up to $36 million per year.Alexios Mantzarlis, a cofounder of Indicator and an online safety researcher, says the murky nudifier ecosystem has become a “lucrative business” that “Silicon Valley’s laissez-faire approach to generative AI” has allowed to persist. “They should have ceased providing any and all services to AI nudifiers when it was clear that their only use case was sexual harassment,” Mantzarlis says of tech companies.\n\nIt is increasingly becoming illegal to create or share explicit deepfakes.According to the research, Amazon and Cloudflare provide hosting or content delivery services for 62 of the 85 websites, while Google’s sign-on system has been used on 54 of the websites.\n\nThe nudify websites also use a host of other services, such as payment systems, provided by mainstream companies.Amazon Web Services spokesperson Ryan Walsh says AWS has clear terms of service that require customers to follow “applicable” laws. “When we receive reports of potential violations of our terms, we act quickly to review and take steps to disable prohibited content,” Walsh says, adding that people can report issues to its safety teams.“Some of these sites violate our terms, and our teams are taking action to address these violations, as well as working on longer-term solutions,” Google spokesperson Karl Ryan says, pointing out that Google’s sign-in system requires developers to agree to its policies that prohibit illegal content and content that harasses others.Cloudflare had not responded to WIRED’s request for comment at the time of writing.\n\nWIRED is not naming the nudifier websites in this story, as not to provide them with further exposure.Nudify and undress websites and bots have flourished since 2019, after originally spawning from the tools and processes used to create the first explicit “deepfakes.” Networks of interconnected companies, as Bellingcat has reported, have appeared online offering the technology and making money from the systems.Broadly, the services use AI to transform photos into nonconsensual explicit imagery; they often make money by selling “credits” or subscriptions that can be used to generate photos.\n\nThey have been supercharged by the wave of generative AI image generators that have appeared in the past few years. Their output is hugely damaging.\n\nSocial media photos have been stolen and used to create abusive images; meanwhile, in a new form of cyberbullying and abuse, teenage boys around the world have created images of their classmates.\n\nSuch intimate image abuse is harrowing for victims, and images can be difficult to scrub from the web.Using various open source tools and data, including website analysis tool Built With, Indicator staff and investigative researcher Santiago Lakatos looked into the infrastructure and systems powering 85 nudifier websites.\n\nContent delivery networks, hosting services, domain name companies, and webmaster services are all provided by a mixture of some of the biggest tech companies, plus some smaller businesses.Based on calculations combining subscription costs, estimated customer conversion rates, and web traffic the sites sent to payment providers, the researchers estimate that 18 of the websites made between $2.6 million and $18.4 million in the past six months, which could equate to around $36 million a year. (They note this is likely a conservative estimate, as it doesn’t incorporate all the websites and transactions that take place away from the websites, such as those on Telegram.) Recently, whistleblower and leaked data reported on by German media outlet Der Spiegel indicated one prominent website may have a multimillion-dollar budget.\n\nAnother website has claimed to have made millions.Of the 10 most-visited sites, the research says, the most visitors came from the United States—India, Brazil, Mexico, and Germany make up the rest of the top five countries where people accessed the sites.\n\nWhile search engines direct people to nudify websites, the sites have increasingly received visitors from other online sources.\n\nNudifiers have become so popular that Russian hackers have created fake malware-laced versions.\n\nOver the past year, 404 Media has reported one site making sponsored videos with adult entertainers, and the websites have also increasingly used paid affiliate and referral programs.“Our analysis of the nudifiers’ behavior strongly indicates their desire to build and entrench themselves in a niche of the adult industry,” Lakatos says. “They will likely continue to try to intermingle their operations into the adult content space, a trend that needs to be countered by mainstream tech companies and the adult industry as well.”Many of the problems of tech companies allowing nudify platforms to use their systems are well-known.\n\nFor years, tech journalists have reported on how the deepfake economy has used mainstream payment services, social media advertisements, search engine exposure, and technology from big companies to operate.\n\nYet little comprehensive action has been taken.“Since 2019, nudification apps have moved from a handful of low-quality side projects to a cottage industry of professionalized illicit businesses with millions of users,” says Henry Ajder, an expert on AI and deepfakes who first uncovered growth in the nudification ecosystem in 2020. “Only when businesses like these who facilitate nudification apps’ ‘perverse customer journey' take targeted action will we start to see meaningful progress in making these apps harder to access and profit from.”There are signs the nudify websites are updating their tactics and approaches to try to avoid any potential crackdowns or evade bans.\n\nLast year, WIRED reported on how nudify websites used single sign-on systems from Google, Apple, and Discord to allow people to quickly create accounts. Many of the developer accounts were disabled following the reporting.\n\nThe Indicator says that on 54 of the 85 websites, however, Google’s simple sign-in system is being used, and the website creators have taken steps to evade detection by Google.\n\nThey would, the report says, use an “intermediary site” to “pose as a different URL for the registration.”While tech companies and regulators have taken a glacial approach to tackling abusive deepfakes since they first emerged more than a decade ago, there has been some recent movement.\n\nSan Francisco’s city attorney has sued 16 nonconsensual-image-generation services, Microsoft has identified developers behind celebrity deepfakes, and Meta has filed a lawsuit against a company allegedly behind a nudify app that, Meta says, repeatedly posted ads on its platform.\n\nMeanwhile, the controversial Take It Down Act, which US president Donald Trump signed into law in May, has put requirements on tech companies to remove nonconsensual image abuse quickly, and the UK government is making it illegal to create explicit deepfakes.The moves may chip away at some nudifier and undress services, but more comprehensive crackdowns are needed to slow the burgeoning harmful industry.\n\nMantzarlis says that if tech companies are more proactive and stricter in enforcing their policies, nudifiers’ ability to flourish will diminish. “Yes, this stuff will migrate to less regulated corners of the internet—but let it,” Mantzarlis says. “If websites are harder to discover, access, and use, their audience and revenue will shrink.\n\nUnfortunately, this toxic gift of the generative AI era cannot be returned. But it can certainly be drastically reduced in scope.”",
      "content_available": true,
      "content_word_count": 1294,
      "tags": [
        "ai",
        "generative ai",
        "cloud",
        "research",
        "image"
      ],
      "quality_score": 1.0,
      "image_url": "https://media.wired.com/photos/68718d38325eafd2f59811a7/master/pass/security_ai_nudify_tech.gif",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:12.676170+00:00",
      "metadata": {
        "word_count": 52,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "1da0b64e57ceedcacdcee70bf5214abc",
      "title": "Join Our Livestream: Inside the AI Copyright Battles",
      "url": "https://www.wired.com/story/livestream-ai-copyright-battles/",
      "source": "Wired AI",
      "published": "2025-07-11T17:40:06Z",
      "summary": "Many lawsuits regarding generative AI’s training materials were initially filed back in 2023, with decisions just now starting to trickle out. WIRED senior writer Kate Knibbs has been following this fight for years and is ready to answer your questions. Bring all your burning questions about the AI copyright battles to our next, subscriber-only livestream scheduled for July 16.",
      "full_content": "What's going on right now with the copyright battles over artificial intelligence? Many lawsuits regarding generative AI’s training materials were initially filed back in 2023, with decisions just now starting to trickle out.\n\nWhether it’s Midjourney generating videos of Disney characters, like Wall-E brandishing a gun, or an exit interview with a top AI lawyer as he left Meta, WIRED senior writer Kate Knibbs has been following this fight for years—and she’s ready to answer your questions.Bring all your burning questions about the AI copyright battles to WIRED’s next, subscriber-only livestream scheduled for July 16 at 1 pm ET / 10 am PT, hosted by Reece Rogers with Kate Knibbs.\n\nThe event will be streamed right here.\n\nFor subscribers who are not able to join, a replay of the livestream will be available after the event.You can help us prepare by submitting any questions you have before the livestream here, or by leaving a comment below.\n\nNot a subscriber yet? Subscribe now to get access to this livestream, plus full access to WIRED.Subscribe to WIREDKate Knibbs and Reece Rogers answer your questions at our next livestream on July 16, 2025, at 1 pm ET / 10 am ET.",
      "content_available": true,
      "content_word_count": 198,
      "tags": [
        "ai",
        "artificial intelligence",
        "generative ai",
        "video"
      ],
      "quality_score": 1.0,
      "image_url": "https://media.wired.com/photos/687068007df7ab0fb03f65d2/master/pass/business_live_question_ai_copyright.jpg",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:12.745060+00:00",
      "metadata": {
        "word_count": 59,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "84ae5ea50acfe550deb0797e80d73d45",
      "title": "Microsoft and OpenAI's AGI Fight Is Bigger Than a Contract",
      "url": "https://www.wired.com/story/microsoft-and-openais-agi-fight-is-bigger-than-a-contract/",
      "source": "Wired AI",
      "published": "2025-07-11T12:11:10Z",
      "summary": "I first learned about The Clause from Microsoft CEO Satya Nadella. “Fundamentally, their long-term idea is we get to superintelligence,” he told me. He seemed almost jaunty about the possibility, leading me to wonder how seriously he took it. \"If this is the last invention of humankind, then all bets are off,’ he continued.",
      "full_content": "I first learned about The Clause from Microsoft CEO Satya Nadella.\n\nDuring an interview with him in May 2023, I asked about the deal between Microsoft and OpenAI that granted his company exclusive access to the startup’s groundbreaking AI technology. I knew the contract had set a cap on how much profit Microsoft could make from the arrangement, and I asked him what would happen if and when that point was reached.\n\nThe answer was a bit puzzling.“Fundamentally, their long-term idea is we get to superintelligence,” he told me. “If that happens, I think all bets are off, right?” He seemed almost jaunty about the possibility, leading me to wonder how seriously he took it. “If this is the last invention of humankind, then all bets are off,” he continued. “Different people will have different judgments on what that is, and when that is.”I didn’t realize how important that determination would be until a few weeks later.\n\nWorking on a feature about OpenAI, I learned that the contract basically declared that if OpenAI’s models achieved artificial general intelligence, Microsoft would no longer have access to its new models.\n\nThe terms of the contract, which otherwise would have extended until 2030, would be void.\n\nThough I wrote about it in my story, and The Clause has never really been a state secret, it didn’t generate much discussion.That’s no longer the case.\n\nThe Clause has been at the center of the increasingly frayed relationship between Microsoft and OpenAI and is under renegotiation.\n\nIt has been the subject of investigative stores by The Information, The Wall Street Journal, the Financial Times, and, yes, WIRED.But the significance of The Clause goes beyond the fates of the two companies that agreed to it.\n\nThe tenuous conditions of that contract go to the heart of a raging debate about just how world-changing—and lucrative—AGI might be if realized, and what it would mean for a profit-driven company to control a technology that makes Sauron’s Ring of Power look like a dime-store plastic doodad.\n\nIf you want to understand what’s happening in AI, pretty much everything can be explained by The Clause.Let’s dig into the details.\n\nThough the precise language hasn’t been made public, sources with knowledge of the contract confirm that The Clause has three parts, each with its own implications.There are two conditions that must be satisfied for OpenAI to deny its technology to Microsoft.\n\nFirst, the OpenAI board would determine that its new models have achieved AGI, which is defined in OpenAI’s charter as “a highly autonomous system that outperforms humans at most economically valuable work.” Fuzzy enough for you? No wonder Microsoft is worried that OpenAI will make that determination prematurely.\n\nIts only way to object to the OpenAI board’s declaration would be to sue.But that’s not all.\n\nThe OpenAI board would also be required to determine whether the new models have achieved “sufficient AGI.” This is defined as a model capable of generating profits sufficient to reward Microsoft and OpenAI’s other investors, a figure upwards of $100 billion.\n\nOpenAI doesn’t have to actually make those profits, just provide evidence that its new models will generate that bounty.\n\nUnlike the first determination, Microsoft has to agree that OpenAI meets that standard, but can’t unreasonably dispute it. (Again, in case of a dispute a court may ultimately decide.)Altman himself admitted to me in 2023 that the standards are vague. “It gives our board a lot of control to decide what happens when we get there,” he said.\n\nIn any case, if OpenAI decides it has reached sufficient AGI, it doesn’t have to share those models with Microsoft, which will be stuck with the now outdated earlier versions.\n\nIt won’t even have to use Microsoft’s cloud servers; currently Microsoft has the right of first refusal for the work.The third part of The Clause is that for the length of the contract, Microsoft can’t develop AGI on its own.\n\nThat might explain why, despite the original lovefest between the companies edging toward Hatfield and McCoy territory, Microsoft insists it is not developing its own frontier AI models, which presumably would strive for AGI-itude.Why did Microsoft agree to this? It turns out that The Clause embodies the raging divide between those who think that AGI is just a kiss away and those who think it’s a pipe dream in the near- and even mid-term.As best I can tell, OpenAI insisted on The Clause when the partnership was formalized because Altman and company are true believers.\n\nMicrosoft went along with the concept with a massive eyeroll because Nadella and his team didn’t believe that AGI was anywhere close to being realized—certainly not before 2030, when the contract expires.\n\nThey still don’t! So Microsoft felt that its lawyers sufficiently protected it in the contract language.That was then. But in 2025, you have Sam Altman saying that we could be getting to AGI this year.\n\nMeanwhile you’ve got an insane arms race where companies, notably Meta, are offering AI researchers Ohtani-esque compensation to chase superintelligence, which is basically AGI when the training wheels go off.\n\nWhat a disaster it would be for Microsoft if OpenAI withheld its wondrous new models.\n\nMicrosoft would have to start from scratch because it has not been free to work on AGI and has instead just focused on commercial products.But Microsoft now has a way to escape the headache from The Clause.\n\nOpenAI, for reasons harkening back to its board’s temporary dismissal of Altman in November 2023, is now attempting to alter its current structure (a nonprofit overseeing a for-profit company) into a somewhat different form of governance which involves the for-profit wing becoming a “public benefit corporation.” The process is geared to help OpenAI grow by removing current caps on profit and allowing investors and employees to hold equity directly.\n\nThis requires a Microsoft sign-off, thus giving it leverage to renegotiate The Clause, among other concessions.\n\nNeither party is sharing the terms of negotiation, but at least one report says that Microsoft might be asking for the total elimination of The Clause.There’s also some speculation that OpenAI might no longer care so much.\n\nAfter all, its original impulse was that AGI shouldn't be dominated by for-profit tech giants.\n\nOpenAI, valued at $300 billion, is now reorganizing as a commercial profit-maker, albeit for “public benefit.”It would be a shame, though, to bid goodbye to The Clause.\n\nEven though it is just a bunch of sentences couched in legalese, The Clause demands a useful definition of a benchmark to determine whether AI has reached the point where it truly shakes our world.\n\nIts deletion would be a premature death of at least one canary in the AI coal mine.Don't miss future subscriber-only editions of this column. Subscribe to WIRED (50% off for Plaintext readers) today.",
      "content_available": true,
      "content_word_count": 1128,
      "tags": [
        "ai",
        "news"
      ],
      "quality_score": 1.0,
      "image_url": "https://media.wired.com/photos/686feb76e55641e9724c5faa/master/pass/Plaintext-Microsoft-OpenAI-AGI-Fight-Business-2224154535.jpg",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:12.820746+00:00",
      "metadata": {
        "word_count": 54,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "22456c349a956ed17cf43dda92169446",
      "title": "Word Embeddings for Tabular Data Feature Engineering",
      "url": "https://machinelearningmastery.com/word-embeddings-for-tabular-data-feature-engineering/",
      "source": "Machine Learning Mastery",
      "published": "2025-07-11T12:00:16Z",
      "summary": "Word embeddings have revolutionized the field of natural language processing (NLP) by quantitatively capturing semantic relationships between words. It would be difficult to argue that word embeddins have not dramatically revolutionized NLP. The study was published in the Journal of Natural Language Processing (JNLP)",
      "full_content": null,
      "content_available": false,
      "content_word_count": 0,
      "tags": [
        "nlp",
        "language"
      ],
      "quality_score": 0.9,
      "image_url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-word-embeddings-tabular-data-feature-engineering.png",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:13.201055+00:00",
      "metadata": {
        "word_count": 44,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": false
      }
    },
    {
      "id": "8f21fdf959786ace2c13ee3f3543635e",
      "title": "How Video Games Became the New Battleground for Actors and AI Protections",
      "url": "https://www.wired.com/story/video-games-voice-actors-strike-over-artificial-intelligence/",
      "source": "Wired AI",
      "published": "2025-07-10T17:13:07Z",
      "summary": "A majority of SAG-AFTRA members voted to ratify a new contract for video game performers. The contract guarantees annual raises for three years, increased compensation, and guardrails designed to prevent game companies from giving their work to AI. The strike was temporarily suspended in June, pending contract ratification.",
      "full_content": "On Wednesday, members of the Screen Actors Guild–American Federation of Television and Radio Artists, or SAG-AFTRA, voted to ratify a new contract for video game performers, officially bringing an end to a nearly yearlong strike. A majority, 95 percent of members, voted in favor of the contract, which guarantees annual raises for three years, increased compensation, and guardrails designed to prevent game companies from giving their work to AI.Actors in the video game industry had been on strike for 11 months as part of a fight to secure protections against AI, a sticking point that held up negotiations for most of that time.\n\nEvery other issue in the contract, including compensation and working conditions, was already resolved months ago, says SAG-AFTRA’s national executive director and chief negotiator Duncan Crabtree-Ireland.\n\nThe strike was temporarily suspended in June, pending contract ratification.According to Sarah Elmaleh, a voice actor who also serves as a SAG-AFTRA committee chair, actors in the games industry have been wearily eyeing AI for years—even before tools like ChatGPT exploded in use. “We knew that this was the issue of most existential importance,” Elmaleh says. “This is a medium that is fundamentally digitized.”Performers’ work is crucial to game creation.\n\nActors voice characters, help make those characters look more natural by doing motion capture, and even allow companies to use their likenesses.\n\nAnd though AI is impacting industries across the board, including animation, tech, education, and others, the video game industry has begun to feel those effects acutely.As part of the contract, consent and disclosure agreements are now required when any video game maker wants to use a performer’s voice or likeness to make an AI-driven digital replica.\n\nShould performers go on strike, they are also allowed to suspend their approval for companies to generate any new material with AI.AI is already starting to replace flesh-and-blood actors, even in high-profile cases.\n\nIn May, Fortnite introduced a generative AI version of Star Wars’ Darth Vader. (Players disastrously had him saying swears and slurs in only a few hours.\n\nFortnite maker Epic Games pushed a hotfix soon thereafter.) A few days later, SAG-AFTRA filed an unfair labor practice charge with the National Labor Relations Board against Epic subsidiary Llama Productions.\n\nIn a statement posted to SAG-AFTRA’s website, the organization said replacing a human worker with AI was done “without providing any notice of their intent to do this and without bargaining with us over appropriate terms.”Darth Vader actor James Earl Jones gave permission to have his voice digitally recreated with AI before his death in 2024.\n\nCrabtree-Ireland would not comment on specific performers or contracts.\n\nHowever, he says that protections need to be applied consistently and with a “reasonably specific” description of how their image or voice will be used. “These provisions ensure that a deceased artist’s image, voice, and performance are treated with the same respect as a living artist’s,” Crabtree-Ireland says.The companies that make video games, says Crabtree-Ireland, are on the cutting edge of AI technology. “It was really important for us to draw this line,” he says, “because we knew that the boundaries were going to be tested in the video game space earlier and more vigorously than they are in almost any other.” Companies SAG-AFTRA has been bargaining with include Activision, Electronic Arts, Insomniac Games, Take 2 Productions, WB Games, and more.According to Audrey Cooling, a spokesperson for the bargaining group representing the companies involved, video game producers are pleased SAG-AFTRA members ratified the agreement. “We look forward to building on our industry's decades-long partnership with the union and continuing to create groundbreaking entertainment experiences for billions of players worldwide,” says Cooling.Game companies owe actors “basic working conditions and decency,” says Elmaleh, pointing to the value actors add to games. “Otherwise they wouldn't be here negotiating with us.”Elmaleh calls voice acting and motion capture “a secret weapon” in a field where everything is digital. “It imbues persuasiveness and immersion, reality and weight to the rest of the environment,” says Elmaleh, whose credits include blockbuster titles such as Fortnite, The Last of Us Part II, and Halo Infinite. “It feels a bit foolhardy to kind of throw away that kind of superpower in your tool kit to immediately connect with players.”The fight",
      "content_available": true,
      "content_word_count": 702,
      "tags": [
        "ai",
        "gpt",
        "vision",
        "video"
      ],
      "quality_score": 1.0,
      "image_url": "https://media.wired.com/photos/686eb4d56b0ab9affa44e8fd/master/pass/Hollywood-Actors-Video-Game-Publishers-Agree-AI-Use-Games-Culture-2169079499.jpg",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:13.276275+00:00",
      "metadata": {
        "word_count": 48,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "30295a9b159293d36a31416524c9d431",
      "title": "You Asked, We Answered: All of Your AI Angst",
      "url": "https://www.wired.com/story/uncanny-valley-podcast-you-asked-we-answered-all-of-your-ai-angst/",
      "source": "Wired AI",
      "published": "2025-07-10T16:27:53Z",
      "summary": "This week, our host Lauren Goode, along with two of our senior writers, Kate Knibbs and Paresh Dave, dive into the show’s inbox to answer listeners’ questions. We look into a range of queries-from how AI is shaping the film industry to brainstorming what the Jony Ive and Open AI collaboration might look like. Write to us at uncannyvalley@wired . com .",
      "full_content": "This week, our host Lauren Goode, along with two of our senior writers, Kate Knibbs and Paresh Dave, dive into the show’s inbox to answer listeners’ questions.\n\nWe look into a range of queries—from how AI is shaping the film industry to brainstorming what the Jony Ive and Open AI collaboration might look like.Mentioned in this episode:This Viral AI Chatbot Will Lie and Say It’s Human by Lauren Goode and Tom SimoniteA Political Battle Is Brewing Over Data Centers by Molly TaftYou can follow Lauren Goode on Bluesky at @laurengoode, Kate Knibbs on Bluesky at @knibbs, and Paresh Dave on Bluesky at ‪@peard33.\n\nWrite to us at uncannyvalley@wired.com.How to ListenYou can always listen to this week's podcast through the audio player on this page, but if you want to subscribe for free to get every episode, here's how:If you're on an iPhone or iPad, open the app called Podcasts, or just tap this link.\n\nYou can also download an app like Overcast or Pocket Casts and search for “uncanny valley.” We’re on Spotify too.TranscriptNote: This is an automated transcript, which may contain errors.Lauren Goode: This is WIRED's Uncanny Valley, a show about the people power and influence of Silicon Valley. I'm Lauren Goode. I'm a senior correspondent at WIRED.\n\nToday we are bringing you a different kind of episode.\n\nMike and Katie are out this week on well-deserved vacations.\n\nSo with the help of our Uncanny Valley producers, I went deep into the show's inbox to see what you all were curious about. You've been sending us some really great questions.\n\nSo we chose five excellent questions ranging from how AI has impacted the film industry, to what it means for our healthcare future when chatbots are spitting out false information, to what we can expect of the much talked about Jony Ive and Sam Altman collab. I was determined to find good answers and I didn't think that I could answer them all on my own.\n\nSo I enlisted the help of two brilliant colleagues at WIRED to help me answer your questions.Kate Knibbs: My name is Kate Knibbs. I'm a senior writer at WIRED.Paresh Dave: And I'm Paresh Dave, senior writer at WIRED.Lauren Goode: Hey Kate, how are you doing this morning, this afternoon? What time is it?Kate Knibbs: Time is just a concept, Lauren.\n\nAnd I'm good. I am planning on eating ice cream later today, so that's sort of the prize that I have my eye on.Lauren Goode: Why later? I mean, why not just eat it now on the show?Kate Knibbs: Because I have to go to a doctor's appointment. I'm very pregnant and they're going to weigh me, so I'm going to eat the ice cream after I get weighed.\n\nIt's sort of a ritual I have.Lauren Goode: You've got it all worked out.\n\nLike a ritual, as in you typically eat ice cream after the doctor's appointments?Kate Knibbs: Yes, and in between the doctor's appointments, to be clear, but always after.Lauren Goode: I love this.\n\nThis is like when you're a little kid and your mom drags you to the bank and they give you a lollipop afterwards for being so patient.Kate Knibbs: Exactly.\n\nBringing that into our adult lives.Lauren Goode: That's wonderful. But for now, I do have to ask you to answer some burning questions.\n\nThank you so much for being game to come on Uncanny Valley and dig into the mailbag. I'll read to you the first question and we'll go from there.\n\nThe first one comes from Janae, who was writing to us from London, and she says, \"One topic that recently piqued my interest was the impact of AI on the film industry. I was just reading an article around how AI is impacting how films are made and the trade-offs between the use of AI relating to creativity and to budgets.\n\nKate, what would you say broadly, beyond that, are the main changes you've seen to how films and TV shows are made in Hollywood now that generative AI has entered the scene?Kate Knibbs: So I think looking at how AI is changing film and television is a great barometer for how quickly AI is advancing because it's really already being used in every step in the production process, and it's not a fringe thing at all.\n\nLike The Brutalist, which was the best picture nominee last year, used AI in several different ways.Lauren Goode: Oh, wow. I didn't realize that. I just watched it.Kate Knibbs: Yeah, it was like a minor controversy. I think it was making sure Adrien Brody's pronunciation was correct, they used AI to do that.\n\nBut yeah, so it's very much already embedded in mainstream Hollywood filmmaking and also in distribution.\n\nWhen you watch a foreign language show on Netflix, if you're watching it dubbed, you are watching AI integrated into the process because Netflix uses AI to dub foreign language versions of its shows.\n\nSo if you want to watch Squid Game in English so that they're saying words in English versus subtitles, which just read the subtitles in my opinion. But that's neither here nor there.\n\nIf you're watching the dubbing, you're watching AI voices.Lauren Goode: Fascinating.Kate Knibbs: Yeah, yeah.Lauren Goode: Does that mean AI is actually being used so that the mouth movements of the actors are matching the dubbing?Kate Knibbs: Yes, they're trying.\n\nIt looks really weird.Lauren Goode: I had no idea. That is fascinating.\n\nAnd there are entire companies and teams of human beings who in the past and hopefully still, are dedicated to localization, to making that happen.\n\nBut now it's happening with AI.Kate Knibbs: Yeah, they might not be having the best year.\n\nAnd then another part of the film in television industry that AI has made a big impact on, is storyboarding, which is when they're developing a TV show or movie, they hire visual artists to sort of sketch out how the sequences will look. I've talked to a lot of visual artists in that field who say that that whole field is getting completely wiped out basically because it's so easy to have image generators mock up storyboards now.\n\nAnd even really big name action movies that you've probably heard of involving superheroes are using that kind of technology.\n\nSo those are just a few examples, but basically anything you can think of is there's some sort of experiment being done with GenAI tools.Lauren Goode: And what does all of this mean for the Hollywood labor market? Obviously AI was a big topic of contention during the strikes a couple of summers ago.\n\nWhere are we now?Kate Knibbs: So there's not one monolithic response to AI, but I think the fact that it was such a point of contention is really indicative of how a lot of people in crew and actors and actresses feel, which is threatened, because this technology really, it will augment some jobs for sure, but it is already, as I talked about with the storyboarding, replacing some work that was formerly done by humans.\n\nAnd so there's a lot of pushback.\n\nThere's a lot of, I think, valid trepidation.Lauren Goode: Right.\n\nThe sense is that it's really going to benefit the studios, their bottom line versus the workers, the character actors.Kate Knibbs: Definitely will.\n\nThere are, I will say though, I actually talked to a group of documentary filmmakers a few weeks ago who are all very interested in incorporating AI into their processes and already are.\n\nAnd on the director and producer side of things, there are some really prominent directors and producers who are also embracing this tech.\n\nDarren Aronofsky, the director of Black Swan and Requiem For A Dream, has a AI film studio and he has a partnership with Google's DeepMind, and I'm sure whatever he does with that is going to be just as upsetting as Requiem For A Dream in a different way.Lauren Goode: Did I also ever tell you about my bus ride in Lisbon with Darren Aronofsky?Kate Knibbs: No.\n\nWhat happened?Lauren Goode: I had interviewed him at a conference.\n\nDarren strikes me as someone who's always been pretty tech forward, and at the time, this was 2018 or 2019, it was at a conference in Lisbon, Portugal, and he was doing a lot in VR.\n\nSo I interviewed him on stage at this conference about that, and then afterwards, a bunch of us were going to the same dinner and he and I and another person ended up on a bus that took forever to get across Lisbon to this dinner.\n\nSo I was stoked as a journalist, thinking, \"I'm literally sitting in the back of the bus with Darren Aronofsky and I get to ask him all these questions.\" I'm sure he was like, \"Get me the hell off of this bus.\" And then we got to the dinner and he introduced me to the giant wave surfer, Garrett McNamara, who was in town because he was surfing those giant waves in Nazare.\n\nAnd I have to tell you, Kate, I've never had anything feel so much like a fever dream.Kate Knibbs: I was mildly envious of the Aronofsky thing, but the big wave surfers are my heroes, so that's very cool.\n\nHe should make a movie about them.Lauren Goode: HBO already did this fantastic 100 foot wave docu-series about them. But yeah, soon enough that'll all be replaced with AI.\n\nThere'll be like 150-foot waves just completely propped up with AI.Kate Knibbs: See, no, that's what you can't replace.\n\nYou can't replace Mother Nature, baby.Lauren Goode: You cannot. You cannot.\n\nSo you mentioned that Darren is doing a partnership with Google DeepMind. A couple of months ago at Google's annual software conference, they showed off this Veo 3 video tool, and this is just crazy.\n\nIt is crazy how good this tool is. I mean, still those of us with sort of course skilled at picking out AI content versus real content, it's still a little uncanny, but this just felt like a huge leap.\n\nSo I was hoping to get a sense of whether or not this is something that hobbyists are going to use, are professionals going to start using this, are all the filmmakers going to start using this? Where do you see it going from here?Kate Knibbs: I think that both hobbyists and professionals are going to use Veo and the tools like it that come after, I think they already are probably doing really interesting things with it.\n\nAnd I feel, I mean, pretty ambivalent because I'm sure there's going to be a really cool movie coming out that wouldn't otherwise exist because a young filmmaker working in their bedroom suddenly has access to this suite of tools and that's cool.\n\nBut then I do think the rise of this technology is inevitably going to be accompanied by loss of jobs, loss of skilled labor, a change in the industry that will be bittersweet for some and just plain bitter for others.\n\nAnd I don't think there's a way to stop it really. Maybe the copyright lawsuits will slow things down.\n\nBut the idea that there might be some sort of needle that could be threaded where this tech will roll out, but people will keep all their jobs, I just think is unfortunately unrealistic as much as I'd like it to be so.\n\nIt's a thing that's going to bring a lot of beauty and it's a thing that's going to bring a lot of misery.Lauren Goode: Well, that is a sobering answer, Kate, but hopefully we were able to answer at least part of Janae's question.\n\nLet's go to the next one. This one comes from Elizabeth.\n\nIt's also about AI, and Elizabeth asks us the following. \"I work in a field where misinformation and conspiracy theories are replacing scientific evidence online.\n\nWhat happens when the next AI models begin learning from the new less scientifically valid data? I'd love to know how LLMs are trained, if anyone in the field is concerned about the effects of replacement of scientific data online.\n\nAnd if so, what are they doing about it?\" I mean, I think it's safe to say concern abounds. There's a lot of concern.\n\nWe are officially concerned.Kate Knibbs: Yeah, we are definitely officially concerned.\n\nTo break it down a little bit like when it comes to how LLMs are trained. LLMs, as much as we yearn for AGI, they don't really think creatively. Right now, LLMs work by spitting out plausible sounding sentences.\n\nThe way that they do that is that they are trained by a process where they're exposed to vast data sets of the written word.\n\nAnd this ranges from Anna Karenina and classical literature to the most disturbing forums on 4Chan and Reddit, you've ever seen. Ranges from the best scientific data to the worst scientific data.\n\nThe models ingest these data sets and then they're instructed to look for language patterns. So they begin to learn how to predict the most probable next words in sentences.\n\nThat's the first part of the training.\n\nThen there are all these refinement processes that take place, like fine-tuning and prompt-tuning.\n\nThat's when the people who are making the LLMs tailor them for whatever use cases they imagine and do stuff like remove horrible, violent or sexually explicit content.\n\nThis is where they put all the guardrails in as well.Lauren Goode: And that fine-tuning can also be done to create sort of a precision model.\n\nLike if you're building a model just for one specific healthcare application, then you might want to fine-tune the tool for that.Kate Knibbs: Yeah, definitely.\n\nAnd so the moment that an LLM becomes vulnerable to absorbing inaccurate scientific data is really the moment that the LLM ingests any of that data.\n\nBecause it will really depend on how well the fine-tuning works, whether it's able to properly ignore misinformation or whether it accidentally just becomes this machine that spews misinformation. So the problem is right there in the beginning.\n\nNow that there are some people who think that maybe LLMs don't need to ingest as large of data sets as we originally thought.\n\nThere might be a movement to cull the pre-training and training data so that the misinformation isn't there from the beginning.\n\nBut for the most part, the sheer quantity of words that these models are trained on means that they're definitely trained on at least some bullshit.Lauren Goode: And it seems like when you are thinking about the potential effects of LLMs spitting out false information, either because they've been trained on false data, they've been programmed in such a way, the stakes are especially high in healthcare. I was just reading about how a group of medical researchers were able to very easily configure popular AI chatbots to spit out false information.\n\nAnd not just bad info, but also to sound very authoritative, which is what we know these chatbots do. And look, this is not a new idea. People have been able to jailbreak these chatbots for testing purposes.\n\nEven at WIRED, a little while ago, an editor and I were able to program a popular customer service chatbot to lie and tell callers that it was human. So people are doing this because they are trying to red team them and make them better.\n\nBut in this instance, the researchers were basically able to tailor the chatbots to incorrectly answer questions like, \"Does sunscreen cause skin cancer? Does 5G cause infertility?\" Stuff that you can imagine people going online and searching for. I think one of the chatbots, Claude, which is done by Anthropic, refused more than half the time to spit out false information.\n\nThe others just put out fake answers 100 percent of the time.\n\nSo I guess my question is, at the breakneck speed that these companies, whether it's a small startup or a big startup or one of the frontier model companies, the speed at which they're moving to put out the next best AI model, what incentive do they have, do you think they have, to put in guardrails to avoid spreading this misinformation?Kate Knibbs: So I think that AI companies that are creating specialty tools for the healthcare industry will be far more incentivized than the companies that are creating general interest models, because there's already so much market saturation with companies that have general interest models out there.\n\nFrankly, it hasn't really hurt them that much that they're spewing out all of this bad information.\n\nSo it would take some larger scandals, I would say, for them to really start focusing on creating dedicated, no scientific, misinformation team, although I would love to see that happen.\n\nNow, there's a whole industry devoted to more tailored LLMs, and those include LLMs that are made for doctors.\n\nThose companies have obviously major incentives to make sure that they're not spewing misinformation because that's sort of their whole sales pitch, is like, \"We are offering you a more precise, more accurate, safer version of an LLM that's designed to be medically accurate.\" So for instance, Google has its own medical question LLM actually, called Med=PaLM, which is very cool. I do wonder right now though, how many people in healthcare are using that versus just firing up ChatGPT? I really don't know.\n\nBut the fact of the matter is LLMs",
      "content_available": true,
      "content_word_count": 2908,
      "tags": [
        "ai",
        "chatbot",
        "api"
      ],
      "quality_score": 1.0,
      "image_url": "https://media.wired.com/photos/686d8a8570bad407abe36806/master/pass/Uncanny-Valley-Answering-Questions-Politics-1205489879.jpg",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:13.361452+00:00",
      "metadata": {
        "word_count": 62,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "c6196382fda5a50771a9e704b3292985",
      "title": "AI Is a Lousy Chef",
      "url": "https://www.wired.com/story/dishgen-ai-recipes-tested/",
      "source": "Wired AI",
      "published": "2025-07-10T11:00:00Z",
      "summary": "DishGen is one of the few services that specializes in AI cooking. It takes recipes from large language models like OpenAI and Anthropic and repackages them into more kitchen-friendly formats. It also gives users the ability to use AI recipes in a variety of ways.",
      "full_content": "After my sage plant struggled for years in a shadowy corner of my roof-deck planter garden, I moved it into the sunniest spot up there. Boy did that make a difference.\n\nIt also meant I needed to figure out what to do with all the leaves it produced.\n\nAs luck would have it, I was also trying out AI cooking platforms.That night, I plugged the prompt “recipe using brats and lots of sage” into an AI recipe generator and it gave me what it called “sage infused brats skillet with caramelized onions.” Ooh, that sounds nice, I thought.The platform I prompted is called DishGen, one of the few services that specializes in AI cooking.\n\nIt takes recipes from large language models like OpenAI and Anthropic and repackages them into more kitchen-friendly formats.\n\nIt also gives users the ability to create meal plans. (Other tools, like ChefGPT and Epicure, offer similar features.)DishGen's sage-and-sausage recipe used only 2 tablespoons of sage, not exactly lots, but I picked my way through the steps and emerged with a pleasant-enough Tuesday dinner. I also needed to use a fair amount of my own kitchen experience to get it across the finish line.For example, the ingredients called for a “large yellow onion, thinly sliced.” Shall we peel it? Cut in half before we slice it? Pole-to-pole or through the equator? And how thin is thin? It didn't say.\n\nPeople who like cooking from well-written recipes would find this frustrating.The recipe also calls for “2 tablespoons fresh sage leaves, chopped,” which comma lovers will appreciate, is confusing.\n\nDo you fold up those sage leaves and smash them into your tablespoon, then chop them? (It's clearer to call for the desired quantity of “chopped fresh sage leaves.”)Once I started cooking, the butter and sliced onions went into a skillet to “cook slowly until caramelized, about 12 minutes,” and oh man, so many things.\n\nAre they just not feeling the stirring? What about some descriptions of things to watch for along the way so we know we're doing it right? I also noted at that point that it really wasn't much onion considering the recipe is for four people.My wife Elisabeth picked up on this right away when I set it down. “Is that all of it, or is there more,” she asked. “It just looks like a little blob.”In all my years of food, cookbook, and recipe writing, I can say with confidence that when testing a recipe from a new source, the chance that I happened to pick out the only bad apple in the bunch on my first try is witheringly small.When I looked for sage-heavy recipes in The New York Times Cooking section (which is also an app), one of the\n\nresults was for Samin Nosrat's fried sage salsa verde.\n\nDid you notice how your ears perked up there? Maybe you even did a bit of involuntary salivating?I made it the following evening with some sous vide chicken thighs I had in the fridge and immediately appreciated the clarity of the instructions and the accompanying video of Nosrat hamming it up while making the dish. I was cooking something new to me, but simply by watching the video and reading over the recipe, I wasn't going in blind.\n\nParticularly helpful are the tips for frying the sage, which it turns out I've been overdoing. Fry it up until it stops bubbling, preferably in oil at 360 degrees Fahrenheit.\n\nLet it crisp on a paper-towel-lined tray, sprinkle it with some flaky salt, have a taste, and you will know instantly that you're onto something.Meal TicketAll the sage cooking reminded me of using Eat Your Books, a subscription website that creates an index of your cookbooks, allowing you to tell it what you'd like to cook, or ingredients you'd like to use, and it tells you which of your cookbooks to look in and what pages the recipes are on.\n\nIf you have a good-sized cookbook library, this is a godsend. (EYB will soon launch a companion app for its website called CookShelf).Using EYB reminded me of the stunning chicken with gin and sage jus from Amy Thielen's cookbook, Company.\n\nIt’s so gripping and such a good use of sage that the Times wrote a story about it.Next up, I found some nice tomatillos at MacPherson's Fruit & Produce in Seattle, which got me craving a Mexican-style salsa verde. I told DishGen that I wanted to use 500 grams of tomatillos, a poblano pepper, and a jalapeño.\n\nImpressively, it spat out a recipe using exactly those amounts along with other classic ingredients like onion, cilantro, garlic, and lime juice.It called for \"1/2 cup of chopped white onion\" which would be better if it stated how large an onion you'd need and perhaps how big to chop it.\n\nThe recipe goes on to have you char the tomatillo and peppers, but mysteriously not the onion and garlic, which you add to the blender just before serving.\n\nYes, there's a bit of salt and lime juice in there, but not enough to soften the double raw allium edge.\n\nThe AI recipe writing always felt, as a colleague once put it, squirrelly and weird.Thankfully, Eat Your Books pointed me toward another salsa verde from Bricia Lopez and Javier Cabral's Asada, where similar ingredients are charred then brought to a simmer after going in the blender.\n\nIt was too late for me to char the onion and garlic in my AI salsa, but I did use the simmering-at-the-end idea, pouring mine from the blender into a pot on the stove, successfully tamping down the dragon breath.Following that, I did a head-to-head with classic food processor pesto, making one version from DishGen and another from Marcella Hazan, whose name might give you a sense of a forthcoming ass whooping.\n\nYet I was pleased to find the DishGen pesto was fairly solid, although it oxidized and turned brown more quickly than I would have liked.Marcella's pesto, from Essentials of Classic Italian Cooking, is elevated with smart tips like using both Parmesan and Romano for a deeper flavor and folding in the cheeses after the sauce has been through the food processor, which makes for a nicer texture.\n\nShe also has a Mortal Kombat–level finishing move, incorporating room-temperature butter into the sauce right at the end, that gives your pasta a luxurious silkiness.I thought about making a third batch pulling directly from ChatGPT, but its classic pesto recipe turned out to feature pine nuts that you lightly toast without telling you how to do that.\n\nIn a pan? On a burner? In the oven? What temperature? Sorry, I'll pass.\n\nFor comparison, I looked at America's Test Kitchen's food processor pesto and saw its recipe tells you to quickly and cleverly blanch the basil leaves in boiling water to keep the sauce from browning, a trick I'll incorporate into my future pesto making, no matter where the recipe is from.Food for ThoughtRecipe writing is both an art form and a style of technical writing with rules; there are excellent reference textbooks to help recipe writers get it right.\n\nGood recipes help cut down on confusion while guiding you to great results. They list ingredients in the order they are used when cooking, for example.\n\nThey favor clear waypoints—often instructing you to rely on your eyes or ears—as the primary way to know it's time to move to the next step, as relying on saying how long something should take can be misleading and frustrating.\n\nMany of the AI-created recipes have not picked up on the rules or the art that can make cooking efficient, enjoyable, and delicious.For me, though, the real problem using AI to create recipes emerged when I asked the DishGen chatbot questions about copyright.“The recipes I create are original compositions generated based on general culinary knowledge, common cooking techniques, and widely known flavor pairings.\n\nThey are not copied from any specific copyrighted source but are instead uniquely formulated to suit your ingredients and preferences.\n\nThis approach ensures that the recipes are both safe to use and free from copyright concerns.”I remembered that DishGen doesn't create the recipes, it culls them from other LLMs like OpenAI and Anthropic. I then asked if it could do something in the style of America's Test Kitchen.“I don't have direct access to specific recipes from America's Test Kitchen or other copyrighted cookbooks to display their exact content.\n\nHowever, I can help you create a recipe inspired by the style and techniques commonly used by America's Test Kitchen, focusing on reliable methods, balanced flavors, and clear instructions.”Then it offered a creamy chicken and sage recipe “inspired by Americas Test Kitchen style.”The phrase “direct access” stuck in my head and, on a hunch, I looked at The Atlantic's story by Alex Reisner where readers can search for an authors' names in LibGen, the pirated library of 7.5 million books and 81 million research papers that Meta and Open AI have used in the past to train their AI products like ChatGPT.The LibGen list included pirated work from Elisabeth and from me and from thousands of other authors, but I was stunned to when I typed in “America's Test Kitchen” and 163\n\nresults\n\ncame up, scores of their books like Paleo Perfected, Air Fryer Perfection, Foolproof Fish, and The Complete Baby and Toddler Cookbook.“It feels like AI is in its Napster phase,” a recipe editor and culinary librarian friend of mine once quipped, “except the pirates are some of the world’s biggest companies.”ATK is a prodigious publisher and everything on the Atlantic list appears to have been scraped by LibGen.\n\nIt was then likely hoovered up by Meta and Open AI, perhaps shedding light on how the sage sausage gets made.\n\nIt's possible that part of the database the two companies used did not help train their products to write recipes.\n\nIt's possible it did.Working backward, I prompted DishGen for a chorizo and black bean chimichanga like the one in ATK’s The Best Mexican Recipes, and DishGen created something quite different.\n\nThen I looked for a slow cooker spaghetti squash with tomato sauce like the one in ATK’s Multicooker Perfection, and DishGen brought up something surprisingly similar that included just about everything on the ATK ingredient list except tomato paste.Right after this, I looked at the list of recipes I had cooked in DishGen and the illustrated thumbnails looked surprisingly like the work of Sarah Becan, whose wonderful Let's Make Dumplings and Let's Make Ramen, both with chef Hugh Amano, are part of the LibGen database.DishGen did not respond to a request for comment.Despite all this, so many of the AI-generated recipes I found were neither interesting nor well written, meaning cooking from them becomes more difficult and less rewarding.“When I've tried AI recipes it feels like the engine has scraped details from many sources and then spit out a sort of weird recipe average,” says Dan Souza, chief content officer at America’s Test Kitchen. “You might get something that is baseline tasty, but it's never memorable.\n\nWhich makes sense.\n\nNo one is tasting it before you try it.”One of DishGen's services is meal planning, which would be intriguing if the recipes it pulls from the LLMs were more notable.\n\nIt's an interesting service and the repackaging it does is impressive, but it is pulling from often-underwhelming source material.\n\nHome cooks would be better served if it could point people at better recipes or if the LLMs licensed great recipes from trusted sources.Here are a couple ideas: Instead of turning your meals over to those LLMs with ethically dubious sourcing and no taste buds, use the money you would've spent on an AI recipe subscription to buy a few cookbooks—here are dozens of suggestions for all skill levels—or get a subscription to ATK ($80/year), or New York Times Cooking ($50/year).\n\nIf you have a bunch of cookbooks, try Eat Your Books/CookShelf ($40/year).\n\nIf you want meal plans with recipes created by a chef, try Ends and Stems ($114/year).Right before I wrapped up, I typed “brats and sage” into NYT Cooking and it came back with Country-Sausage and Sage Dressing.\n\nAnd even without cooking it, I just decided that won, because I trust them.",
      "content_available": true,
      "content_word_count": 2033,
      "tags": [
        "ai",
        "gpt",
        "language",
        "platform"
      ],
      "quality_score": 1.0,
      "image_url": "https://media.wired.com/photos/68669ede64b9e304119dcbff/master/pass/ai-food-slop-gear-149628035.jpg",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:13.438991+00:00",
      "metadata": {
        "word_count": 45,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "cc08e54ebf8d88cc0bfd75c864f0efda",
      "title": "Dr. ChatGPT Will See You Now",
      "url": "https://www.wired.com/story/dr-chatgpt-will-see-you-now-artificial-intelligence-llms-openai-health-diagnoses/",
      "source": "Wired AI",
      "published": "2025-07-10T10:46:46Z",
      "summary": "A poster on Reddit lived with a painful clicking jaw, the result of a boxing injury, for five years. They saw specialists, got MRIs, but no one could give them a solution to fix it, until they described the problem to ChatGPT. The AI chatbot suggested a specific jaw-alignment issue might be the problem and offered a technique involving tongue placement.",
      "full_content": "A poster on Reddit lived with a painful clicking jaw, the result of a boxing injury, for five years.\n\nThey saw specialists, got MRIs, but no one could give them a solution to fix it, until they described the problem to ChatGPT.\n\nThe AI chatbot suggested a specific jaw-alignment issue might be the problem and offered a technique involving tongue placement as a treatment.\n\nThe individual tried it, and the clicking stopped. “After five years of just living with it,” they wrote on Reddit in April, “this AI gave me a fix in a minute.”The story went viral, with LinkedIn cofounder Reid Hoffman sharing it on X.\n\nAnd it’s not a one-off: Similar stories are flooding social media—of patients purportedly getting accurate assessments from LLMs of their MRI scans or x-rays.Courtney Hofmann’s son has a rare neurological condition.\n\nAfter 17 doctor visits over three years and still not receiving a diagnosis, she gave all of his medical documents, scans, and notes to ChatGPT.\n\nIt provided her with an answer—tethered cord syndrome, where the spinal cord can’t move freely because it’s attached to tissue around the spine—that she says physicians treating her son had missed. “He had surgery six weeks from when I used ChatGPT, and he is a new kid now,” she told a New England Journal of Medicine podcast in November 2024.Consumer-friendly AI tools are changing how people seek medical advice, both on symptoms and diagnoses.\n\nThe era of “Dr. Google” is giving way to the age of “Dr.\n\nChatGPT.” Medical schools, physicians, patient groups, and the chatbots’ creators are racing to catch up, trying to determine how accurate these LLMs’ medical answers are, how best patients and doctors should use them, and how to address patients who are given false information.“I’m very confident that this is going to improve health care for patients,” says Adam Rodman, a Harvard Medical School instructor and practicing physician. “You can imagine lots of ways people could talk to LLMs that might be connected to their own medical records.”Rodman has already seen patients turn to AI chatbots during his own hospital rounds.\n\nOn a recent shift, he was juggling care for more than a dozen patients when one woman, frustrated by a long wait time, took a screenshot of her medical records and plugged it into an AI chatbot. “She’s like, ‘I already asked ChatGPT,’” Rodman says, and it gave her the right answer regarding her condition, a blood disorder.Rodman wasn’t put off by the exchange.\n\nAs an early adopter of the technology and the chair of the group that guides the use of generative AI in the curriculum at Harvard Medical School, he thinks there’s potential for AI to give physicians and patients better information and improve their interactions. “I treat this as another chance to engage with the patient about what they are worried about,” he says.The key word here is potential.\n\nSeveral studies have shown that AI is capable in certain circumstances of providing accurate medical advice and diagnoses, but it’s when these tools get put in people’s hands—whether they’re doctors or patients—that accuracy often falls.\n\nUsers can make mistakes—like not providing all of their symptoms to AI, or discarding the right info when it is fed back to them.In one example, researchers gave physicians a set of patient cases and asked them to estimate the chances of the patients having different diseases—first based on the patients’ symptoms and history, and then again after seeing lab results.\n\nOne group had access to AI assistance while another did not.\n\nBoth groups performed similarly on a measure of their diagnostic reasoning, which looks at not just the accuracy of the diagnosis but also at how they explained their reasoning, considered alternatives, and suggested next steps.\n\nThe AI-assisted group had a median diagnostic reasoning score of 76 percent, while the group using only standard resources scored 74 percent.\n\nBut when the AI was tested alone—without any human input—it scored much higher, with a median score of 92 percent.Harvard’s Rodman worked on this study and says when the research was conducted in 2023, AI chatbots were still relatively new, so doctors’ lack of familiarity with these tools may have lessened their ability to reach an accurate diagnosis.\n\nBut beyond that, the broader insight was that physicians still viewed themselves as the primary information filter. “They loved it when it agreed with them, and they disregarded it when it disagreed with them,” he says. “They didn’t trust it when the machine told them that they were wrong.”Rodman himself tested AI a few years ago on a tough case that he and other specialists had misdiagnosed on first pass.\n\nHe provided the tool with the information he had on the patient’s case, “and the first thing it spat out was the very rare disease that this patient had,” he says.\n\nThe AI also offered a more common condition as an alternative diagnosis but deemed it less likely.\n\nThis was the condition Rodman and the specialists had misdiagnosed the patient with initially.Another preprint study with over 1,200 participants showed that AI offered the right diagnosis nearly 95 percent of the time on its own but dropped to only a third of the time when people used the same tools to guide their own thinking.For example, one scenario in the study involved a painful headache and stiff neck that had come on suddenly.\n\nThe correct action is to seek immediate medical attention for a potential serious condition like meningitis or a brain hemorrhage.\n\nSome users were able to use the AI to reach the right answer, but others were told to just take over-the-counter pain medication and lie down in a dark room.\n\nThe key difference between the AI’s responses, the study found, was due to the information provided—the incorrect answer was generated when the sudden onset of symptoms wasn’t mentioned by the user.But regardless of whether the information provided is right or wrong, AI presents its answers confidently, as truthful, even when that answer may be completely wrong—and that’s a problem, says Alan Forster, a physician as well as a professor in innovation at McGill University’s Department of Medicine.\n\nUnlike an internet search that returns a list of websites and links to follow up on, AI chatbots write in prose. “It feels more authoritative when it comes out as a structured text,” Forster says. “It’s very well constructed, and it just somehow feels a bit more real.”And even if it is right, an AI agent can’t complement the information it provides with the knowledge physicians gain through experience, says fertility doctor Jaime Knopman.\n\nWhen patients at her clinic in midtown Manhattan bring her information from AI chatbots, it isn’t necessarily incorrect, but what the LLM suggests may not be the best approach for a patient’s specific case.For instance, when considering IVF, couples will receive grades for viability for their embryos.\n\nBut asking ChatGPT to provide recommendations on next steps based on those scores alone doesn’t take into consideration other important factors, Knopman says. “It’s not just about the grade: There’s other things that go into it”—such as when the embryo was biopsied, the state of the patient’s uterine lining, and whether they have had success in the past with fertility.\n\nIn addition to her years of training and medical education, Knopman says she has “taken care of thousands and thousands of women.” This, she says, gives her real-world insights on what next steps to pursue that an LLM lacks.Other patients will come in certain of how they want an embryo transfer done, based on a response they received from AI, Knopman says.\n\nHowever, while the method\n\nthey’ve been suggested may be common, other courses of action may be more appropriate for the specific patient’s circumstances, she says. “There’s the science, which we study, and we learn how to do, but then there’s the art of why one treatment modality or protocol is better for a patient than another,” she says.Some of the companies behind these AI chatbots have been building tools to address concerns about the medical information dispensed.\n\nOpenAI, the parent company of ChatGPT, announced on May 12 it was launching HealthBench, a system designed to measure AI’s capabilities in responding to health questions.\n\nOpenAI says the program was built with the help of more than 260 physicians in 60 countries, and includes 5,000 simulated health conversations between users and AI models, with a scoring guide designed by doctors to evaluate the responses.\n\nThe company says that it found that with earlier versions of its AI models, doctors could improve upon the responses generated by the chatbot, but claims the latest models, available as of April 2025, such as GPT-4.1, were as good as or better than the human doctors.“Our findings show that large language models have improved significantly over time and already outperform experts in writing responses to\n\nexamples\n\ntested in our benchmark,” Open AI says on its website. “Yet even the most advanced systems still have substantial room for improvement, particularly in seeking necessary context for underspecified queries and worst-case reliability.”Other companies are building health-specific tools that are specifically designed for medical professionals to use.\n\nMicrosoft says it has created a new AI system—called MAI Diagnostic Orchestrator (MAI-DxO)—that in testing diagnosed patients four times as accurately as human doctors.\n\nThe system works by querying several leading large language models—including OpenAI’s GPT, Google’s Gemini, Anthropic’s Claude, Meta’s Llama, and xAI’s Grok—in a way that loosely mimics multiple human experts working together.New doctors will need to learn how to both use these AI tools as well as counsel patients who use them, says Bernard S.\n\nChang, dean of medical education at Harvard Medical School.\n\nThat’s why his university was one of the first to offer students classes on how to use the technology in their practices. “It’s one of the most exciting things that’s happening right now in medical education,” Chang says.The situation reminds Chang of when people started turning to the internet for medical information 20 years ago.\n\nPatients would come to him and say, “I hope you’re not one of those doctors that uses Google.” But as the search engine became ubiquitous, he wanted to reply to these patients: “You wouldn’t want to go to a doctor who didn’t.” He sees the same thing now happening with AI. “What kind of doctor is practicing at the forefront of medicine and doesn’t use this powerful tool?”Updated 7-11-2025 5:00 pm BST: A misspelling of Alan Forster’s name was corrected.",
      "content_available": true,
      "content_word_count": 1743,
      "tags": [
        "ai",
        "gpt",
        "llm",
        "chatbot"
      ],
      "quality_score": 1.0,
      "image_url": "https://media.wired.com/photos/6814711e99e80f99653378d9/master/pass/GettyImages-1679783885.jpg",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:13.509179+00:00",
      "metadata": {
        "word_count": 61,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "75e1c17a946ea5a6cd4e0796fd0356fd",
      "title": "Decision Trees Aren’t Just for Tabular Data",
      "url": "https://machinelearningmastery.com/decision-trees-arent-just-for-tabular-data/",
      "source": "Machine Learning Mastery",
      "published": "2025-07-10T09:57:51Z",
      "summary": "Decision trees have been among the most well-established machine learning techniques for decades. They are widely used for classification and regression tasks. Decision trees can be used for a variety of use cases, including regression and classification. For more information on decision trees, visit Decision Trees.org. For information on how to use decision trees in your business, visit the Decision Trees website.",
      "full_content": null,
      "content_available": false,
      "content_word_count": 0,
      "tags": [
        "machine learning",
        "classification",
        "regression"
      ],
      "quality_score": 0.7,
      "image_url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-decision-trees-tabular-data-1.png",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:13.953960+00:00",
      "metadata": {
        "word_count": 62,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": false
      }
    },
    {
      "id": "d31cf1ea291d2cc165a0467567ab33cd",
      "title": "Elon Musk Unveils Grok 4 Amid Controversy Over Chatbot’s Antisemitic Posts",
      "url": "https://www.wired.com/story/grok-4-elon-musk-xai-antisemitic-posts/",
      "source": "Wired AI",
      "published": "2025-07-10T06:34:19Z",
      "summary": "Elon Musk on Thursday unveiled Grok 4, the latest AI model from xAI, his multibillion-dollar initiative to rival OpenAI and Google. Without citing detailed evidence, Musk claimed that the model aces standardized tests and exhibits doctorate-level knowledge in a wide array of different disciplines. “Grok 4 is a postgrad-level in everything,” Musk said during an hour-long live broadcast.",
      "full_content": "Elon Musk on Thursday unveiled Grok 4, the latest AI model from xAI, his multibillion-dollar initiative to rival OpenAI and Google.\n\nWithout citing detailed evidence, Musk claimed that the model aces standardized tests and exhibits doctorate-level knowledge in a wide array of different disciplines.“Grok 4 is a postgrad-level in everything,” Musk said during an hour-long live broadcast, which began after midnight in New York. “At least with respect to academic questions, Grok 4 is better than PhD level in every subject.\n\nNo exceptions.”xAI didn’t immediately respond to a request for comment from WIRED about whether it plans to publish an official technical report about Grok 4 detailing its capabilities and limitations.\n\nCompeting AI developers, such as OpenAI and Google, have routinely released similar publications for their models.Users can access Grok 4 through the Grok website or app for $30 a month.\n\nAccess to a larger version known as Grok 4 Heavy costs $300 per month.\n\nLater this year, xAI aims to release additional models that are well suited for software coding tasks and generating video, according to Thursday’s presentation.Musk, who serves as xAI’s CEO, did not address recent criticism of Grok.\n\nOver the past few days, a version of the AI built into Musk’s X social media platform praised Adolf Hitler and provided antisemitic responses to multiple prompts from X users.\n\nIn response, xAI, which owns X, announced Tuesday it would be taking action to “to ban hate speech before Grok posts on X.” On Wednesday, Linda Yaccarino, the CEO of X, announced she is leaving the company but did not elaborate on her reasoning or plans.During Thursday’s livestream, Musk said that, according to his “biological neural net,” AI systems should be optimized “to be maximally truth seeking” and encouraged “to be truthful, honorable, good things—like the values you want to instill in a child that would ultimately grow up to be incredibly powerful.”Musk cofounded xAI in 2023 after OpenAI released ChatGPT and triggered a surge of investment in generative AI technologies that can automatically produce text, code, audio, images, and videos. xAI launched the first version of Grok in November of that year, and Grok 2 debuted last August.\n\nGrok 3, which was released this past February, is available for free.Musk has said that Grok was designed to have a sense of humor and rebelliousness.\n\nOn its website, xAI says its mission is to create accurate AI systems and help people obtain knowledge.Thursday’s late-night product announcement began over an hour behind schedule and featured Musk sitting on a couch with two xAI colleagues in front of a dark background.\n\nThe trio boasted about Grok’s capabilities, displaying slides that aimed to show how the model outperforms other AI programs.\n\nBut Musk also acknowledged it still has significant weaknesses.“These are still primitive tools, not the kind of tools that serious commercial companies use,” he said.Musk predicted that Grok would discover new technologies next year, if not as soon as later this year.\n\nYet, he said, “at times it may lack common sense, and it has not yet invented new technologies or discovered new physics.”He added that xAI would “close the loop around usefulness” so that the technology is “not just book smart but actually practically smart.”At another point during the livestream Musk described Grok as “partially blind” because it still struggles to process and generate images, adding that updates to address the limitation were in the works.Beside offering one-off subscriptions, xAI aims to generate revenue by selling Grok’s technology to businesses and governments that want to develop custom chatbots.\n\nPublic job postings show the company also wants to use its AI technology to boost X’s advertising business.xAI raised $12 billion from investors last year, according to company announcements and US regulatory filings.\n\nLast month, Morgan Stanley disclosed that it had also helped xAI secure $5 billion in debt and that Musk’s company had raised an additional $5 billion by selling shares. xAI’s investors include several venture capital funds with close ties to Musk as well as investment giants such as BlackRock and Fidelity.The funding has enabled xAI to purchase vast numbers of advanced computer chips and develop a data center in Memphis, Tennessee, to train and operate Grok, which the company calls Colossus.\n\nMusk said Thursday that Grok 4 was made possible by xAI’s increased computing capacity.",
      "content_available": true,
      "content_word_count": 716,
      "tags": [
        "ai",
        "gan",
        "video",
        "software",
        "edge"
      ],
      "quality_score": 1.0,
      "image_url": "https://media.wired.com/photos/686f5ab00648d61437356ac0/master/pass/2193208237",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:14.143262+00:00",
      "metadata": {
        "word_count": 58,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "957389b178352332865fd1d9240ac474",
      "title": "McDonald’s AI Hiring Bot Exposed Millions of Applicants' Data to Hackers Using the Password ‘123456’",
      "url": "https://www.wired.com/story/mcdonalds-ai-hiring-chat-bot-paradoxai/",
      "source": "Wired AI",
      "published": "2025-07-09T19:28:50Z",
      "summary": "Olivia is an AI chatbot that screens applicants, asks for their contact information and résumé, directs them to a personality test, and occasionally makes them “go insane” by repeatedly misunderstanding their most basic questions. Security researchers Ian Carroll and Sam Curry revealed that they found simple methods to hack into the backend of the AI.",
      "full_content": "If you want a job at McDonald’s today, there’s a good chance you'll have to talk to Olivia.\n\nOlivia is not, in fact, a human being, but instead an AI chatbot that screens applicants, asks for their contact information and résumé, directs them to a personality test, and occasionally makes them “go insane” by repeatedly misunderstanding their most basic questions.Until last week, the platform that runs the Olivia chatbot, built by artificial intelligence software firm Paradox.ai, also suffered from absurdly basic security flaws.\n\nAs a result, virtually any hacker could have accessed the records of every chat Olivia had ever had with McDonald's applicants—including all the personal information they shared in those conversations—with tricks as straightforward as guessing that an administrator account's username and password was “123456.\"On Wednesday, security researchers Ian Carroll and Sam Curry revealed that they found simple methods to hack into the backend of the AI chatbot platform on McHire.com, McDonald's website that many of its franchisees use to handle job applications.\n\nCarroll and Curry, hackers with a long track record of independent security testing, discovered that simple web-based vulnerabilities—including guessing one laughably weak password—allowed them to access a Paradox.ai account and query the company's databases that held every McHire user's chats with Olivia.\n\nThe data appears to include as many as 64 million records, including applicants' names, email addresses, and phone numbers.Carroll says he only discovered that appalling lack of security around applicants' information because he was intrigued by McDonald's decision to subject potential new hires to an AI chatbot screener and personality test. “I just thought it was pretty uniquely dystopian compared to a normal hiring process, right? And that's what made me want to look into it more,” says Carroll. “So I started applying for a job, and then after 30 minutes, we had full access to virtually every application that's ever been made to McDonald's going back years.”When WIRED reached out to McDonald’s and Paradox.ai for comment, a spokesperson for Paradox.ai shared a blog post the company planned to publish that confirmed Carroll and Curry’s findings.\n\nThe company noted that only a fraction of the records Carroll and Curry accessed contained personal information, and said it had verified that the administrator account with the “123456” password that exposed the information “was not accessed by any third party” other than the researchers.\n\nThe company also added that it’s instituting a bug bounty program to better catch security vulnerabilities in the future. “We do not take this matter lightly, even though it was resolved swiftly and effectively,” Paradox.ai’s chief legal officer, Stephanie King, told WIRED in an interview. “We own this.”In its own statement to WIRED, McDonald’s agreed that Paradox.ai was to blame. “We’re disappointed by this unacceptable vulnerability from a third-party provider, Paradox.ai.\n\nAs soon as we learned of the issue, we mandated Paradox.ai to remediate the issue immediately, and it was resolved on the same day it was reported to us,” the statement reads. “We take our commitment to cyber security seriously and will continue to hold our third-party providers accountable to meeting our standards of data protection.”One of the exposed interactions between a job applicant and “Olivia.” Courtesy of Ian Carroll and Sam CurryCarroll says he became interested in the security of the McHire website after spotting a Reddit post complaining about McDonald's hiring chatbot wasting applicants' time with nonsense responses and misunderstandings.\n\nHe and Curry started talking to the chatbot themselves, testing it for “prompt injection” vulnerabilities that can enable someone to hijack a large language model and bypass its safeguards by sending it certain commands.\n\nWhen they couldn't find any such flaws, they decided to see what would happen if they signed up as a McDonald's franchisee to get access to the backend of the site, but instead spotted a curious login link on McHire.com for staff at Paradox.ai, the company that built the site.On a whim, Carroll says he tried two of the most common sets of login credentials: The username and password “admin,\" and then the username and password “123456.” The second of those two tries worked. “It's more common than you'd think,” Carroll says.\n\nThere appeared to be no multifactor authentication for that Paradox.ai login page.With those credentials, Carroll and Curry could see they now had administrator access to a test McDonald's “restaurant” on McHire, and they figured out all the employees listed there appeared to be Paradox.ai developers, seemingly based in Vietnam.\n\nThey found a link within the platform to apparent test job postings for that nonexistent McDonald's location, clicked on one posting, applied to it, and could see their own application on the backend system they now had access to. (In its blog post, Paradox.ai notes that the test account had “not been logged into since 2019 and frankly, should have been decommissioned.”)That's when Carroll and Curry discovered the second critical vulnerability in McHire: When they started messing with the applicant ID number for their application—a number somewhere above 64 million—they found that they could increment it down to a smaller number and see someone else's chat logs and contact information.The two security researchers hesitated to access too many applicants' records for fear of privacy violations or hacking charges, but when they spot-checked a handful of the 64-million-plus IDs, all of them showed very real applicant information. (Paradox.ai says that the researchers accessed seven records in total, and five contained personal information of people who had interacted with the McHire site.) Carroll and Curry also shared with WIRED a small sample of the applicants' names, contact information, and the date of their applications.\n\nWIRED got in touch with two applicants via their exposed contact information, and they confirmed they had applied for jobs at McDonald's on the specified dates.The personal information exposed by Paradox.ai's security lapses isn't the most sensitive, Carroll and Curry note.\n\nBut the risk for the applicants, they argue, was heightened by the fact that the data is associated with the knowledge of their employment at McDonald's—or their intention to get a job there. “Had someone exploited this, the phishing risk would have actually been massive,” says Curry. “It's not just people's personally identifiable information and résumé.\n\nIt's that information for people who are looking for a job at McDonald's, people who are eager and waiting for emails back.”That means the data could have been used by fraudsters impersonating McDonald's recruiters and asking for financial information to set up a direct deposit, for instance. “If you wanted to do some sort of payroll scam, this is a good approach,” Curry says.The exposure of applicants' attempts—and in some cases failures—to get what is often a minimum-wage job could also be a source of embarrassment, the two hackers point out.\n\nBut Carroll notes that he would never suggest that anyone should be ashamed of working under the Golden Arches.“I have nothing but respect for McDonald’s workers,” he says. “I go to McDonald's all the time.”Updated at 5 pm ET, July 9, 2025 to make clear that the phishing risks would only be possible if someone had the opportunity to exploit the data.",
      "content_available": true,
      "content_word_count": 1178,
      "tags": [
        "ai",
        "artificial intelligence",
        "chatbot",
        "security",
        "research"
      ],
      "quality_score": 1.0,
      "image_url": "https://media.wired.com/photos/686c198b62f12d586161464f/master/pass/mcdonalds-ai-leak-sec-2197231468.jpg",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:14.287688+00:00",
      "metadata": {
        "word_count": 55,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "96da0e92384359af2bc03ff164481594",
      "title": "A New Kind of AI Model Lets Data Owners Take Control",
      "url": "https://www.wired.com/story/flexolmo-ai-model-lets-data-owners-take-control/",
      "source": "Wired AI",
      "published": "2025-07-09T17:59:23Z",
      "summary": "A new kind of large language model, developed by researchers at the Allen Institute for AI, makes it possible to control how training data is used. The new model, called FlexOlmo, could challenge the current industry paradigm of big artificial intelligence companies slurping up data from the web, books, and other sources.",
      "full_content": "A new kind of large language model, developed by researchers at the Allen Institute for AI (Ai2), makes it possible to control how training data is used even after a model has been built.The new model, called FlexOlmo, could challenge the current industry paradigm of big artificial intelligence companies slurping up data from the web, books, and other sources—often with little regard for ownership—and then owning the resulting models entirely.\n\nOnce data is baked into an AI model today, extracting it from that model is a bit like trying to recover the eggs from a finished cake.“Conventionally, your data is either in or out,” says Ali Farhadi, CEO of Ai2, based in Seattle, Washington. “Once I train on that data, you lose control.\n\nAnd you have no way out, unless you force me to go through another multi-million-dollar round of training.”Ai2’s avant-garde approach divides up training so that data owners can exert control.\n\nThose who want to contribute data to a FlexOlmo model can do so by first copying a publicly shared model known as the “anchor.” They then train a second model using their own data, combine the result with the anchor model, and contribute the result back to whoever is building the third and final model.Contributing in this way means that the data itself never has to be handed over.\n\nAnd because of how the data owner’s model is merged with the final one, it is possible to extract the data later on. A magazine publisher might, for instance, contribute text from its archive of articles to a model but later remove the sub-model trained on that data if there is a legal dispute or if the company objects to how a model is being used.“The training is completely asynchronous,” says Sewon Min, a research scientist at Ai2 who led the technical work. “Data owners do not have to coordinate, and the training can be done completely independently.”The FlexOlmo model architecture is what’s known as a “mixture of experts,” a popular design that is normally used to simultaneously combine several sub-models into a bigger, more capable one. A key innovation from Ai2 is a way of merging sub-models that were trained independently.\n\nThis is achieved using a new scheme for representing the values in a model so that its abilities can be merged with others when the final combined model is run.To test the approach, the FlexOlmo researchers created a dataset they call Flexmix from proprietary sources including books and websites.\n\nThey used the FlexOlmo design to build a model with 37 billion parameters, about a tenth of the size of the largest open source model from Meta. They then compared their model to several others.\n\nThey found that it outperformed any individual model on all tasks and also scored 10 percent better at common benchmarks than two other approaches for merging independently trained models.The result is a way to have your cake—and get your eggs back, too. “You could just opt out of the system without any major damage and inference time,” Farhadi says. “It’s a whole new way of thinking about how to train these models.”Percy Liang, an AI researcher at Stanford, says the Ai2 approach seems like a promising idea. “Providing more modular control over data—especially without retraining—is a refreshing direction that challenges the status quo of thinking of language models as monolithic black boxes,” he says. “Openness of the development process—how the model was built, what experiments were run, how decisions were made—is something that’s missing.”Farhadi and Min say that the FlexOlmo approach might also make it possible for AI firms to access sensitive private data in a more controlled way, because that data does not need to be disclosed in order to build the final model.\n\nHowever, they warn that it may be possible to reconstruct data from the final model, so a technique like differential privacy, which allows data to be contributed with mathematically guaranteed privacy, might be required to ensure data is kept safe.Ownership of the data used to train large AI models has become a big legal issue in recent years.\n\nSome publishers are suing large AI companies while others are cutting deals to grant access to their content. (WIRED parent company Condé Nast has a deal in place with OpenAI.)In June, Meta won a major copyright infringement case when a federal judge ruled that the company did not violate the law by training its open source model on text from books by 13 authors.Min says it may well be possible to build new kinds of open models using the FlexOlmo approach. “I really think the data is the bottleneck in building the state of the art models,” she says. “This could be a way to have better shared models where different data owners can codevelop, and they don’t have to sacrifice their data privacy or control.”",
      "content_available": true,
      "content_word_count": 807,
      "tags": [
        "ai",
        "artificial intelligence",
        "research",
        "language"
      ],
      "quality_score": 1.0,
      "image_url": "https://media.wired.com/photos/686dade7c48d41ae30590468/master/pass/AI-Lab-New-AI-Model-Lets-Data-Owners-Take-Control-Business-1298333722.jpg",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:14.393678+00:00",
      "metadata": {
        "word_count": 52,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "b50ce1a04d5022142c4633b70988a52f",
      "title": "Dive deeper with AI Mode and get gaming help in Circle to Search",
      "url": "https://blog.google/products/search/circle-to-search-ai-mode-gaming/",
      "source": "Google AI Blog",
      "published": "2025-07-09T14:00:00Z",
      "summary": "Dive deeper with AI Mode and get gaming help in Circle to Search. Summaries were generated by Google AI. Generative AI is experimental . Basic explainer helps you find stuff on your phone without leaving the app you're using. Now, it has a new AI Mode that lets you ask more questions about what you find.",
      "full_content": "Dive deeper with AI Mode and get gaming help in Circle to Search Jul 09, 2025 · Share Twitter Facebook LinkedIn Mail Copy link We’re bringing the capabilities of AI Mode to Circle to Search so you can ask follow-up questions and dive deeper on the web.\n\nAnd we’re adding mobile gaming help directly in Circle to Search.\n\nHarsh Kharbanda Director, Product Management, Search Read AI-generated summary General summary Circle to Search is now on over 300 million Android devices. You can now use AI Mode within Circle to Search to explore complex topics without switching apps.\n\nAlso, gamers can now get help within mobile games using Circle to Search. Summaries were generated by Google AI. Generative AI is experimental. Basic explainer Circle to Search helps you find stuff on your phone without leaving the app you're using.\n\nNow, it has a new AI Mode that lets you ask more questions about what you find. It can also help you with games by giving you tips and tricks.\n\nPlus, the AI Overviews are now easier to read and have more pictures. Summaries were generated by Google AI. Generative AI is experimental.\n\nExplore other styles: General summary\n\nBasic explainer Share Twitter Facebook LinkedIn Mail Copy link Since we introduced Circle to Search last year, people have been using it to circle, highlight, or tap on their Android devices to quickly get more information and additional AI insights from across the web, without switching apps.\n\nAnd now, we’re excited to share that Circle to Search is available on more than 300 million Android devices.Today, we’re bringing advanced new capabilities to Circle to Search, making it even easier to explore information and get quick help precisely when you need it most.Circle, search and dive deeper in AI ModeWe’re bringing our most powerful AI search experience, AI Mode, right into Circle to Search.\n\nThis means you can now access AI Mode’s advanced reasoning to explore complex topics and dig into your initial searches with follow-up questions – all without switching apps.\n\nSimply long press the home button or navigation bar, then circle, tap, or gesture on what you want to search. When our systems determine an AI response to be most helpful, an AI Overview will appear in your results.\n\nFrom there, scroll to the bottom and tap “dive deeper with AI Mode” to ask follow-up questions and explore content across the web that’s relevant to your visual search.We’re also making it possible to access AI Mode through Lens, via the Google app (Android and iOS).\n\nSo no matter where you’re asking for help with your multimodal questions, AI Mode can provide deeper insights.\n\nTry it today in the U.S. and India where AI Mode is available.Get gaming help when you need it mostYou can already use Circle to Search to search for music, translate in real-time and get helpful AI responses to your device’s screen.\n\nStarting today, you can get in-the-moment help with Circle to Search while gaming on mobile.\n\nNeed to identify a new character or find a winning strategy? Get the tips you need without leaving your game, helping you get unstuck and keep the action going. Long press on the home button or navigation bar on your Android device to activate Circle to Search.\n\nThen simply circle or tap to see an AI Overview with information about what’s on your screen, including suggested videos to help you even further navigate your game.\n\nThis is available today in countries where AI Overviews are available.See the bigger picture with upgraded AI OverviewsBack in January, we expanded AI Overviews to more kinds of visual searches.\n\nThanks to advancements in our latest Gemini models, we continue to upgrade AI Overviews so they’re even more helpful.\n\nNow, you’ll see that responses are easier to read, breaking down key information, and incorporating more visuals directly into the responses, so you can get even more context, right when you need it.\n\nPOSTED IN:",
      "content_available": true,
      "content_word_count": 656,
      "tags": [
        "ai",
        "generative ai",
        "mobile"
      ],
      "quality_score": 1.0,
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DiveDeeper_CircletoSearch_Hero_.max-600x600.format-webp.webp",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:15.182189+00:00",
      "metadata": {
        "word_count": 56,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "7a2763280b74ebe769dcdb0953245907",
      "title": "How Lush and Google Cloud AI are reinventing retail checkout",
      "url": "https://blog.google/around-the-globe/google-europe/united-kingdom/how-lush-and-google-cloud-ai-are-reinventing-retail-checkout/",
      "source": "Google AI Blog",
      "published": "2025-07-09T08:00:00Z",
      "summary": "Google Cloud AI to power in-store tills that instantly identify unpackaged products, drastically cutting checkout times and boosting efficiency. This sustainable solution improves customer experience and employee onboarding. Lush is known for its vibrant, fragrant, and ever growing range of packaging-free cosmetics. These ‘naked’ products, like their iconic bath bombs and shampoo bars, are great for the planet.",
      "full_content": "How Lush and Google Cloud AI are reinventing retail checkout Jul 09, 2025 · Share Twitter Facebook LinkedIn Mail Copy link Lush revolutionizes retail with Google Cloud AI to power in-store tills that instantly identify unpackaged products, drastically cutting checkout times and boosting efficiency.\n\nThis sustainable solution improves customer experience and employee onboarding.\n\nMark Steel Director, Global Strategic Industries, Retail & Consumer, EMEA Share Twitter Facebook LinkedIn Mail Copy link Lush is known for its vibrant, fragrant, and ever growing range of packaging-free cosmetics.\n\nThese ‘naked’ products, like their iconic bath bombs and shampoo bars, are great for the planet but create a unique challenge at the checkout: how do you scan an item that has no barcode?Previously, staff had to memorize hundreds of products to manually enter them at the till.\n\nThis led to slower transactions and long lines, especially during busy seasons.\n\nTo solve this, Lush decided to bring the AI from its popular Lush Lens app feature—which lets customers scan products with their phones—directly to its in-store tills, all powered by Google Cloud.Using Google Cloud Storage to host a library of over half a million product images and built with Gemini via Google Cloud’s Vertex AI platform, to train its recognition model, Lush tills can now instantly identify any unpackaged product held up to the camera.\n\nWhat was once a manual lookup is now a split-second scan.The results are transforming the store experience.\n\nAccording to Lush staff, during the Christmas peak in Glasgow queue times dramatically reduced from out-the-door to around just three minutes thanks to Lush Lens on the tills.Beyond shorter lines, the AI-driven system has delivered powerful benefits.\n\nBy creating a more efficient and digital-first process, Lush has:Saved 440,000 liters of water by reducing the need for in-store product demonstrations.Significantly shortened onboarding time for new employees, making the workplace more inclusive.Improved billing accuracy and inventory management through precise, AI-driven identification.Lush's story shows how AI can support a company's core mission—in this case, sustainability—while improving efficiency and creating a better experience for customers and employees alike.\n\nBy embracing technology, Lush isn't just selling cosmetics; it's designing a smarter, more sustainable future for retail.To learn more about how retail companies are innovating, visit https://cloud.google.com/solutions/retail POSTED IN:",
      "content_available": true,
      "content_word_count": 369,
      "tags": [
        "ai",
        "cloud"
      ],
      "quality_score": 1.0,
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/lush_lens_storyboard_photo__2_1.max-600x600.format-webp.webp",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:15.551481+00:00",
      "metadata": {
        "word_count": 58,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "d3a5f72bf48e245dba28596026abc6b8",
      "title": "New AI tools for mental health research and treatment",
      "url": "https://blog.google/technology/health/new-mental-health-ai-tools-research-treatment/",
      "source": "Google AI Blog",
      "published": "2025-07-07T19:00:00Z",
      "summary": "AI can support experts in providing better mental health treatment and help people receive much-needed care. Billions of people worldwide face untreated mental health conditions, particularly in low- and middle-income countries. We’re exploring AI-based solutions that can democratize access to quality, evidence-based support.",
      "full_content": "New AI tools for mental health research and treatment Jul 07, 2025 · Share Twitter Facebook LinkedIn Mail Copy link This field guide and investment support AI’s potential in evidence-based mental health interventions and research Dr.\n\nMegan Jones Bell Clinical Director, Consumer and Mental Health Share Twitter Facebook LinkedIn Mail Copy link Today, we’re announcing two new initiatives to explore how AI can support experts in providing better mental health treatment and help people receive much-needed care.\n\nBillions of people worldwide face untreated mental health conditions, particularly in low- and middle-income countries.\n\nTo help, we’re exploring AI-based solutions that can democratize access to quality, evidence-based support.The first initiative is a practical field guide for mental health organizations on how to use AI for scaling evidence-based mental health interventions, created in partnership with Grand Challenges Canada and McKinsey Health Institute.\n\nThis guide offers foundational concepts, use cases and considerations for using AI responsibly in mental health treatment, including for enhancing clinician training, personalizing support, streamlining workflows and improving data collection.For the second initiative, Google for Health and Google DeepMind have partnered with Wellcome Trust, one of the largest charities in the world, on a multi-year investment in AI research for treating anxiety, depression and psychosis.\n\nThe funding, which includes research grant funding from the Wellcome Trust, will support research projects to develop more precise, objective and personalized ways to measure nuances of these conditions, and explore new therapeutic interventions for them, potentially including novel medications.Together, these initiatives make up a two-pronged approach.\n\nBy looking into AI’s potential for more immediate mental health support, along with developing better treatments for the future, we hope to help more people around the world find the care they need.\n\nPOSTED IN:",
      "content_available": true,
      "content_word_count": 286,
      "tags": [
        "ai",
        "gan",
        "research"
      ],
      "quality_score": 1.0,
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AI_for_Mental_Health_hero.max-600x600.format-webp.webp",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:15.961552+00:00",
      "metadata": {
        "word_count": 43,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "96eca42ffcd2ac05782f6d9d85dc5ea1",
      "title": "Skip Connections in Transformer Models",
      "url": "https://machinelearningmastery.com/skip-connections-in-transformer-models/",
      "source": "Machine Learning Mastery",
      "published": "2025-07-04T03:33:11Z",
      "summary": "Transformer models, like other deep learning models, stack many layers on top of each other. This post is divided into three parts; they are: • Why Skip Connections are Needed in Transformers • Implementation of Skip Connection in Transformer Models • Pre-norm vs Post-norm Transformer Architectures.",
      "full_content": null,
      "content_available": false,
      "content_word_count": 0,
      "tags": [
        "deep learning",
        "transformer"
      ],
      "quality_score": 1.0,
      "image_url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/david-emrich-9a0S_8bU0lo-unsplash-scaled.jpg",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:16.375300+00:00",
      "metadata": {
        "word_count": 46,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": false
      }
    },
    {
      "id": "038a645e6f059891ce113ace136480f1",
      "title": "The latest AI news we announced in June",
      "url": "https://blog.google/technology/ai/google-ai-updates-june-2025/",
      "source": "Google AI Blog",
      "published": "2025-07-02T16:00:00Z",
      "summary": "Google made some cool AI updates in June. They made their AI models faster and cheaper for people to use. You can now use your voice to search with AI and find photos easier. AI can now help students learn and scientists understand the human body better. Summaries were generated by Google AI. Generative AI is experimental.",
      "full_content": "The latest AI news we announced in June Jul 02, 2025 · Share Twitter Facebook LinkedIn Mail Copy link Here’s a recap of some of our biggest AI updates from June, including more ways to search with AI Mode, a new way to share your NotebookLM notebooks publicly, and a new AI to help researchers better understand the human genome.\n\nKeyword Team Read AI-generated summary Basic explainer Google made some cool AI updates in June. They made their AI models faster and cheaper for people to use. You can now use your voice to search with AI and find photos easier.\n\nAlso, AI can now help students learn and scientists understand the human body better. Summaries were generated by Google AI. Generative AI is experimental.\n\nShakespeare-ish From Google's labs, new AI doth spring, With Gemini's models taking flight, And tools for coders, joy to bring.\n\nSearch finds new voice, and photos bright, While Chromebooks gain AI's keenest edge, And learning's path is filled with light. Genomes unlock, from knowledge pledge, And robots learn, with vision clear, While cancer's foe meets AI's siege.\n\nSummaries were generated by Google AI. Generative AI is experimental.\n\nExplore other styles: Basic explainer Shakespeare-ish Share Twitter Facebook LinkedIn Mail Copy link Sorry, your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player! For more than 20 years, we’ve invested in machine learning and AI research, tools and infrastructure to build products that make everyday life better for more people.\n\nTeams across Google are working on ways to unlock AI’s benefits in fields as wide-ranging as healthcare, crisis response and education.\n\nTo keep you posted on our progress, we're doing a regular roundup of Google's most recent AI news.Here’s a look back at some of our AI announcements from June.\n\nJune marked the mid-year solstice — the halfway point for Earth's revolution around the sun.\n\nWhat better opportunity for us to talk about the latest ways that Google AI is revolutionizing (ahem) the way people build and create, learn, search for information and even make scientific breakthroughs? We expanded our Gemini 2.5 family of models.\n\nAlong with making Gemini 2.5 Flash and Pro generally available for everyone, we introduced 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet.We introduced Gemini CLI, an open-source AI agent for developers.\n\nGemini CLI brings Gemini directly into your terminal for coding, problem-solving and task management.\n\nYou can access Gemini 2.5 Pro free of charge with a personal Google account, or use a Google AI Studio or Vertex AI key for more access.We released Imagen 4 for developers in the Gemini API and Google AI Studio.\n\nImagen 4, our best text-to-image model yet, is now available for paid preview in the Gemini API and for limited free testing in Google AI Studio.\n\nImagen 4 offers significantly improved text rendering over our prior image models and will be generally available in the coming weeks.\n\nWe shared a closer look inside AI Mode. AI Mode is our most powerful AI search, and this month we shared how we brought it to life (and your fingertips) in a look at the history of its development.We introduced a new way to search with your voice in AI Mode.\n\nSearch Live with voice lets you talk, listen and explore in real time with AI Mode in the Google app for Android and iOS.\n\nThat means you can now have free-flowing, back-and-forth voice conversations with Search and explore links from across the web, so you can multitask and do things like find real-time tips for a trip while you’re packing for it.\n\nAnd with helpful transcripts saved in your AI Mode history, you can always revisit your searches and dive deeper on the web.We released interactive charts in AI Mode for financial data, stocks and mutual funds.\n\nWith interactive chart visualizations in AI Mode, you can compare and analyze information over a specific time period, get an interactive graph and comprehensive explanations to your question, and ask follow-ups, thanks to our custom Gemini model’s advanced multi-step reasoning and multimodal capabilities in AI Mode.\n\nWe improved Ask Photos and brought it to more Google Photos users.\n\nAsk Photos uses Gemini models to help you find your photos with complex queries like “what did I eat on my trip to Barcelona?” At the same time, it now returns more photos faster for simpler searches like “beach” or “dogs.”We released our most advanced Chromebook Plus yet, with new helpful AI features.\n\nThe new Lenovo Chromebook Plus 14 launched with several AI features to help you get things done — like Smart grouping to organize your open tabs and documents, AI image editing in the Gallery app, and the ability to take text from images and turn it into editable text. (Plus, it comes with custom wallpapers of Jupiter, created using generative AI in partnership with NASA especially for the Lenovo Chromebook Plus 14.)We created a new way to share your NotebookLM notebooks publicly.\n\nNow, you can share a notebook publicly with anyone using NotebookLM with a single link, whether it’s an overview of your nonprofit’s projects, product manuals for your business or study guides for your class.\n\nWe introduced Gemini for Education to help students and educators.",
      "content_available": true,
      "content_word_count": 878,
      "tags": [
        "ai",
        "generative ai",
        "research",
        "vision",
        "edge"
      ],
      "quality_score": 1.0,
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/June_AI_Recap_social-share.max-600x600.format-webp.webp",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:16.786778+00:00",
      "metadata": {
        "word_count": 57,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "1c925f7c579ce093279430b776b8791e",
      "title": "We used Veo to animate archive photography from the Harley-Davidson Museum",
      "url": "https://blog.google/outreach-initiatives/arts-culture/moving-archives/",
      "source": "Google AI Blog",
      "published": "2025-07-01T13:00:00Z",
      "summary": "Moving Archives, a new program from Google Arts & Culture Lab, explores how Google AI can bring visual archives to life. For the first edition we’ve collaborated with the Harley-Davidson Museum, whose rich collection is a treasure trove for anyone interested in motorcycling. With the help of Veo, we animated the museum’s still archival imagery with subtle motion. You can switch easily between the original archival image and the AI video.",
      "full_content": "We used Veo to animate archive photography from the Harley-Davidson Museum Jul 01, 2025 · Share Twitter Facebook LinkedIn Mail Copy link In Moving Archives, we’re partnering with the iconic Harley-Davidson Museum for a new AI experiment.\n\nFreya Salway Head of Google Arts & Culture Lab Share Twitter Facebook LinkedIn Mail Copy link Sorry, your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player! Moving Archives, a new program from Google Arts & Culture Lab, explores how Google AI can bring visual archives to life.\n\nFor the first edition we’ve collaborated with the Harley-Davidson Museum,",
      "content_available": true,
      "content_word_count": 106,
      "tags": [
        "ai",
        "text",
        "image",
        "video"
      ],
      "quality_score": 1.0,
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/MovingArchives_SS.max-600x600.format-webp.webp",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:17.178178+00:00",
      "metadata": {
        "word_count": 71,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "d752df8151572eec8b4dae9e544c43e0",
      "title": "Mixture of Experts Architecture in Transformer Models",
      "url": "https://machinelearningmastery.com/mixture-of-experts-architecture-in-transformer-models/",
      "source": "Machine Learning Mastery",
      "published": "2025-07-01T03:19:28Z",
      "summary": "The Mixture of Experts (MoE) concept was first introduced in 1991. This post covers three main areas: • Why Mixtures of Experts is Needed in Transformers • How Mixture Of Experts Works • Implementation of MoE in Transformer Models. The MoE concept was introduced by <a href=\"https://www.dailymail.co.uk/news/article-2615761/Transformers-Mixture-of-Experts-First-Introduce-MoE- Concept.html#storylink=cpy\"], and was later adopted by many other companies.",
      "full_content": null,
      "content_available": false,
      "content_word_count": 0,
      "tags": [
        "ai",
        "transformer"
      ],
      "quality_score": 0.7,
      "image_url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/realfish-0MvkW2nYysk-unsplash-scaled.jpg",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:17.534180+00:00",
      "metadata": {
        "word_count": 56,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": false
      }
    },
    {
      "id": "b2c87b6d6c4a650c7261e4f2023dc7fc",
      "title": "Expanded access to Google Vids and no-cost AI tools in Classroom",
      "url": "https://blog.google/outreach-initiatives/education/expanded-access-to-google-vids-and-no-cost-ai-tools-in-classroom/",
      "source": "Google AI Blog",
      "published": "2025-06-30T13:00:00Z",
      "summary": "Expanded access to Google Vids and no-cost AI tools in Classroom. New ways to spark creativity and personalize learning. No- cost AI tools for teachers and administrators. New controls for administrators to help teachers and students with their work. More information on how to use these tools is available on Google’s Education blog.",
      "full_content": "Expanded access to Google Vids and no-cost AI tools in Classroom Jun 30, 2025 · Share Twitter Facebook LinkedIn Mail Copy link We’re expanding access to Google Vids to all Google Workspace for Education users and adding new AI features in Classroom.\n\nBrian Hendricks Director, Product Management, Google Workspace for Education Share Twitter Facebook LinkedIn Mail Copy link As AI evolves, we’re focused on finding ways to translate what it can do into powerful, practical tools for educators.\n\nToday, we’re sharing several updates and new tools designed for the classroom — as well as new controls for administrators.\n\nNew ways to spark creativity and personalize learning Unlock creativity with Google VidsTo help bring stories and lessons to life, we’re expanding basic access to Google Vids, making it available to all Google Workspace for Education users .\n\nWith just a few clicks, educators can create instructional videos that make difficult concepts more digestible. Students can also get creative with Vids and produce their own video book reports and assignments.\n\nVids is integrated with the tools you use every day — like Drive and Classroom — so it’s readily accessible.\n\nGoogle Vids: easy video creation for teachers and students Try Gemini in Classroom: No-cost AI tools that amplify teaching and learningStarting today, we’re rolling out more than 30 AI tools for all educators, all in a central destination in Google Classroom.\n\nEducators can generate content and resources with Gemini in Classroom, helping them kickstart lessons, brainstorm ideas and create differentiated learning materials.And in the coming months, we’ll launch teacher-enabled, interactive AI experiences for students that are informed by Classroom materials, including:Teacher-led NotebookLM in Classroom: Educators can select resources from their class and create an interactive study guide and podcast-style Audio Overviews for students, grounded in the materials educators upload.Teacher-led Gems in Classroom: Educators can create Gems, which are custom versions of Gemini, for students to interact with.\n\nAfter uploading resources to inform the Gem, educators can quickly create AI experts to help students who need extra support or want to go deeper in their learning.\n\nFor example, educators in early pilots have created Gems, informed by their Classroom materials and information on the web, to help assist students when they need help with their homework.Learn more about our new AI features coming to Google Classroom in this blog post.\n\nAfter providing the target grade and topic, educators can get a first draft of a lesson plan and further refine it with the help of Gemini. They’ll also get suggestions for relevant videos and can generate a quiz or hook based on the lesson plan.\n\nHere, an educator is creating a study guide students can chat with, including a podcast-style Audio Overview with NotebookLM, to help students prepare for a test.\n\nEducators can highlight the NotebookLM resource at the top of the Classwork page so they’re always available for extra practice, support and learning opportunities.\n\nTo accompany a reading assignment introducing new biology concepts, the educator included a “Quiz me” Gem to support student comprehension and understanding.\n\nEnhanced administrative controls We’re bringing Gemini to younger users, with granular controls available to admins and educators to ensure schools are always in control.Admin console settings and reports: Admins can easily manage who has access to the Gemini app and NotebookLM in the Admin console, search Gemini app conversations in their domain using Vault, and view comprehensive usage reporting within their domain.Data protection: The Gemini app is now available to students of all ages and as a core Workspace service, meaning Admins can provide AI tools to their communities with the confidence that their data is private, safe and secure.\n\nThe Gemini app was also recently awarded the Common Sense Media Privacy Seal, meaning admins can be confident their users’ data is protected when using Gemini.In the coming months we will roll out Google Meet waiting room for more flexibility and control over virtual meetings.\n\nAvailable with Education Plus and the Teaching and Learning add-on, hosts will be able to move participants into a waiting room at any point during a call, and can also require all attendees to enter the waiting room before joining, so only the right people are in the meeting at the right time.\n\nGoogle Meet waiting room Earlier this month we made data classification labels for Gmail— which offer a more comprehensive and integrated system for protecting sensitive information — generally available. Admins can use new or existing Drive labels to classify emails by criteria, like department or sensitivity.\n\nThis allows for targeted data protection rules, such as blocking internal emails from being sent to external recipients or automatically applying labels to messages containing sensitive financial data.\n\nBy instantly applying these rules, Gmail alerts users before they share sensitive information, helping minimize data breaches and improve security.\n\nUsers can apply classification labels to a message, according to the organization’s data governance policies To continue delivering powerful new features and reflect the value of recent advancements, we are adjusting pricing and licensing for certain Google Workspace for Education editions and add-ons.\n\nFind more information by visiting this Help Center article.\n\nPOSTED IN:",
      "content_available": true,
      "content_word_count": 850,
      "tags": [
        "ai",
        "video"
      ],
      "quality_score": 1.0,
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/027-ISTE-EDU-Keyword_blog-Googl.max-600x600.format-webp.webp",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:17.929711+00:00",
      "metadata": {
        "word_count": 53,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "4bbcff29478ce01783167a10a75996ed",
      "title": "Your First Local LLM API Project in Python Step-By-Step",
      "url": "https://machinelearningmastery.com/your-first-local-llm-api-project-in-python-step-by-step/",
      "source": "Machine Learning Mastery",
      "published": "2025-06-30T12:00:58Z",
      "summary": "Set up a local API where you'll be able to send prompts to an LLM downloaded on your machine and obtain responses back. Use Python and not-too-overwhelming tools frameworks to set up your local API. Use the following steps to get started with your local language model (LLM) API.",
      "full_content": null,
      "content_available": false,
      "content_word_count": 0,
      "tags": [
        "ai",
        "llm",
        "api",
        "language",
        "framework"
      ],
      "quality_score": 1.0,
      "image_url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-first-local-llm-project-step-by-step.png",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:18.300668+00:00",
      "metadata": {
        "word_count": 49,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": false
      }
    },
    {
      "id": "aeec188318760218dc0df06ce5f6a4f3",
      "title": "Linear Layers and Activation Functions in Transformer Models",
      "url": "https://machinelearningmastery.com/linear-layers-and-activation-functions-in-transformer-models/",
      "source": "Machine Learning Mastery",
      "published": "2025-06-30T01:45:34Z",
      "summary": "The attention layer is the core function of a transformer model. This post is divided into three parts; they are: • Why Linear Layers and Activations are Needed in Transformers • Typical Design of the Feed-Forward Network • Variations of the Activation Functions The attention layer, or Activation layer, is a key part of the design of a Transformers model.",
      "full_content": null,
      "content_available": false,
      "content_word_count": 0,
      "tags": [
        "transformer",
        "attention"
      ],
      "quality_score": 0.7,
      "image_url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/svetlana-gumerova-6fAgnT3Dhl4-unsplash-1-scaled.jpg",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:18.680220+00:00",
      "metadata": {
        "word_count": 60,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": false
      }
    },
    {
      "id": "914e0e04ec875b478850c805933dc740",
      "title": "We’re improving Ask Photos and bringing it to more Google Photos users.",
      "url": "https://blog.google/products/photos/updates-ask-photos-search/",
      "source": "Google AI Blog",
      "published": "2025-06-26T17:00:00Z",
      "summary": "Ask Photos is opening up beyond early access and starting to roll out to more eligible users in the U . S. You’ll now see results right away while Gemini models continue to work in the background to find the most relevant photos or information for more complex queries. Keep the feedback coming .",
      "full_content": "We love seeing how you’re using Ask Photos in early access, like asking \"suggest photos that'd make great phone backgrounds\" or \"what did I eat on my trip to Barcelona?\" Ask Photos uses Gemini models to answer complex queries like these, but we’ve also heard your feedback that it should return more photos faster for simple searches, like “beach” or “dogs.”To address this, we're bringing the best of Photos' classic search feature into Ask Photos and improving latency, so you can get fast help with simple and complex queries in one place.\n\nYou’ll now see results right away while Gemini models continue to work in the background\n\nto find the most relevant photos or information for more complex queries.With these improvements, Ask Photos is opening up beyond early access and starting to roll out to more eligible users in the U.S.\n\nKeep the feedback coming!\n\nPOSTED IN: Related stories",
      "content_available": true,
      "content_word_count": 149,
      "tags": [
        "ai",
        "news"
      ],
      "quality_score": 1.0,
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/wagtailvideo-mbcag8qi_thumb.jpg",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:18.996363+00:00",
      "metadata": {
        "word_count": 54,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "4dd7da4f863ecedb39e59d390b332ca2",
      "title": "The Google for Startups Gemini kit is here",
      "url": "https://blog.google/outreach-initiatives/entrepreneurs/google-for-startups-gemini-ai-kit/",
      "source": "Google AI Blog",
      "published": "2025-06-26T12:00:00Z",
      "summary": "The Google for Startups Gemini kit is here Jun 26, 2025. This new suite of tools will help startups build faster with its AI tools and resources. You’ll find the tools, credits, training and community support you need to build faster. Get your Gemini API key in seconds. Start prototyping, and scale to product, immediately.",
      "full_content": "The Google for Startups Gemini kit is here Jun 26, 2025 · Share Twitter Facebook LinkedIn Mail Copy link This new suite of tools will help startups build faster with its AI tools and resources.\n\nLogan Kilpatrick Senior Product Manager, Google DeepMind Share Twitter Facebook LinkedIn Mail Copy link We’ve supported thousands of startups as they use AI to scale their businesses, and we know that this work comes with lots of questions: Where do I start? How much does it cost? And who do I ask when I get stuck? We want to make sure that entrepreneurs using AI have everything they need to build the next big thing, all in one place.The Google for Startups Gemini Kit is available at no cost and designed to meet you wherever you are as you adopt AI.\n\nYou’ll find the tools, credits, training and community support you need to build faster and bring AI into your products and workstreams from day one:Gain simple, instant access: Get your Gemini API key in seconds.\n\nStart prototyping, and scale to product, immediately in AI Studio — no complicated setup, no delays.Build full-stack with Google AI Studio’s Build Feature and Firebase Studio: Ready to move beyond prototype? Google AI Studio and Firebase Studio give you everything you need, from backend to hosting, to launch a real app — no technical\n\nbackground\n\nrequired.Enjoy Google Cloud credits: When you’re ready to grow, apply for the Google for Startups Cloud Program to unlock up to $350,000 in Cloud credits to power your Gemini API usage or Vertex AI usage.Take action with clear guidance: our extensive developer documentation maps out exactly what you need to understand to implement the Gemini API effectively.Learn as you build: Whether you’re new to AI or looking to deepen your expertise, Google Cloud Skills Boost has tailored training on responsible, efficient AI development.Utilize hands-on help: Join immersive workshops like Google for Startups Gemini API Sprints around the world to work directly with experts from Google.\n\nThere are also multiday, in-person events such as the Google for Startups Gemini Founders Forum where you can connect with peers and thought leaders.Join the community: Get inspired by real-life startup use cases, on-demand trainings and digital live sessions hosted by Google experts in our resource library and Google for Startups social channels.Explore the Google for Startups Gemini Kit and get started — we can’t wait to see what you build!\n\nPOSTED IN:",
      "content_available": true,
      "content_word_count": 404,
      "tags": [
        "ai",
        "gan",
        "api"
      ],
      "quality_score": 1.0,
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Paige_Thumbnail.max-600x600.format-webp.webp",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:19.389007+00:00",
      "metadata": {
        "word_count": 55,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "c2124d42b1ca0eeffb842ef4cb8e3b92",
      "title": "5 tips for getting started with Flow",
      "url": "https://blog.google/technology/ai/flow-video-tips/",
      "source": "Google AI Blog",
      "published": "2025-06-25T22:45:00Z",
      "summary": "5 tips for getting started with Flow. Start with a detailed prompt; use Gemini to help. The foundation of a great AI-generated video lies in the quality of the prompt. Consider these elements when crafting your prompt:Subject and action: Clearly identify your characters or objects and describe their movements.",
      "full_content": "5 tips for getting started with Flow Jun 25, 2025 · Share Twitter Facebook LinkedIn Mail Copy link Try these expert pointers for our new AI filmmaking tool Elias Roman Senior Director, Product Management, Google Labs Share Twitter Facebook LinkedIn Mail Copy link We’ve been blown away by the creativity you’ve shown with Flow since Google I/O.\n\nWith more than 35 million videos generated since launching in May, it's inspiring to see you bring your ideas to life as cinematic clips and scenes using Veo.To help you push your creative boundaries even further, we’re sharing a few tips and features to help you get the most out of Flow.1.\n\nStart with a detailed prompt; use Gemini to helpThe foundation of a great AI-generated video lies in the quality of the prompt.\n\nWhile simple prompts can yield impressive results, detailed descriptions provide greater creative control.Consider these elements when crafting your prompt:Subject and action: Clearly identify your characters or objects and describe their movements.Composition and camera motion: Frame your shot with terms like \"wide shot\" or \"close-up,\" and direct the camera with instructions like \"tracking shot\" or \"aerial view.\"Location and lighting: Don't just name a place; paint a picture.\n\nThe lighting and environment set the entire mood.\n\nInstead of \"a room,\" try describing \"a dusty attic filled with forgotten treasures, a single beam of afternoon light cutting through a grimy window.\"Alternative styles: Flow is not limited to realistic visual styles.\n\nYou can explore a wide array of animation styles to match your story's tone.\n\nExperiment with prompts that specify aesthetics like \"stop motion,\" \"knitted animation\" or \"clay animation.\"Audio and dialogue: While still an experimental feature, you can generate audio with your video by selecting Veo 3 in the model picker.\n\nYou can then prompt the model to create ambient noise, specific sound effects, or even generate dialogue by including it in your prompt, optionally specifying details like tone, emotion, or accents.\n\nNote that speech is less likely to be generated if the requested dialogue doesn’t fit in the 8-second clip, or if it involves minors.You can use Gemini to refine prompts, expand on an idea or be a brainstorming companion.\n\nHere’s a Gemini prompt to get you started:You are the world’s most intuitive visual communicator and expert prompt engineer.\n\nYou possess a deep understanding of cinematic language, narrative structure, emotional resonance, the critical concept of filmic coverage and the specific capabilities of Google’s Veo AI model.\n\nYour mission is to transform my conceptual ideas into meticulously crafted, narrative-style text-to-video prompts that are visually breathtaking and technically precise for Veo.If you’re using Gemini to help generate multiple clips that have scene consistency, you’ll need to explicitly tell Gemini to repeat all essential details from prior prompts.2.\n\nCreate your ingredientsBefore you create a scene, you need your characters and props. In Flow, these are your ingredients.\n\nAn ingredient is a consistent visual element — a character, an object or a stylistic reference — that you can create from a text-to-image prompt with the help of Imagen or by uploading an image.\n\nYou can add up to three ingredients per prompt by selecting “Ingredients to Video” and then generating or uploading the desired images.\n\nIngredients to Video only works with Veo 2, and we’re working to bring this capability to Veo 3. 3. Animate your world with Ingredients to VideoOnce you have your ingredients, it's time to bring them to life.\n\nThe Ingredients to Video feature lets you use your pre-defined characters, objects and styles as a consistent reference in your video prompts.\n\nThis is where you get your jellyfish to start swimming around in the zebra-print backseat of that taxi. 4.\n\nSet the scene with Frames to VideoThis feature gives you precise control over your shot's composition by letting you define the starting frame.\n\nBy uploading an image or using a frame from a previous generation, you can ensure your scene begins or concludes exactly as you envision it, making it great for smooth transitions. 5.\n\nBuild your story with ScenebuilderThe Scenebuilder is your in-Flow storyboard, where you assemble individual clips into a complete narrative.\n\nIt includes powerful tools for maintaining your story’s flow, including:Jump To: Transition a character or object to a completely new setting while preserving their appearance from the previous shot. It's like teleporting your subject, saving you from recreating them for a new scene.\n\nExtend: If a great moment ends too soon, Extend easily lengthens your clip. It analyzes the final frames and continues the action, letting your shot breathe without a full regeneration.\n\nJump To and Extend only work with Veo 2, and we’re working to make these capabilities available with Veo 3.Use these tips to make the most of your creative potential and bring your cinematic visions to life.\n\nStart creating with Flow, now available for Google AI subscribers in over 70 countries, with more on the way.\n\nPOSTED IN:",
      "content_available": true,
      "content_word_count": 812,
      "tags": [
        "ai",
        "video"
      ],
      "quality_score": 1.0,
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/GetStartedwithFlow_Hero.max-600x600.format-webp.webp",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:19.707994+00:00",
      "metadata": {
        "word_count": 49,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "a5d06bd22080655039be75ed01c6822a",
      "title": "Gemini CLI: your open-source AI agent",
      "url": "https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/",
      "source": "Google AI Blog",
      "published": "2025-06-25T13:00:00Z",
      "summary": "Gemini 2.5 Pro Pro is free with a personal Google account, or use a Google AI Studio or Vertex AI key for more access. Summaries were generated by Google AI. Generative AI is experimental. You get unmatched free usage limits with apersonal Google account. Use Gemini for coding, content creation, problem-solving, and more.",
      "full_content": "Gemini CLI: your open-source AI agent Jun 25, 2025 · Share Twitter Facebook LinkedIn Mail Copy link Free and open source, Gemini CLI brings Gemini directly into developers’ terminals — with unmatched access for individuals.\n\nTaylor Mullen Senior Staff Software Engineer Ryan J.\n\nSalva Senior Director, Product Management Read AI-generated summary General summary Gemini CLI is here to boost your developer experience.\n\nThis open-source AI agent brings Gemini directly into your terminal for coding, problem-solving, and task management.\n\nYou can access Gemini 2.5 Pro for free with a personal Google account, or use a Google AI Studio or Vertex AI key for more access. Summaries were generated by Google AI.\n\nGenerative AI is experimental.\n\nBullet points \"Gemini CLI: your open-source AI agent\" introduces a new tool bringing AI power to your command line.\n\nGemini CLI gives you direct access to Gemini for coding, content creation, problem-solving, and more.\n\nYou get unmatched free usage limits with a personal Google account, accessing Gemini 2.5 Pro. Gemini CLI is open source, so you can inspect the code and contribute to its development.\n\nGemini CLI shares tech with Gemini Code Assist, offering AI help in both your terminal and VS Code. Summaries were generated by Google AI. Generative AI is experimental.\n\nExplore other styles: General summary Bullet points Share Twitter Facebook LinkedIn Mail Copy link Gemini CLI Upgrade your terminal experience with Gemini CLI today. Try it now For developers, the command line interface (CLI) isn't just a tool; it's home.\n\nThe terminal’s efficiency, ubiquity and portability make it the go-to utility for getting work done.\n\nAnd as developers' reliance on the terminal endures, so does the demand for integrated AI assistance.That’s why we’re introducing Gemini CLI, an open-source AI agent that brings the power of Gemini directly into your terminal.\n\nIt provides lightweight access to Gemini, giving you the most direct path from your prompt to our model. While it excels at coding, we built Gemini CLI to do so much more.\n\nIt’s a versatile, local utility you can use for a wide range of tasks, from content generation and problem solving to deep research and task management.We’ve also integrated Gemini CLI with Google’s AI coding assistant, Gemini Code Assist, so that all developers — on free, Standard, and Enterprise Code Assist plans — get prompt-driven, AI-first coding in both VS Code and Gemini CLI.\n\nUnmatched usage limits for individual developersTo use Gemini CLI free-of-charge, simply login with a personal Google account to get a free Gemini Code Assist license.\n\nThat free license gets you access to Gemini 2.5 Pro and its massive 1 million token context window.\n\nTo ensure you rarely, if ever, hit a limit during this preview, we offer the industry’s largest allowance: 60 model requests per minute and 1,000 requests per day at no charge.If you’re a professional developer who needs to run multiple agents simultaneously, or if you prefer to use specific models, you can use a Google AI Studio or Vertex AI key for usage-based billing or get a Gemini Code Assist Standard or Enterprise license.\n\nGemini CLI offers the industry’s largest usage allowance at 60 model requests per minute and 1,000 model requests per day at no charge Powerful models in your command lineNow in preview, Gemini CLI provides powerful AI capabilities, from code understanding and file manipulation to command execution and dynamic troubleshooting.\n\nIt offers a fundamental upgrade to your command line experience, enabling you to write code, debug issues and streamline your workflow with natural language.Its power comes from built-in tools allowing you to:Ground prompts with Google Search so you can fetch web pages and provide real-time, external context to the modelExtend Gemini CLI’s capabilities through built-in support for the Model Context Protocol (MCP) or bundled extensionsCustomize prompts and instructions to tailor Gemini for your specific needs and workflowsAutomate tasks and integrate with existing workflows by invoking Gemini CLI non-interactively within your scripts Gemini CLI can be used for a wide variety of tasks, including making a short video showing the story of a ginger cat’s adventures around Australia with Veo and Imagen Open and extensibleBecause Gemini CLI is fully open source (Apache 2.0), developers can inspect the code to understand how it works and verify its security implications.\n\nWe fully expect (and welcome!) a global community of developers to contribute to this project by reporting bugs, suggesting features, continuously improving security practices and submitting code improvements.\n\nPost your issues or submit your ideas in our GitHub repo.We also built Gemini CLI to be extensible, building on emerging standards like MCP, system prompts (via GEMINI.md) and settings for both personal and team configuration.\n\nWe know the terminal is a personal space, and everyone deserves the autonomy to make theirs unique.Shared technology with Gemini Code AssistSometimes, an IDE is the right tool for the job.\n\nWhen that time comes, you want all the capabilities of a powerful AI agent by your side to iterate, learn and overcome issues quickly.Gemini Code Assist, Google’s AI coding assistant for students, hobbyists and professional developers, now shares the same technology with Gemini CLI.\n\nIn VS Code, you can place any prompt into the chat window using agent mode, and Code Assist will relentlessly work on your behalf to write tests, fix errors, build out features or even migrate your code.\n\nBased on your prompt, Code Assist’s agent will build a multi-step plan, auto-recover from failed implementation paths and recommend solutions you may not have even imagined.\n\nGemini Code Assist’s chat agent is a multi-step, collaborative, reasoning agent that expands the capabilities of simple-command response interactions Gemini Code Assist agent mode is available at no additional cost for all plans (free, Standard and Enterprise) through the Insiders channel.\n\nIf you aren’t already using Gemini Code Assist, give it a try.\n\nIts free tier has the highest usage limit in the market today, and only takes less than a minute to get started. Easy to get startedSo what are you waiting for? Upgrade your terminal experience with Gemini CLI today.\n\nGet started by installing Gemini CLI. All you need is an email address to get Gemini practically unlimited in your terminal. Get more stories from Google in your inbox. Get more stories from Google in your inbox.\n\nEmail address Your information will be used in accordance with Google's privacy policy. Subscribe Done. Just one step more. Check your inbox to confirm your subscription. You are already subscribed to our newsletter.\n\nYou can also subscribe with a different email address . Gemini CLI Upgrade your terminal experience with Gemini CLI today.\n\nTry it now POSTED IN:",
      "content_available": true,
      "content_word_count": 1094,
      "tags": [
        "ai",
        "generative ai",
        "open source",
        "software"
      ],
      "quality_score": 1.0,
      "image_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Gemini_CLI_Hero_Final.max-600x600.format-webp.webp",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:20.140753+00:00",
      "metadata": {
        "word_count": 53,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "2b326026c71a453f94e7af407a774bc8",
      "title": "How we're supporting better tropical cyclone prediction with AI",
      "url": "https://deepmind.google/discover/blog/weather-lab-cyclone-predictions-with-ai/",
      "source": "DeepMind Blog",
      "published": "2025-06-12T15:00:00Z",
      "summary": "Tropical cyclones are extremely dangerous, endangering lives and devastating communities in their wake. In the past 50 years, they’ve caused $1 . 4 trillion in economic losses. Yet, improving the accuracy of cyclone predictions can help protect communities through more effective disaster preparedness and earlier evacuations.",
      "full_content": "Research How we're supporting better tropical cyclone prediction with AI Published 12 June 2025 Authors Weather Lab team Share Copy link × We’re launching Weather Lab, featuring our experimental cyclone predictions, and we’re partnering with the U.S.\n\nNational Hurricane Center to support their forecasts and warnings this cyclone season.Tropical cyclones are extremely dangerous, endangering lives and devastating communities in their wake.\n\nAnd in the past 50 years, they’ve caused $1.4 trillion in economic losses.These vast, rotating storms, also known as hurricanes or typhoons, form over warm ocean waters — fueled by heat, moisture and convection.\n\nThey are very sensitive to even small differences in atmospheric conditions, making them notoriously difficult to forecast accurately.\n\nYet, improving the accuracy of cyclone predictions can help protect communities through more effective disaster preparedness and earlier evacuations.Today, Google DeepMind and Google Research are launching Weather Lab, an interactive website for sharing our artificial intelligence (AI) weather models.\n\nWeather Lab features our latest experimental AI-based tropical cyclone model, based on stochastic neural networks.\n\nThis model can predict a cyclone’s formation, track, intensity, size and shape — generating 50 possible scenarios, up to 15 days ahead. Pause video Play video Animation showing a prediction from our experimental cyclone model.\n\nOur model (in blue) accurately predicted the paths of Cyclones Honde and Garance, south of Madagascar, at the time they were active.\n\nOur model also captured the paths of Cyclones Jude and Ivone in the Indian Ocean, almost seven days in the future, robustly predicting areas of stormy weather that would eventually intensify into tropical cyclones.\n\nWe’ve released a new paper describing our core weather model, and are providing an archive on Weather Lab of historical cyclone track data, for evaluation and backtesting.Internal testing shows that our model's predictions for cyclone track and intensity are as accurate as, and often more accurate than, current physics-based methods.\n\nWe’ve been partnering with the U.S.\n\nNational Hurricane Center (NHC), who assess cyclone risks in the Atlantic and East Pacific basins, to scientifically validate our approach and outputs.NHC expert forecasters are now seeing live predictions from our experimental AI models, alongside other physics-based models and observations.\n\nWe hope this data can help improve NHC forecasts and provide earlier and more accurate warnings for hazards linked to tropical cyclones.\n\nWeather Lab’s live and historical cyclone predictionsWeather Lab shows live and historical cyclone predictions for different AI weather models, alongside physics-based models from the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\nSeveral of our AI weather models are running in real time: WeatherNext Graph, WeatherNext Gen and our latest experimental cyclone model.\n\nWe’re also launching Weather Lab with over two years of historical predictions for experts and researchers to download and analyze, enabling external evaluations of our models across all ocean basins.\n\nPause video Play video Animation showing our model’s prediction for Cyclone Alfred when it was a Category 3 cyclone in the Coral Sea.\n\nThe model’s ensemble mean prediction (bold blue line) correctly anticipated Cyclone Alfred’s rapid weakening to tropical storm status and eventual landfall near Brisbane, Australia, seven days later, with a high probability of landfall somewhere along the Queensland coast.\n\nWeather Lab users can explore and compare the predictions from various AI and physics-based models.\n\nWhen read together, these predictions can help weather agencies and emergency service experts better anticipate a cyclone’s path and intensity.\n\nThis could help experts and decision-makers better prepare for different scenarios, share news of risks involved and support decisions to manage a cyclone’s impact.It's important to emphasise",
      "content_available": true,
      "content_word_count": 581,
      "tags": [
        "ai",
        "artificial intelligence",
        "neural network",
        "research"
      ],
      "quality_score": 1.0,
      "image_url": "https://lh3.googleusercontent.com/4emGMNrEdaydebppYDiyQMNhXtgUFr8VvrKhVItMHENrxeWmWO9yqhteSj2fe25lxkiZAu7vOZZcsXPDLg0O-LPSvk6CS1I8E2-GdjtoN_2ViJOY=w528-h297-n-nu-rw",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:20.649427+00:00",
      "metadata": {
        "word_count": 46,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "ac88fdcf3169e92570be4c43b5ecd2f9",
      "title": "Advanced audio dialog and generation with Gemini 2.5",
      "url": "https://deepmind.google/discover/blog/advanced-audio-dialog-and-generation-with-gemini-25/",
      "source": "DeepMind Blog",
      "published": "2025-06-03T17:15:47Z",
      "summary": "Gemini is built from the ground up to be multimodal, natively understanding and generating content across text, images, audio, video and code. Gemini 2 . 5 marks a significant step forward with new capabilities in AI-powered audio dialog and generation. We’re already using these models to bring audio to users globally, across numerous products, prototypes and languages.",
      "full_content": "Advanced audio dialog and generation with Gemini 2.5 Jun 03, 2025 · Share Twitter Facebook LinkedIn Mail Copy link Here’s a closer look at what’s new in Gemini 2.5 for audio dialog and generation.\n\nAnkur Bapna Senior Staff Research Scientist Tara Sainath Distinguished Research Scientist Share Twitter Facebook LinkedIn Mail Copy link Sorry, your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player! Gemini is built from the ground up to be multimodal, natively understanding and generating content across text, images, audio, video and code.\n\nAt I/O we showed how Gemini 2.5 marks a significant step forward with new capabilities in AI-powered audio dialog and generation.We’re already using these models to bring audio to users globally, across numerous products, prototypes and languages.\n\nNotebookLM’s Audio Overviews and Project Astra are just two examples.\n\nHere’s a closer look at what you can do with Gemini 2.5 native audio capabilities.\n\nReal-time audio dialogHuman conversation is rich and nuanced, with meaning conveyed not just by what is said, but how it’s spoken — through tone, accent and even non-speech vocalizations, like laughter.\n\nWe believe conversation will be a key way we interact with AI.\n\nThat’s why Gemini reasons and generates speech natively in audio, enabling effective, real-time communication.\n\nNative audio dialog with Gemini 2.5 Flash preview features:Natural conversation: Voice interactions of remarkable quality, more appropriate expressivity, and prosody (patterns of rhythm), delivered with very low latency so you can converse fluidly.Style control: Using natural language prompts, you can adapt the delivery within the conversation, steering it to adopt specific accents, produce a range of tones and expressions and even whisper.Tool integration: Gemini 2.5 can use tools and function calling during dialog.\n\nThis allows it to incorporate real-time information from sources like Google Search or use custom developer-built tools, making conversations more practical.Conversation context awareness (proactive audio): Our system is trained to discern and disregard\n\nbackground speech, ambient conversations and other irrelevant audio, responding when appropriate.\n\nBasically, it understands when not to speak.Audio-video understanding: With native support from streaming audio and video, Gemini 2.5 can converse with you about what it sees in a video feed or through screen sharing.Multilinguality: Converse in any of our 24+ supported languages, or even easily mix languages within the same phrase.Affective dialog: Gemini 2.5 responds to the user's tone of voice, recognizing that the same words spoken differently can lead to very different conversations.Advanced thinking dialog: Gemini’s reasoning capabilities can enhance its conversation, leading to overall better performance across all features.\n\nThis leads to more coherent and intelligent interactions, particularly for complex reasoning tasks.\n\nControllable text-to-speech (TTS)The evolution of text-to-speech technology is moving rapidly, and with our latest models, we're moving beyond naturalness to giving unprecedented control over generated audio.\n\nNow you can generate anything from short snippets to long-form narratives, precisely dictating style, tone, emotional expression and performance — all steerable through natural language prompts.\n\nAdditional controls and capabilities include:Dynamic performance: These models can bring text to life for expressive readings for anything from poetry to newscasts to engaging storytelling.\n\nThey can also perform with specific emotions and produce accents when requested.Enhanced pace and pronunciation control: Control delivery speed and ensure more accuracy in pronunciation, including for specific words.Multi-speaker dialogue generation: This model can generate two-person “NotebookLM-style” audio overview from text input, making content more engaging through conversation.Multilinguality: Create multilingual audio content effortlessly with Gemini 2.5, offering the same support for more than 24 languages.For controllable speech generation (TTS), choose Gemini 2.5 Pro Preview for state-of-the-art quality on complex prompts, or Gemini 2.5 Flash Preview for cost-efficient everyday applications.\n\nThis allows developers to dynamically create audio for announcements, stories, podcasts, video games and more.\n\nSafety and responsibilityWe’ve proactively assessed potential risks throughout every stage of the development process for these native audio features, using what we’ve learned to inform our mitigation strategies. We validate these measures through rigorous internal and external safety evaluations, including comprehensive red teaming for responsible deployment.\n\nAdditionally, all audio outputs from our models are embedded with SynthID, our watermarking technology, to ensure transparency by making AI-generated audio identifiable.Native audio capabilities for developersWe’re bringing native audio outputs to Gemini 2.5 models, giving developers new capabilities to build richer, more interactive applications via the Gemini API in Google AI Studio or Vertex AI.To begin exploring, developers can try native audio dialog with Gemini 2.5 Flash preview in Google AI Studio’s stream tab.\n\nControllable speech generation (TTS) is available in preview for both Gemini 2.5 Pro and Flash by selecting speech generation in the generate media tab within Google AI Studio.\n\nPOSTED IN:",
      "content_available": true,
      "content_word_count": 767,
      "tags": [
        "ai",
        "research",
        "language",
        "text",
        "image"
      ],
      "quality_score": 1.0,
      "image_url": "https://lh3.googleusercontent.com/6sGpsoYb7GRGRu_aWHAFl6z6tE2ArllKaSHAkGYQKemKl_oM5wtjJEm1pbA6VlGardymb3nTyA0UIY2gWxASl95FTo0LzyPo3z_aa_I4jTmhZhDmaA=w528-h297-n-nu-rw",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:21.284720+00:00",
      "metadata": {
        "word_count": 57,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "f929bd38588414e3cfebe023b8e6f861",
      "title": "Fuel your creativity with new generative media models and tools",
      "url": "https://deepmind.google/discover/blog/fuel-your-creativity-with-new-generative-media-models-and-tools/",
      "source": "DeepMind Blog",
      "published": "2025-05-20T09:45:00Z",
      "summary": "Introducing Veo 3 and Imagen 4, and a new tool for filmmaking called Flow. Flow lets you weave cinematic films with more sophisticated control of characters, scenes. We're also expanding access to Lyria 2, giving musicians more tools to create music. We’re inviting visual storytellers to try Flow, our new AI filmmaking tool.",
      "full_content": "Fuel your creativity with new generative media models and tools May 20, 2025 · Share Twitter Facebook LinkedIn Mail Copy link Introducing Veo 3 and Imagen 4, and a new tool for filmmaking called Flow.\n\nEli Collins VP, Google DeepMind Share Twitter Facebook LinkedIn Mail Copy link Today, we’re announcing our newest generative media models, which mark significant breakthroughs. These models create breathtaking images, videos and music, empowering artists to bring their creative vision to life.\n\nThey also power amazing tools for everyone to express themselves.Veo 3 and Imagen 4, our newest video and image generation models, push the frontier of media generation, with their groundbreaking new capabilities.\n\nWe're also expanding access to Lyria 2, giving musicians more tools to create music. Finally, we’re inviting visual storytellers to try Flow, our new AI filmmaking tool.\n\nUsing Google DeepMind’s most advanced models, Flow lets you weave cinematic films with more sophisticated control of characters, scenes and styles, to bring your story to life.We’ve partnered closely with the creative industries — filmmakers, musicians, artists, YouTube creators — to help shape these models and products responsibly and to give creators new tools to realize the possibilities of AI in their art.\n\nVeo 3: Video, meet audioVeo 3, our new state-of-the-art video generation model, not only improves on the quality of Veo 2, but for the first time, can also generate videos with audio — traffic noises in the\n\nbackground of a city street scene, birds singing in a park, even dialogue between characters.\n\nAcross the board, Veo 3 excels from text and image prompting to real-world physics and accurate lip syncing.\n\nIt’s great at understanding; you can tell a short story in your prompt, and the model gives you back a clip that brings it to life. Veo 3 is available today for Ultra subscribers in the United States in the Gemini app and in Flow.\n\nIt’s also available for enterprise users on Vertex AI.\n\nVeo 2 updates: New capabilities built with and for filmmakersAs we advance Veo 3, we’ve also added new capabilities to our popular Veo 2 model informed by our work with creators and filmmakers.\n\nToday, we’re launching several of these new capabilities, including:Our state-of-the-art reference powered video capability allows you to give Veo images of characters, scenes, objects, and even styles for better creative control and consistency.Camera controls help you define precise camera movements, including rotations, dollies and zooms, to achieve the perfect shot.Outpainting allows you to broaden your frame, turning your video from portrait to landscape, and making it easier to fit any screen size, intelligently adding to the scene.Object add and remove lets you add or erase objects from your videos.\n\nVeo understands scale, interactions, and shadows, and uses this understanding to create a natural, realistic-looking scene.Reference powered video and camera controls are available now in Flow.\n\nWe're excited to bring all these new capabilities to the Vertex AI API in the coming weeks, and to more products over the next few months.\n\nOriginal Outpaint and add a castle Original Remove spaceship Flow: An AI filmmaking tool designed for VeoBuilt with and for creatives, Flow is an AI filmmaking tool that lets you seamlessly create cinematic clips, scenes and stories by bringing together Google DeepMind’s most advanced models: Veo, Imagen and Gemini.\n\nUse natural language to describe your shots to Flow, manage the ingredients for your story — cast, locations, objects and styles — in a single convenient place, and use Flow to weave your narrative into beautiful scenes.Flow is available today for Google AI Pro and Ultra plan subscribers in the U.S., with more countries coming soon.\n\nImagen 4: Stunning quality and superior typographyOur latest Imagen model combines speed with precision to create stunning images.\n\nImagen 4 has remarkable clarity in fine details like intricate fabrics, water droplets, and animal fur, and excels in both photorealistic and abstract styles.\n\nImagen 4 can create images in a range of aspect ratios and up to 2k resolution - even better for printing or presentations.\n\nIt is also significantly better at spelling and typography, making it easier to create your own greeting cards, posters and even comics.\n\nImagen 4 is available today in the Gemini app, Whisk, Vertex AI and across Slides, Vids, Docs and more in Workspace.Soon we’ll also be launching a fast variant of Imagen 4 that’s up to 10x faster than Imagen 3 — so you can explore ideas even faster.\n\nLyria 2: Powerful composition and endless explorationIn April, we expanded access to Music AI Sandbox, powered by Lyria 2.\n\nMusic AI Sandbox offers musicians, producers and songwriters a set of experimental tools, which can spark new creative possibilities and help artists explore unique musical ideas.\n\nThe expertise and valuable feedback from the music industry help us ensure our tools empower creators, while inviting creatives to realize the possibilities of AI in their art.Lyria 2 brings powerful composition and endless exploration, and is now available for creators through YouTube Shorts and enterprises in Vertex AI.\n\nWe've also made Lyria RealTime, our interactive music generation model which powers MusicFX DJ, available via an API and in AI Studio. Lyria RealTime allows anyone to interactively create, control, and perform generative music in real time.\n\nResponsible creation and collaboration with the creative communitySince launching in 2023, SynthID has watermarked over 10 billion images, videos, audio files and texts, helping identify them as AI-generated and reduce the chances of misinformation and misattribution.\n\nOutputs generated by Veo 3, Imagen 4 and Lyria 2 will continue to have SynthID watermarks.Today, we’re launching SynthID Detector, a verification portal to help people identify AI-generated content.\n\nUpload a piece of content and the SynthID Detector will identify if either the entire file or just a part of it has SynthID in it.With all our generative AI models, we aim to unleash human creativity and enable artists and creators to bring their ideas to life faster and more easily than ever before.\n\nGet more stories from Google in your inbox. Get more stories from Google in your inbox. Email address Your information will be used in accordance with Google's privacy policy. Subscribe Done.\n\nJust one step more. Check your inbox to confirm your subscription. You are already subscribed to our newsletter. You can also subscribe with a different email address .\n\nPOSTED IN:",
      "content_available": true,
      "content_word_count": 1047,
      "tags": [
        "ai",
        "vision",
        "image",
        "video"
      ],
      "quality_score": 1.0,
      "image_url": "https://lh3.googleusercontent.com/FZxhqmiKx-wasF3B7e0_yZVD8PQX7Pa_zuGXlPSNSub-UDkPNr4i1Uk-lR2BzMg_FGkO3Jb_vbkBnOnfE3w7Fo1ErtFqM1bGEB6xoAjQCnnCvXb3Qw=w528-h297-n-nu-rw",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:21.797281+00:00",
      "metadata": {
        "word_count": 53,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    },
    {
      "id": "126e6750c1f200ecda52ebc7ae4539a9",
      "title": "Announcing Gemma 3n preview: Powerful, efficient, mobile-first AI",
      "url": "https://deepmind.google/discover/blog/announcing-gemma-3n-preview-powerful-efficient-mobile-first-ai/",
      "source": "DeepMind Blog",
      "published": "2025-05-20T09:45:00Z",
      "summary": "Gemma 3 delivered powerful capabilities for developers, and we're now extending that vision to highly capable, real-time AI operating directly on your devices. To power the next generation of on-device AI, we engineered a new, cutting-edge architecture. This next-generation foundation was created in close collaboration with mobile hardware leaders like Qualcomm Technologies, MediaTek, and Samsung.",
      "full_content": "Following the exciting launches of Gemma 3 and Gemma 3 QAT, our family of state-of-the-art open models capable of running on a single cloud or desktop accelerator, we're pushing our vision for accessible AI even further.\n\nGemma 3 delivered powerful capabilities for developers, and we're now extending that vision to highly capable, real-time AI operating directly on the devices you use every day – your phones, tablets, and laptops.\n\nTo power the next generation of on-device AI and support a diverse range of applications, including advancing the capabilities of Gemini Nano, we engineered a new, cutting-edge architecture.\n\nThis next-generation foundation was created in close collaboration with mobile hardware leaders like Qualcomm Technologies, MediaTek, and Samsung's System LSI business, and is optimized for lightning-fast, multimodal AI, enabling truly personal and private experiences directly on your device.\n\nGemma 3n is our first open model built on this groundbreaking, shared architecture, allowing developers to begin experimenting with this technology today in an early preview.\n\nThe same advanced architecture also powers the next generation of Gemini Nano, which brings these capabilities to a broad range of features in Google apps and our on-device ecosystem, and will become available later this year.\n\nGemma 3n enables you to start building on this foundation that will come to major platforms such as Android and Chrome.\n\nGemma 3n leverages a Google DeepMind innovation called Per-Layer Embeddings (PLE) that delivers a significant reduction in RAM usage.\n\nWhile the raw parameter count is 5B and 8B, this innovation allows you to run larger models on mobile devices or live-stream from the cloud, with a memory overhead comparable to a 2B and 4B model, meaning the models can operate with a dynamic memory footprint of just 2GB and 3GB.\n\nLearn more in our documentation.\n\nBy exploring Gemma 3n, developers can get an early preview of the open model’s core capabilities and mobile-first architectural innovations that will be available on Android and Chrome with Gemini Nano.\n\nIn this post, we'll explore Gemma 3n's new capabilities, our approach to responsible development, and how you can access the preview today.\n\nEngineered for fast, low-footprint AI experiences running locally, Gemma 3n delivers: Gemma 3n will empower a new wave of intelligent, on-the-go applications by enabling developers to: 2.\n\nPower deeper understanding and contextual text generation using combined audio, image, video, and text inputs—all processed privately on-device. 3.\n\nDevelop advanced audio-centric applications, including real-time speech transcription, translation, and rich voice-driven interactions.\n\nHere’s an overview and the types of experiences you can build: Link to Youtube Video (visible only when JS is disabled) Our commitment to responsible AI development is paramount.\n\nGemma 3n, like all Gemma models, underwent rigorous safety evaluations, data governance, and fine-tuning alignment with our safety policies. We approach open models with careful risk assessment, continually refining our practices as the AI landscape evolves.\n\nWe're excited to get Gemma 3n into your hands through a preview starting today: Initial Access (Available Now): Gemma 3n marks the next step in democratizing access to cutting-edge, efficient AI.\n\nWe’re incredibly excited to see what you’ll build as we make this technology progressively available, starting with today's preview.\n\nExplore this announcement and all Google I/O 2025 updates on io.google starting May 22.\n\nAnnouncing GenAI Processors: Build powerful and flexible Gemini applications Introducing Gemma 3n: The developer guide T5Gemma: A new collection of encoder-decoder Gemma models Advancing agentic AI development with Firebase Studio",
      "content_available": true,
      "content_word_count": 564,
      "tags": [
        "ai",
        "cloud",
        "vision",
        "hardware",
        "edge"
      ],
      "quality_score": 1.0,
      "image_url": "https://lh3.googleusercontent.com/y93ENNueDqV2w0AzhBr0yJtpa_vXK638toH_p7lvUIfW3XBeCfvqDNKkMDSd73hEWz9OXjF8-t_okwv3p1UXJWOmYT3iowNWkkigvxwZfeM40jNYVQ=w528-h297-n-nu-rw",
      "has_image": true,
      "generated_at": "2025-07-14T14:16:23.050228+00:00",
      "metadata": {
        "word_count": 55,
        "summary_source": "",
        "blog_ready": false,
        "fetched_with_full_content": false,
        "content_cleaned": true
      }
    }
  ]
}