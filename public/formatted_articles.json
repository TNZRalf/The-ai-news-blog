[
  {
    "id": 1,
    "slug": "large-language-models-a-self-study-roadmap",
    "title": "Large Language Models: A Self-Study Roadmap",
    "description": "LLMs learn the rules of language, like grammar and meaning, which allows them to perform many tasks. They can answer questions, summarize long texts, and even create stories.",
    "content": "<p>Large Language Models: A Self-Study Roadmap A complete beginner’s roadmap to understanding and building with large language models explained simply and with hands-on resources. By Kanwal Mehreen, KDnuggets Technical Editor & Content Specialist on July 7, 2025 in Language Models Image by Author | Canva Large language models are a big step forward in artificial intelligence. They can predict and generate text that sounds like it was written by a human. LLMs learn the rules of language, like grammar and meaning, which allows them to perform many tasks. They can answer questions, summarize long texts, and even create stories. The growing need for automatically generated and organized content is driving the expansion of the large language model market. According to one report, Large Language Model (LLM) Market Size & Forecast: “The global LLM Market is currently witnessing robust growth, with estimates indicating a substantial increase in market size. Projections suggest a notable expansion in market value, from USD 6.4 billion in 2024 to USD 36.1 billion by 2030, reflecting a substantial CAGR of 33.2% over the forecast period” This means 2025 might be the best year to start learning LLMs. Learning advanced concepts of LLMs includes a structured, stepwise approach that includes concepts, models, training, and optimization as well as deployment and advanced retrieval methods. This roadmap presents a step-by-step method to gain expertise in LLMs. So, let's get started. Step 1: Cover the Fundamentals You can skip this step if you already know the basics of programming, machine learning, and natural language processing. However, if you are new to these concepts consider learning them from the following resources: Programming: You need to learn the basics of programming in Python, the most popular programming language for machine learning. These resources can help you learn Python: Learn Python - Full Course for Beginners [Tutorial] - YouTube (Recommended) Python Crash Course For Beginners - YouTube TEXTBOOK: Learn Python The Hard Way Machine Learning: After you learn programming, you have to cover the basic concepts of machine learning before moving on with LLMs. The key here is to focus on concepts like supervised vs. unsupervised learning, regression, classification, clustering, and model evaluation. The best course I found to learn the basics of ML is: Machine Learning Specialization by Andrew Ng | Coursera - It is a paid course that you can buy in case you need a certification, but fortunately, I have found it on YouTube for free too: Machine Learning by Professor Andrew Ng Natural Language Processing: It is very important to learn the fundamental topics of NLP if you want to learn LLMs. Focus on the key concepts: tokenization, word embeddings, attention mechanisms, etc. I have given a few resources that might help you learn NLP: Coursera: DeepLearning.AI Natural Language Processing Specialization - Focuses on NLP techniques and applications (Recommended) Stanford CS224n (YouTube): Natural Language Processing with Deep Learning - A comprehensive lecture series on NLP with deep learning. Step 2: Understand Core Architectures Behind Large Language Models Large language models rely on various architectures, with transformers being the most prominent foundation. Understanding these different architectural approaches is essential for working effectively with modern LLMs. Here are the key topics and resources to enhance your understanding: Understand transformer architecture and emphasize on understanding self-attention, multi-head attention, and positional encoding. Start with Attention Is All You Need, then explore different architectural variants: decoder-only models (GPT series), encoder-only models (BERT), and encoder-decoder models (T5, BART). Use libraries like Hugging Face's Transformers to access and implement various model architectures. Practice fine-tuning different architectures for specific tasks like classification, generation, and summarization. Recommended Learning Resources The Illustrated Transformer (Blog & Visual Guide): A must-read visual explanation of transformer models. Transformers Explained – Yannic Kilcher: Explaining the implementation of “Attention is All You Need”: An accessible breakdown of the transformer paper (Recommended). Language Models are Few-Shot Learners - Decoder-Only Architectures (GPT Series). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - T5. Hugging Face Tutorial (2024) - This comprehensive guide covers various NLP tasks, including building a sentiment analysis model with Hugging Face (Recommended). Fine-Tuning BERT for Text Classification - A video guide explaining how to adapt BERT for text classification using an example to classify phishing URLs (Recommended). Fine tuning gpt2 | Transformers huggingface | conversational chatbot | GPT2LMHeadModel - A video guide explaining the fine tuning of the gpt2 model to create a conversational chatbot. Step 3: Specializing in Large Language Models With the basics in place, it’s time to focus specifically on LLMs. These courses are designed to deepen your understanding of their architecture, ethical implications, and real-world applications: LLM University – Cohere (Recommended): Offers both a sequential track for newcomers and a non-sequential, application-driven path for seasoned professionals. It provides a structured exploration of both the theoretical and practical aspects of LLMs. Stanford CS324: Large Language Models (Recommended): A comprehensive course exploring the theory, ethics, and hands-on practice of LLMs. You will learn how to build and evaluate LLMs. Maxime Labonne Guide (Recommended): This guide provides a clear roadmap for two career paths: LLM Scientist and LLM Engineer. The LLM Scientist path is for those who want to build advanced language models using the latest techniques. The LLM Engineer path focuses on creating and deploying applications that use LLMs. It also includes The LLM Engineer’s Handbook, which takes you step by step from designing to launching LLM-based applications. Princeton COS597G: Understanding Large Language Models: A graduate-level course that covers models like BERT, GPT, T5, and more. It is Ideal for those aiming to engage in deep technical research, this course explores both the capabilities and limitations of LLMs. Fine Tuning LLM Models – Generative AI Course When working with LLMs, you will often need to fine-tune LLMs, so consider learning efficient fine-tuning techniques such as LoRA and QLoRA, as well as model quantization techniques. These approaches can help reduce model size and computational requirements while maintaining performance. This course will teach you fine-tuning using QLoRA and LoRA, as well as Quantization using LLama2, Gradient, and the Google Gemma model. Finetune LLMs to teach them ANYTHING with Huggingface and Pytorch | Step-by-step tutorial: It provides a comprehensive guide on fine-tuning LLMs using Hugging Face and PyTorch. It covers the entire process, from data preparation to model training and evaluation, enabling viewers to adapt LLMs for specific tasks or domains. Step 4: Build, Deploy & Operationalize LLM Applications Learning a concept theoretically is one thing; applying it practically is another. The former strengthens your understanding of fundamental ideas, while the latter enables you to translate those concepts into real-world solutions. This section focuses on integrating large language models into projects using popular frameworks, APIs, and best practices for deploying and managing LLMs in production and local environments. By mastering these tools, you'll efficiently build applications, scale deployments, and implement LLMOps strategies for monitoring, optimization, and maintenance. Application Development: Learn how to integrate LLMs into user-facing applications or services. LangChain: LangChain is the fast and efficient framework for LLM projects. Learn how to build applications using LangChain. API Integrations: Explore how to connect various APIs, like OpenAI’s, to add advanced features to your projects. Local LLM Deployment: Learn to set up and run LLMs on your local machine. LLMOps Practices: Learn the methodologies for deploying, monitoring, and maintaining LLMs in production environments. Recommended Learning Resources & Projects Building LLM applications: LangChain Crash Course For Beginners | LangChain Tutorial - A practical guide to building applications with LangChain. LangChain Master Class 2024 - Covers over 20 real-world use cases for LangChain.(Recommended) OpenAI Api Crash Course For Beginners | Financial Data Extraction Tool Using OpenAI API - This tutorial includes step-by-step instructions to use OpenAI API by building a project. (Recommended) Build your own LLM chatbot from scratch | End to End Gen AI | End to End LLM | Mistrak 7B LLM - It provides a comprehensive, step-by-step guide to creating a chatbot powered by the Mistral 7B Large Language Model (LLM). LLM Course – Build a Semantic Book Recommender (Python, OpenAI, LangChain, Gradio) - A detailed guide to building a semantic book recommender. Youtube Free Playlist Consisting of LLM End to End Projects - A free youtube playlist consisting of 20+ LLM end to end projects. (Recommended) Local LLM Deployment: How to Deploy an LLM on Your Own Machine - Deploy a Large Language Model locally, covering setup and integration. How to Run Any Open-Source Large Language Model Locally - Set up and run open-source LLMs on your local hardware. Foundations of Local Large Language Models - Offered by Duke University, this course teaches you how to set up a local environment to run various LLMs and interact with them via web interfaces and APIs. (Recommended) Beginning Llamafile for LLMs - Learn to serve large language models as production-ready web APIs using the llama.cpp framework. Containerizing LLM-Powered Apps: Chatbot Deployment – A step-by-step guide to deploying local LLMs with Docker. Deploying & Managing LLM applications In Production Environments: How to deploy LLMs (Large Language Models) as APIs using Hugging Face + AWS - Covers deploying LLMs as APIs in the cloud. LLMOps Instructional Video Series - A comprehensive 5-part series with live demonstrations in Azure AI Studio, guiding you through various aspects of LLMOps. Large Language Model Operations (LLMOps) Specialization - This coursera specialization offered by Duke university covers deploying, managing, and optimizing LLMs across platforms like Azure, AWS, Databricks, and local infrastructure. (Recommended) Simplify LLMOps & Build LLM Pipeline in Minutes - A tutorial teaching how to streamline LLMOps and construct efficient LLM pipelines using the Vext platform. GitHub Repositories: Awesome-LLM: It is a curated collection of papers, frameworks, tools, courses, tutorials, and resources focused on large language models (LLMs), with a special emphasis on ChatGPT. Awesome-langchain: This repository is the hub to track initiatives and projects related to LangChain's ecosystem. Step 5: RAG & Vector Databases Retrieval-Augmented Generation (RAG) is a hybrid approach that combines information retrieval with text generation. Instead of relying only on pre-trained knowledge, RAG retrieves relevant documents from external sources before generating responses. This improves accuracy, reduces hallucinations, and makes models more useful for knowledge-intensive tasks. Understand RAG & its Architectures: Standard RAG, Hierarchical RAG, Hybrid RAG etc. Vector Databases: Understand how to implement vector databases with RAG. Vector databases store and retrieve information based on semantic meaning rather than exact keyword matches. This makes them ideal for RAG-based applications as these allow for fast and efficient retrieval of relevant documents. Retrieval Strategies: Implement dense retrieval, sparse retrieval, and hybrid search for better document matching. LlamaIndex & LangChain: Learn how these frameworks facilitate RAG. Scaling RAG for Enterprise Applications: Understand distributed retrieval, caching, and latency optimizations for handling large-scale document retrieval. Recommended Learning Resources & Projects Basic Foundational courses: Vector Database: Faiss - Introduction to Similarity Search - This video covers the basics of FAISS and explains how it improves similarity search. Chroma - Vector Database for LLM Applications | OpenAI integration - Learn how ChromaDB can help you manage vector data for retrieval-based applications Learn RAG From Scratch – Python AI Tutorial from a LangChain Engineer - This Python course teaches you how to use RAG to integrate your own custom data with Large Language Models (LLMs). (Recommended) Introduction to LlamaIndex with Python (2024) - Covers how to use LlamaIndex for efficient data retrieval and integration with LLMs in Python. Introduction to Retrieval Augmented Generation (RAG) | Coursera - An introduction to RAG, teaching how to integrate external data with LLMs to enhance accuracy and reduce hallucinations. (Recommended) Advanced RAG Architectures & Implementations: Retrieval-Augmented Generation (RAG) Patterns and Best Practices -This video teaches different RAG architectures, patterns, and best practices for optimizing retrieval and generation processes. (Recommended) Fundamentals of AI Agents Using RAG and LangChain - Learn advanced RAG techniques, prompt engineering, and the use of LangChain for building AI agents. HybridRAG: Ultimate RAG Engine – Knowledge Graphs + Vector Retrieval – YouTube - Explores the integration of knowledge graphs with vector retrieval to create advanced RAG systems. (Recommended) Retrieval Augmented Generation LlamaIndex & LangChain Course - This course teaches how to build efficient RAG systems using modern tools, covering vector databases, embeddings, and real-world applications. Enterprise-Grade RAG & Scaling: RAG: Building enterprise ready retrieval-augmented generation applications - YouTube - This YouTube playlist provides a comprehensive guide to building and optimizing RAG systems, covering concepts, architectures, and practical implementations. Multimodal RAG using the Vertex AI Gemini API – Coursera - This project-based course teaches how to perform multimodal RAG using Google's Vertex AI Gemini API, focusing on enterprise-level applications. (Recommended) Learn Advanced RAG Tricks with Zain – YouTube - A session on advanced RAG techniques for enterprise applications. Step 6: Optimize LLM Inference Optimizing inference is crucial for making LLM-powered applications efficient, cost-effective, and scalable. This step focuses on techniques to reduce latency, improve response times, and minimize computational overhead. Key Topics Model Quantization: Reduce model size and improve speed using techniques like 8-bit and 4-bit quantization (e.g., GPTQ, AWQ). Efficient Serving: Deploy models efficiently with frameworks like vLLM, TGI (Text Generation Inference), and DeepSpeed. LoRA & QLoRA: Use parameter-efficient fine-tuning methods to enhance model performance without high resource costs. Batching & Caching: Optimize API calls and memory usage with batch processing and caching strategies. On-Device Inference: Run LLMs on edge devices using tools like GGUF (for llama.cpp) and optimized runtimes like ONNX and TensorRT. Recommended Learning Resources Efficiently Serving LLMs – Coursera - A guided project on optimizing and deploying large language models efficiently for real-world applications. Mastering LLM Inference Optimization: From Theory to Cost-Effective Deployment – YouTube - A tutorial discussing the challenges and solutions in LLM inference. It focuses on scalability, performance, and cost management. (Recommended) MIT 6.5940 Fall 2024 TinyML and Efficient Deep Learning Computing - It covers model compression, quantization, and optimization techniques to deploy deep learning models efficiently on resource-constrained devices. (Recommended) Inference Optimization Tutorial (KDD) – Making Models Run Faster – YouTube - A tutorial from the Amazon AWS team on methods to accelerate LLM runtime performance. Large Language Model inference with ONNX Runtime (Kunal Vaishnavi) - A guide on optimizing LLM inference using ONNX Runtime for faster and more efficient execution. Run Llama 2 Locally On CPU without GPU GGUF Quantized Models Colab Notebook Demo - A step-by-step tutorial on running LLaMA 2 models locally on a CPU using GGUF quantization. Tutorial on LLM Quantization w/ QLoRA, GPTQ and Llamacpp, LLama 2 - Covers various quantization techniques like QLoRA and GPTQ. Inference, Serving, PagedAtttention and vLLM - Explains inference optimization techniques, including PagedAttention and vLLM, to speed up LLM serving. Wrapping Up This guide covers a comprehensive roadmap to learning and mastering LLMs in 2025. I know it might seem overwhelming at first, but trust me — if you follow this step-by-step approach, you'll cover everything in no time. If you have any questions or need more help, do comment. Kanwal Mehreen Kanwal is a machine learning engineer and a technical writer with a profound passion for data science and the intersection of AI with medicine. She co-authored the ebook \"Maximizing Productivity with ChatGPT\". As a Google Generation Scholar 2022 for APAC, she champions diversity and academic excellence. She's also recognized as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded FEMCodes to empower women in STEM fields.More On This TopicLearn About Large Language ModelsIntroducing Healthcare-Specific Large Language Models from John Snow LabsWhat are Large Language Models and How Do They Work?AI: Large Language & Visual ModelsIntroducing TPU v4: Googles Cutting Edge Supercomputer for Large…More Free Courses on Large Language Models Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggets Privacy Policy Leave this field empty if you're human: <= Previous post Next post =></p>",
    "image": "/images/articles/cbf7ceb4529a430fd53be18328ec4b5d.png",
    "author": "Kanwal Mehreen",
    "date": "2025-07-07",
    "tags": [
      "ai",
      "artificial intelligence",
      "llm",
      "gan",
      "language"
    ],
    "status": "Published",
    "originalId": "cbf7ceb4529a430fd53be18328ec4b5d",
    "originalUrl": "https://www.kdnuggets.com/large-language-models-a-self-study-roadmap",
    "source": "KDnuggets",
    "qualityScore": 1,
    "wordCount": 2674
  },
  {
    "id": 2,
    "slug": "serve-machine-learning-models-via-rest-apis-in-under-10-minutes",
    "title": "Serve Machine Learning Models via REST APIs in Under 10 Minutes",
    "description": "Serve them to the world with this quick and powerful setup. For that, you need to serve it — expose it through a web API so that other programs (or humans) can send data and get predictions back.",
    "content": "<p>Serve Machine Learning Models via REST APIs in Under 10 Minutes Stop leaving your models on your laptop. Serve them to the world with this quick and powerful setup. By Kanwal Mehreen, KDnuggets Technical Editor & Content Specialist on July 4, 2025 in Machine Learning Image by Author | Canva If you like building machine learning models and experimenting with new stuff, that’s really cool — but to be honest, it only becomes useful to others once you make it available to them. For that, you need to serve it — expose it through a web API so that other programs (or humans) can send data and get predictions back. That’s where REST APIs come in. In this article, you will learn how we’ll go from a simple machine learning model to a production-ready API using FastAPI, one of Python’s fastest and most developer-friendly web frameworks, in just under 10 minutes. And we won’t just stop at a “make it run” demo, but we will add things like: Validating incoming data Logging every request Adding background tasks to avoid slowdowns Gracefully handling errors So, let me just quickly show you how our project structure is going to look before we move to the code part: ml-api/ │ ├── model/ │ └── train_model.py # Script to train and save the model │ └── iris_model.pkl # Trained model file │ ├── app/ │ └── main.py # FastAPI app │ └── schema.py # Input data schema using Pydantic │ ├── requirements.txt # All dependencies └── README.md # Optional documentation Step 1: Install What You Need We’ll need a few Python packages for this project: FastAPI for the API, Scikit-learn for the model, and a few helpers like joblib and pydantic. You can install them using pip: pip install fastapi uvicorn scikit-learn joblib pydantic And save your environment: pip freeze > requirements.txt Step 2: Train and Save a Simple Model Let’s keep the machine learning part simple so we can focus on serving the model. We’ll use the famous Iris dataset and train a random forest classifier to predict the type of iris flower based on its petal and sepal measurements. Here’s the training script. Create a file called train_model.py in a model/ directory: from sklearn.datasets import load_iris from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split import joblib, os X, y = load_iris(return_X_y=True) clf = RandomForestClassifier() clf.fit(*train_test_split(X, y, test_size=0.2, random_state=42)[:2]) os.makedirs(\"model\", exist_ok=True) joblib.dump(clf, \"model/iris_model.pkl\") print(\"✅ Model saved to model/iris_model.pkl\") This script loads the data, splits it, trains the model, and saves it using joblib. Run it once to generate the model file: python model/train_model.py Step 3: Define What Input Your API Should Expect Now we need to define how users will interact with your API. What should they send, and in what format? We'll use Pydantic, a built-in part of FastAPI, to create a schema that describes and validates incoming data. Specifically, we’ll ensure that users provide four positive float values — for sepal length/width and petal length/width. In a new file app/schema.py, add: from pydantic import BaseModel, Field class IrisInput(BaseModel): sepal_length: float = Field(..., gt=0, lt=10) sepal_width: float = Field(..., gt=0, lt=10) petal_length: float = Field(..., gt=0, lt=10) petal_width: float = Field(..., gt=0, lt=10) Here, we’ve added value constraints (greater than 0 and less than 10) to keep our inputs clean and realistic. Step 4: Create the API Now it’s time to build the actual API. We’ll use FastAPI to: Load the model Accept JSON input Predict the class and probabilities Log the request in the background Return a clean JSON response Let’s write the main API code inside app/main.py: from fastapi import FastAPI, HTTPException, BackgroundTasks from fastapi.responses import JSONResponse from app.schema import IrisInput import numpy as np, joblib, logging # Load the model model = joblib.load(\"model/iris_model.pkl\") # Set up logging logging.basicConfig(filename=\"api.log\", level=logging.INFO, format=\"%(asctime)s - %(message)s\") # Create the FastAPI app app = FastAPI() @app.post(\"/predict\") def predict(input_data: IrisInput, background_tasks: BackgroundTasks): try: # Format the input as a NumPy array data = np.array([[input_data.sepal_length, input_data.sepal_width, input_data.petal_length, input_data.petal_width]]) # Run prediction pred = model.predict(data)[0] proba = model.predict_proba(data)[0] species = [\"setosa\", \"versicolor\", \"virginica\"][pred] # Log in the background so it doesn’t block response background_tasks.add_task(log_request, input_data, species) # Return prediction and probabilities return { \"prediction\": species, \"class_index\": int(pred), \"probabilities\": { \"setosa\": float(proba[0]), \"versicolor\": float(proba[1]), \"virginica\": float(proba[2]) } } except Exception as e: logging.exception(\"Prediction failed\") raise HTTPException(status_code=500, detail=\"Internal error\") # Background logging task def log_request(data: IrisInput, prediction: str): logging.info(f\"Input: {data.dict()} | Prediction: {prediction}\") Let’s pause and understand what’s happening here. We load the model once when the app starts. When a user hits the /predict endpoint with valid JSON input, we convert that into a NumPy array, pass it through the model, and return the predicted class and probabilities. If something goes wrong, we log it and return a friendly error. Notice the BackgroundTasks part — this is a neat FastAPI feature that lets us do work after the response is sent (like saving logs). That keeps the API responsive and avoids delays. Step 5: Run Your API To launch the server, use uvicorn like this: uvicorn app.main:app --reload Visit: http://127.0.0.1:8000/docs You’ll see an interactive Swagger UI where you can test the API. Try this sample input: { \"sepal_length\": 6.1, \"sepal_width\": 2.8, \"petal_length\": 4.7, \"petal_width\": 1.2 } or you can use CURL to make the request like this: curl -X POST \"http://127.0.0.1:8000/predict\" -H \"Content-Type: application/json\" -d \\ '{ \"sepal_length\": 6.1, \"sepal_width\": 2.8, \"petal_length\": 4.7, \"petal_width\": 1.2 }' Both of the them generates the same response which is this: {\"prediction\":\"versicolor\", \"class_index\":1, \"probabilities\": { \"setosa\":0.0, \"versicolor\":1.0, \"virginica\":0.0 } } Optional Step: Deploy Your API You can deploy the FastAPI app on: Render.com (zero config deployment) Railway.app (for continuous integration) Heroku (via Docker) You can also extend this into a production-ready service by adding authentication (such as API keys or OAuth) to protect your endpoints, monitoring requests with Prometheus and Grafana, and using Redis or Celery for background job queues. You can also refer to my article : Step-by-Step Guide to Deploying Machine Learning Models with Docker. Wrapping Up That’s it — and it’s already better than most demos. What we’ve built is more than just a toy example. However, it: Validates input data automatically Returns meaningful responses with prediction confidence Logs every request to a file (api.log) Uses background tasks so the API stays fast and responsive Handles failures gracefully And all of it in under 100 lines of code. Kanwal Mehreen Kanwal is a machine learning engineer and a technical writer with a profound passion for data science and the intersection of AI with medicine. She co-authored the ebook \"Maximizing Productivity with ChatGPT\". As a Google Generation Scholar 2022 for APAC, she champions diversity and academic excellence. She's also recognized as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded FEMCodes to empower women in STEM fields.More On This TopicFastAPI Tutorial: Build APIs with Python in MinutesBuild a Data Cleaning & Validation Pipeline in Under 50 Lines of PythonTop 5 Machine Learning APIs Practitioners Should Know5 Machine Learning Models Explained in 5 Minutes3 APIs to Access Gemini 2.5 ProNew ChatGPT and Whisper APIs from OpenAI Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggets Privacy Policy Leave this field empty if you're human: <= Previous post Next post =></p>",
    "image": "/images/articles/83969c4674cfbed28a643d33e8c4ce08.png",
    "author": "Kanwal Mehreen",
    "date": "2025-07-04",
    "tags": [
      "ai",
      "machine learning",
      "api",
      "image",
      "framework"
    ],
    "status": "Published",
    "originalId": "83969c4674cfbed28a643d33e8c4ce08",
    "originalUrl": "https://www.kdnuggets.com/serve-machine-learning-models-via-rest-apis-in-under-10-minutes",
    "source": "KDnuggets",
    "qualityScore": 1,
    "wordCount": 1246
  },
  {
    "id": 3,
    "slug": "a-gentle-introduction-to-principal-component-analysis-pca-in-python",
    "title": "A Gentle Introduction to Principal Component Analysis (PCA) in Python",
    "description": "Hopefully, at this point I have convinced you about the practical relevance of PCA when handling complex data. /data\", train=True, download=True) data = [] for img, label in mnist_data: img_array = list(img.",
    "content": "<p>A Gentle Introduction to Principal Component Analysis (PCA) in Python The most popular method for feature reduction and data compression, gently explained via implementation with Scikit-learn in Python. By Iván Palomares Carrascosa, KDnuggets Technical Content Specialist on July 4, 2025 in Python Image by Author | Ideogram Principal component analysis (PCA) is one of the most popular techniques for reducing the dimensionality of high-dimensional data. This is an important data transformation process in various real-world scenarios and industries like image processing, finance, genetics, and machine learning applications where data contains many features that need to be analyzed more efficiently. The reasons for the significance of dimensionality reduction techniques like PCA are manifold, with three of them standing out: Efficiency: reducing the number of features in your data signifies a reduction in the computational cost of data-intensive processes like training advanced machine learning models. Interpretability: by projecting your data into a low-dimensional space, while keeping its key patterns and properties, it is easier to interpret and visualize in 2D and 3D, sometimes helping gain insight from its visualization. Noise reduction: often, high-dimensional data may contain redundant or noisy features that, when detected by methods like PCA, can be eliminated while preserving (or even improving) the effectiveness of subsequent analyses. Hopefully, at this point I have convinced you about the practical relevance of PCA when handling complex data. If that's the case, keep reading, as we'll start getting practical by learning how to use PCA in Python. How to Apply Principal Component Analysis in Python Thanks to supporting libraries like Scikit-learn that contain abstracted implementations of the PCA algorithm, using it on your data is relatively straightforward as long as the data are numerical, previously preprocessed, and free of missing values, with feature values being standardized to avoid issues like variance dominance. This is particularly important, since PCA is a deeply statistical method that relies on feature variances to determine principal components: new features derived from the original ones and orthogonal to each other. We will start our example of using PCA from scratch in Python by importing the necessary libraries, loading the MNIST dataset of low-resolution images of handwritten digits, and putting it into a Pandas DataFrame: import pandas as pd from torchvision import datasets mnist_data = datasets.MNIST(root=\"./data\", train=True, download=True) data = [] for img, label in mnist_data: img_array = list(img.getdata()) data.append([label] + img_array) columns = [\"label\"] + [f\"pixel_{i}\" for i in range(28*28)] mnist_data = pd.DataFrame(data, columns=columns) In the MNIST dataset, each instance is a 28x28 square image, with a total of 784 pixels, each containing a numerical code associated with its gray level, ranging from 0 for black (no intensity) to 255 for white (maximum intensity). These data must firstly be rearranged into a unidimensional array — rather than bidimensional as per its original 28x28 grid arrangement. This process called flattening takes place in the above code, with the final dataset in DataFrame format containing a total of 785 variables: one for each of the 784 pixels plus the label, indicating with an integer value between 0 and 9 the digit originally written in the image. MNIST Dataset | Source: TensorFlow In this example, we won't need the label — useful for other use cases like image classification — but we will assume we may need to keep it handy for future analysis, therefore we will separate it from the rest of the features associated with image pixels in a new variable: X = mnist_data.drop('label', axis=1) y = mnist_data.label Although we will not apply a supervised learning technique after PCA, we will assume we may need to do so in future analyses, hence we will split the dataset into training (80%) and testing (20%) subsets. There's another reason we are doing this, let me clarify it a bit later. from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.2, random_state=42) Preprocessing the data and making it suitable for the PCA algorithm is as important as applying the algorithm itself. In our example, preprocessing entails scaling the original pixel intensities in the MNIST dataset to a standardized range with a mean of 0 and a standard deviation of 1 so that all features have equal contribution to variance computations, avoiding dominance issues in certain features. To do this, we will use the StandardScaler class from sklearn.preprocessing, which standardizes numerical features: from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) Notice the use of fit_transform for the training data, whereas for the test data we used transform instead. This is the other reason why we previously split the data into training and test data, to have the opportunity to discuss this: in data transformations like standardization of numerical attributes, transformations across the training and test sets must be consistent. The fit_transform method is used on the training data because it calculates the necessary statistics that will guide the data transformation process from the training set (fitting), and then applies the transformation. Meanwhile, the transform method is utilized on the test data, which applies the same transformation \"learned\" from the training data to the test set. This ensures that the model sees the test data in the same target scale as that used for the training data, preserving consistency and avoiding issues like data leakage or bias. Now we can apply the PCA algorithm. In Scikit-learn's implementation, PCA takes an important argument: n_components. This hyperparameter determines the proportion of principal components to retain. Larger values closer to 1 mean retaining more components and capturing more variance in the original data, whereas lower values closer to 0 mean keeping fewer components and applying a more aggressive dimensionality reduction strategy. For example, setting n_components to 0.95 implies retaining sufficient components to capture 95% of the original data's variance, which may be appropriate for reducing the data's dimensionality while preserving most of its information. If after applying this setting the data dimensionality is significantly reduced, that means many of the original features did not contain much statistically relevant information. from sklearn.decomposition import PCA pca = PCA(n_components = 0.95) X_train_reduced = pca.fit_transform(X_train_scaled) X_train_reduced.shape Using the shape attribute of the resulting dataset after applying PCA, we can see that the dimensionality of the data has been drastically reduced from 784 features to just 325, while still keeping 95% of the important information. Is this a good result? Answering this question largely depends on the later application or type of analysis you want to perform with your reduced data. For instance, if you want to build an image classifier of digit images, you may want to build two classification models: one trained with the original, high-dimensional dataset, and one trained with the reduced dataset. If there is no significant loss of classification accuracy in your second classifier, good news: you achieved a faster classifier (dimensionality reduction normally implies greater efficiency in training and inference), and similar classification performance as if you were using the original data. Wrapping Up This article illustrated through a Python step-by-step tutorial how to apply the PCA algorithm from scratch, starting from a dataset of handwritten digit images with high dimensionality. Iván Palomares Carrascosa is a leader, writer, speaker, and adviser in AI, machine learning, deep learning & LLMs. He trains and guides others in harnessing AI in the real world.More On This TopicPrincipal Component Analysis (PCA) with Scikit-LearnA Gentle Introduction to Rust for Python ProgrammersA Gentle Introduction to Go for Python ProgrammersA Gentle Introduction to Support Vector MachinesA Gentle Introduction to Symbolic AIIntroduction to Python Libraries for Data Cleaning Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggets Privacy Policy Leave this field empty if you're human: <= Previous post Next post =></p>",
    "image": "/images/articles/57e110706bec31e8419dc4880d0bbad3.webp",
    "author": "Iván Palomares Carrascosa",
    "date": "2025-07-04",
    "tags": [
      "ai",
      "machine learning",
      "scikit-learn",
      "image",
      "interpretability"
    ],
    "status": "Published",
    "originalId": "57e110706bec31e8419dc4880d0bbad3",
    "originalUrl": "https://www.kdnuggets.com/gentle-introduction-principal-component-analysis-pca-in-python",
    "source": "KDnuggets",
    "qualityScore": 1,
    "wordCount": 1311
  },
  {
    "id": 4,
    "slug": "provider-of-covert-surveillance-app-spills-passwords-for-62000-users",
    "title": "Provider of covert surveillance app spills passwords for 62,000 users",
    "description": "A security flaw in the app, branded Catwatchful, allowed researcher Eric Daigle to download a trove of sensitive data, which belonged to account holders who used the covert app to monitor phones. Unstoppable Catwatchful creators emphasize the app's stealth and security.",
    "content": "<p>Text settings Story text Size Small Standard Large Width * Standard Wide Links Standard Orange * Subscribers only Learn more Minimize to nav The maker of a phone app that is advertised as providing a stealthy means for monitoring all activities on an Android device spilled email addresses, plain-text passwords, and other sensitive data belonging to 62,000 users, a researcher discovered recently. A security flaw in the app, branded Catwatchful, allowed researcher Eric Daigle to download a trove of sensitive data, which belonged to account holders who used the covert app to monitor phones. The leak, made possible by a SQL injection vulnerability, allowed anyone who exploited it to access the accounts and all data stored in them. Unstoppable Catwatchful creators emphasize the app's stealth and security. While the promoters claim the app is legal and intended for parents monitoring their children's online activities, the emphasis on stealth has raised concerns that it's being aimed at people with other agendas. \"Catwatchful is invisible,\" a page promoting the app says. \"It cannot be detected. It cannot be uninstalled. It cannot be stopped. It cannot be closed. Only you can access the information it collects.\" The promoters go on to say users \"can monitor a phone without [owners] knowing with mobile phone monitoring software. The app is invisible and undetectable on the phone. It works in a hidden and stealth mode.\" Daigle said the app does indeed stay hidden on devices as it uploads content in real time that can then be viewed from a web dashboard. The app, however, has a hidden backdoor that allows it to be uninstalled when a user inputs the numbers 543210 into the phone app keyboard, TechCrunch reported Wednesday. Daigel said other data spilled in the dump allowed him to identify the app operators and some of the online services they rely on. \"Dumping a stalkerware service’s database lets you do lots of fun things like identify who runs it and report it to various cloud providers who claim they’ll take it down,\" the researcher wrote. TechCrunch said that a web service hosting the app infrastructure terminated service after being contacted by the publication. Then, TechCrunch said, web host HostGator started hosting the infrastructure. Representatives of HostGator didn't immediately respond to questions about whether Catwatchful violates any of the host’s terms of service. Google has added new protections for Google Play Protect, its security tool for detecting malicious apps on Android phones, TechCrunch said. The protections will detect the Catwatchful spyware or its installer on a user’s phone. Dan Goodin Senior Security Editor Dan Goodin Senior Security Editor Dan Goodin is Senior Security Editor at Ars Technica, where he oversees coverage of malware, computer espionage, botnets, hardware hacking, encryption, and passwords. In his spare time, he enjoys gardening, cooking, and following the independent music scene. Dan is based in San Francisco. Follow him at here on Mastodon and here on Bluesky. Contact him on Signal at DanArs.82. 81 Comments</p>",
    "image": "/images/articles/adcb2b0edeee7b4a749a008919a38962.jpg",
    "author": "a SQL injection vulnerability",
    "date": "2025-07-03",
    "tags": [
      "ai",
      "security",
      "research",
      "text"
    ],
    "status": "Published",
    "originalId": "adcb2b0edeee7b4a749a008919a38962",
    "originalUrl": "https://arstechnica.com/security/2025/07/provider-of-covert-surveillance-app-spills-passwords-for-62000-users/",
    "source": "Ars Technica AI",
    "qualityScore": 1,
    "wordCount": 493
  },
  {
    "id": 5,
    "slug": "ai-first-google-colab-is-all-you-need",
    "title": "AI-First Google Colab is All You Need",
    "description": "It has democratized access to what amount to essential computing resources in today's world such as graphics processing units (GPUs) and tensor processing units (TPUs), and has offered a free no-config hosted Jupyter Notebook environment in the browser. But the landscape of artificial intelligence is evolving at an incredible pace, and the tools we use must evolve with it.",
    "content": "<p>AI-First Google Colab is All You Need Let's take a closer look at Google Colab's new AI features, and find out how you can use them to increase your daily data workflow productivity. By Matthew Mayo, KDnuggets Managing Editor on July 3, 2025 in Programming Image by Author | ChatGPT Introduction For years, Google Colab has stood as a cornerstone for data scientists, machine learning engineers, students, and researchers. It has democratized access to what amount to essential computing resources in today's world such as graphics processing units (GPUs) and tensor processing units (TPUs), and has offered a free no-config hosted Jupyter Notebook environment in the browser. This platform has been instrumental in everything from learning Python and TensorFlow to developing and training modern neural networks. But the landscape of artificial intelligence is evolving at an incredible pace, and the tools we use must evolve with it. Recognizing this shift, Google has unveiled a reimagined AI-first Colab. Announced at Google I/O 2025 and now accessible to all, this new iteration moves beyond being a simple, hosted coding environment to become an AI-powered development workflow partner. By integrating the power of Gemini, Colab now functions as an agentic collaborator that can understand your code, intent, and goals, lowering the barrier to entry for tackling today's data problems. This isn't just an update; it's genuinely a fundamental change in how we can approach data science and machine learning development. Let's take a closer look at Google Colab's new AI features, and find out how you can use them to increase your daily data workflow productivity. Why AI-First is a Game-Changer The traditional machine learning workflow can be painstaking. It involves a series of distinct, often repetitive tasks: exploratory data analysis, data cleaning and preparation, feature engineering, algorithm selection, hyperparameter tuning, model training, and model evaluation. Each step requires not only deep domain knowledge but also significant time investment in writing code, consulting documentation, and debugging. An AI-first environment like the new Colab aims to compress this workflow significantly, embedding AI into the development environment itself. Early usage of these new AI-powered features suggests a 2x gain in user efficiency, transforming hours of manual labor into a guided, conversational experience, allowing you to focus on the more creative and critical aspects of your work. Consider these common development hurdles: Repetitive coding: Writing code to load data, clean missing values, or generate standard plots is a necessary but tedious part of the process The \"blank page\" problem: Staring at an empty notebook and attempting to figure out the best library or function for a specific task can be daunting, especially for newcomers Debugging hell: An obscure error message can derail progress for hours as you search through forums and documentation for a solution Complex visualizations: Creating publication-quality charts often requires extensive tweaking of plotting library parameters, a task that distracts from the actual data exploration The new AI-first Colab addresses these pain points directly, acting as a pair programmer that helps generate code, suggest fixes, and even automate entire analytical workflows. This paradigm shift means you spend less time on the mechanics of coding and more time on strategic thinking, hypothesis testing, and results interpretation. Colab's Core AI Features Now powered by Gemini 2.5 Flash, here are 3 concrete AI features that Colab offers to make your workflows easier. 1. Iterative Querying and Intelligent Assistance At the heart of the new experience is the Gemini chat interface. You can find it either via the Gemini spark icon in the bottom toolbar for quick prompts or in a side panel for more in-depth discussions. This isn't just a simple chatbot; it's context-aware and can perform a range of tasks, including: Code generation from natural language: Simply describe what you want to do, and Colab will generate the necessary code. This can range from a simple function to refactoring an entire notebook. This feature drastically reduces the time spent on writing boilerplate and repetitive code. Library exploration: Need to use a new library? Ask Colab for an explanation and sample usage, grounded in the context of your current notebook. Intelligent error fixing: When an error occurs, Colab doesn't just identify it, it iteratively suggests fixes and presents the proposed code changes in a clear diff view, allowing you to review and accept the changes. 2. Next-Generation Data Science Agent The upgraded Data Science Agent (DSA) is another welcome addition to Colab. The DSA can autonomously carry out complex analytical tasks from start to finish. You can trigger a complete workflow simply by asking. The agent will: Generate a plan: Outlines the steps it will take to accomplish your goal Execute code: Writes and runs the necessary Python code across multiple cells Reason about results: Analyzes the output to inform its next steps Present findings: Summarizes its findings and presents them back to you The DSA allows for interactive feedback during execution, enabling you to refine or reroute the process to ensure the analysis aligns with your objectives during the entire process. This makes complex tasks like taking a raw dataset and performing end-to-end cleaning, feature analysis, model training, and evaluation a streamlined, conversational process. 3. Code Transformation and Visualization Refactoring or modifying existing code is simple; just describe the change you need in natural language. Colab will identify the relevant code blocks and suggest the necessary changes in a diff view for your approval. Furthermore, data visualization, a critical but often tedious part of data exploration, is now straightforward. Users can ask Colab to graph their data, and the agent will generate clearly labeled charts without the need to manually wrestle with the minutiae of libraries like Matplotlib or Seaborn. Getting Started with the New AI-First Colab Google has made accessing the new Colab features simple. There's no complex setup or waitlist; they're just there, ready for you to use — even in the free tier. Once logged in to Colab with a notebook open, look for the Gemini spark icon in the bottom toolbar. Click the icon to start interacting with your notebook, making coding requests, or asking questions. Note that, while these features are available in the free tier, the paid tiers offer more reliable access, longer runtimes, and faster GPUs. Walkthrough Using the New AI-First Colab Let's use some of the Colab sample data to test out the DSA. To do so, I opened the Gemini chat panel and typed the following simple exploratory data analysis task prompt (note that I didn't have to explicitly ask Colab to use its DSA capabilities but did anyways): Let's analyze some data! Use your Data Science Agent capabilities to examine and report back on the anscombe.json sample data in the sample_data folder (/content/sample_data/anscombe.json). The new Google Colab UI with Gemini chat panel visible (click to enlarge) Colab's Data Science Agent then responded with a plan: Load the dataset: Load the anscombe.json file into a pandas DataFrame. Explore the dataset: Display the first few rows, check the data types, and get a summary of the dataset to understand its structure and content. Calculate summary statistics: Calculate descriptive statistics (mean, median, standard deviation, etc.) for each of the four datasets within the anscombe.json file. Visualize the datasets: Create scatter plots for each of the four datasets to visualize the relationship between the x and y variables. Report the findings: Summarize the key findings from the analysis and visualizations, highlighting the importance of visualization in addition to summary statistics. Finish task: Put the findings from the earlier stages into a format that anyone can read. The agent began executing the code, cell by cell. If it encounters a date format it's unsure about, it might pause and ask for clarification. You providesome visualizations — a combined task that could have taken a significant amount of manual coding and debugging. Final Thoughts The reimagined Colab marks a milestone in Google's journey toward more intuitive and powerful development tools, especially those in the area of data science. By embedding an agentic collaborator at the core of the Colab notebook experience, Google has created a platform that both accelerates the work of professionals as well as makes the world of data science and machine learning more accessible to everyone. It may not be full-fledged vibe coding that ww know of in other settings, but Colab provides what might be called vibe analysis... or vibe notebooking? The future of coding is collaborative, and with Colab, your AI partner is now just a click and a prompt away. Matthew Mayo (@mattmayo13) holds a master's degree in computer science and a graduate diploma in data mining. As managing editor of KDnuggets & Statology, and contributing editor at Machine Learning Mastery, Matthew aims to make complex data science concepts accessible. His professional interests include natural language processing, language models, machine learning algorithms, and exploring emerging AI. He is driven by a mission to democratize knowledge in the data science community. Matthew has been coding since he was 6 years old.More On This TopicFine Tuning LLAMAv2 with QLora on Google Colab for FreeRunning Mixtral 8x7b On Google Colab For FreeRAPIDS cuDF for Accelerated Data Science on Google ColabTextbooks Are All You Need: A Revolutionary Approach to AI Training5 Skills All Marketing Analytics and Data Science Pros Need TodayPopular Google Certification for All Areas in the Tech Industry Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggets Privacy Policy Leave this field empty if you're human: <= Previous post Next post =></p>",
    "image": "/images/articles/7bc513ae784eb1c634876ff719a1e768.png",
    "author": "Matthew Mayo",
    "date": "2025-07-03",
    "tags": [
      "ai",
      "artificial intelligence",
      "machine learning",
      "neural network",
      "gpt"
    ],
    "status": "Published",
    "originalId": "7bc513ae784eb1c634876ff719a1e768",
    "originalUrl": "https://www.kdnuggets.com/ai-first-google-colab-is-all-you-need",
    "source": "KDnuggets",
    "qualityScore": 1,
    "wordCount": 1607
  },
  {
    "id": 6,
    "slug": "geforce-nows-20-july-games-bring-the-heat-to-the-cloud",
    "title": "GeForce NOW’s 20 July Games Bring the Heat to the Cloud",
    "description": "Share Email0 The forecast this month is showing a 100% chance of epic gaming. And to make the summer even hotter, the GeForce NOW Summer Sale is in full swing.",
    "content": "<p>Share Email0 The forecast this month is showing a 100% chance of epic gaming. Catch the scorching lineup of 20 titles coming to the cloud, which gamers can play whether indoors or on the go. Six new games are landing on GeForce NOW this week, including launch day titles Figment and Little Nightmares II. And to make the summer even hotter, the GeForce NOW Summer Sale is in full swing. It’s the last chance to upgrade to a six-month Performance membership for just $29.99 and stream top titles like the recently released classic Borderlands series, DOOM: The Dark Ages, FBC: Firebreak, and more with GeForce RTX power. Jump Into July Face your nightmares. In Figment, a whimsical action-adventure game set in the human mind, players guide Dusty — the grumpy, retired voice of courage — and his upbeat companion Piper on a surreal journey to restore lost bravery after a traumatic event. Blending hand-drawn visuals, clever puzzles and musical boss battles, Figment explores themes of fear, grief and emotional healing in a colorful, dreamlike world filled with humor and song. In addition, members can look for the following games to stream this week: Little Nightmares II (New release on Xbox, available on PC Game Pass, July 1) Figment (New release on Epic Games Store, free, July 3) Path of Exile 2 (Kakao Games) Clicker Heroes (Steam) Fabledom (Steam) Rogue: Genesia (Steam) Schedule I (Steam) Here’s what’s coming in the rest of July: The Ascent (New release on Xbox, PC Game Pass, July 8) Every Day We Fight (New release on Steam, July 10) Mycopunk (New release on Steam, July 10) Brickadia (New release on Steam, July 11) HUNTER×HUNTER NEN×IMPACT (New release on Steam, July 15) Stronghold Crusader: Definitive Edition (New release on Steam, July 15) DREADZONE (New release on Steam, July 17) The Drifter (New release on Steam, July 17) He Is Coming (New release on Steam, July 17) Killing Floor 3 (New release on Steam, July 24) RoboCop: Rogue City – Unfinished Business (New release on Steam, July 17) Wildgate (New release on Steam, July 22) Wuchang: Fallen Feathers (New release on Steam and Epic Games Store, July 23) Battle Brothers (Steam) June-tastic Games In addition to the 25 games announced last month, 11 more joined the GeForce NOW library: Frosthaven Demo (New release on Steam, June 9) Kingdom Two Crowns (New release on Xbox, available on PC Game Pass, June 11) Firefighting Simulator – The Squad (Xbox, available on PC Game Pass) JDM: Japanese Drift Master (Steam) Hellslave (Steam) Date Everything! (New release on Steam, June 17) METAL EDEN Demo (Steam) Torque Drift 2 (Epic Games Store) Broken Age (Steam) Sandwich Simulator (Steam) We Happy Few (Steam) What are you planning to play this weekend? Let us know on X or in the comments below. 📆 New month, new energy. What are your cloud gaming goals for July? ☁️🎮 — 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) July 2, 2025</p>",
    "image": "/images/articles/ade1106ab52af0cfe6b788b6e678880e.jpg",
    "author": "NVIDIA Blog",
    "date": "2025-07-03",
    "tags": [
      "cloud",
      "ai"
    ],
    "status": "Published",
    "originalId": "ade1106ab52af0cfe6b788b6e678880e",
    "originalUrl": "https://blogs.nvidia.com/blog/geforce-now-thursday-july-2025-games/",
    "source": "NVIDIA Blog",
    "qualityScore": 1,
    "wordCount": 490
  },
  {
    "id": 7,
    "slug": "the-velvet-sundown-unveiling-ais-center-stage-act",
    "title": "The Velvet Sundown: Unveiling AI’s Center Stage Act",
    "description": "The band may possibly be not be a real band at all, and its music may be AI-generated. In fact, the growing consensus is that this almost certainly is the case.",
    "content": "<p>The Velvet Sundown: Unveiling AI’s Center Stage Act A popular new \"band\" has sparked the debate: could AI-generated content destabilize the music industry? By Iván Palomares Carrascosa, KDnuggets Technical Content Specialist on July 3, 2025 in Artificial Intelligence Image credit: The Velvet Sundown (Band) - Official X account Introduction In recent days, the music industry has witnessed an avalanche of headlines surrounding a music band called The Velvet Sundown. The reason? The band may possibly be not be a real band at all, and its music may be AI-generated. In fact, the growing consensus is that this almost certainly is the case. This article takes a look at the controversy that surrounds this \"band\" and reflects on the impact it may have, and the actions that may be taken in creative industries like music to mitigate the negative effects of AI-generated content that may continue to become viral in popular streaming platforms. About The Band and Recent Suspicions The Velvet Sundown has been introduced as a band of a mixed psychedelic and alt-pop genre that emerged on Spotify in early 2025, and has so far cumulated between 400K and 650K monthly listeners in just a matter of a few weeks. At the time of writing, the band has released two albums — Floating on Echoes and Dust and Silence. What has been perceived as surprising — and more than a little suspicious — by many is that these two albums were launched within just fifteen days of one another, with a third being scheduled for July 14th. So, why the rising controversy about The Velvet Sundown being arguably an AI-generated band? Interestingly, and despite all existing photos of the band looking like the work of AI as well, The Velvet Sundown has emphatically denied being AI-generated on their X (formerly Twitter) account, claiming that its four members are real people and that their work is also real. But it wasn't long before these claims were probed and dismantled. Just a few hours prior to the time of writing this article, a band spokesperson admitted that the entire process behind the band has been an \"art hoax,\" and that their music has been generated using Suno, an AI-powered music creation tool. In the spokesperson's words: \"[I]t's marketing. It's trolling ... things that are fake have sometimes even more impact than things that are real.\" In the days leading up to this reveal, and based on multiple sources, investigations, and surmisings, the consensus on the band being created by AI was already almost absolute, with several clear hints in retrospect. \"Members\" of The Velvet Sundown music band | Image source: AAA Backstage Perhaps one of the most obvious hints at something going on are the suspiciously-looking images of the band members on the internet (see above): they all look similarly styled and have an \"overly smooth\" appearance with traits that share a lot in common with typical AI-generated avatars. Honestly (in the author's opinion), it would be hard not to notice that the four men in images like this are not even real: just look at the fingers in the hand holding the guitar. Suspiciously, there are no human traces of the band whatsoever, with no records of interviews with media, concerts, social media accounts linked to their members, nor public presence at all. And this is despite the numerous \"photos\" of them out and about, even seemingly \"on the road\" at times. Notably, apps like Deezer, which exhaustively analyze every track uploaded to their platform, have already labeled some of the band's tracks as \"possibly AI-generated\". Other detection tools like Ircam Amplify analyzed the tracks and labeled 10 out of 13 songs with 100% probability of being AI-generated, plus one with 98% probability, pointing also at Suno as the generative tool being used. AI-Generated Music's Potential Effects and Impact After having admitted that there were some trolling and marketing actions performed with AI-generated content to draw mass attention, we cannot help to take a pause and reflect on the impact of AI tools in today's music industry. When projects like the one under analysis are capable of becoming hits on platforms like Spotify, this means the line between human and artificial creations may have blurred significantly, questioning the real value of authenticity and perhaps negatively affecting legitimate musicians and bands. From an economic dimension, if cases like this occur again, it would be a matter of time before AI floods major streaming platforms with synthetic content, thereby distorting playlists, biasing metrics in favor, and impacting the revenue of real (human) artists. The public would likewise show a tendency to distrust what they listen to, sometimes even affecting the relationship between artists and listeners, out of a potential lack of confidence in whether what they are listening to is real or not. That said, not everything should be perceived as negative regarding the use of AI in creative industries: AI has the opportunity be a valuable tool for composing, trialling, and collaborating, provided it is always used under solid ethical standards. Streaming platforms must enforce a visible AI label whenever any track has been totally or partly AI-generated. Additional methods may be needed to improve the traceability of creators, like incorporating fiscal or personal data, and sophisticated verification methods when authenticating. Another important measure envisaged to protect human artists could be the creation of a legal status that recognizes the rights of traditional creators, including unfair competition compensation against synthetically generated content. Let me wrap up with some examples of what regulatory steps some regions are taking: The European Union is discussing, as part of their AI Act, some measures that would require declaring whether generative AI has been used in any cultural content, including music. In the United States, the Human Artistry Campaign has been launched to demand transparency and consent in the use of voices and real artists' styles in AI-generated work. Conclusion As of today, there really is no question that The Velvet Sundown is not a real music band, but rather an AI-powered project presented to challenge the boundaries of art, authenticity, and wider music consumption. Recently admitted as an \"art hoax\" by its creators, the band's case brings serious debate topics to the table: what should be the value given to a musical work not directly composed by humans? How should the music industry and platforms act against AI-generated content like this? The stage is set for AI's encore, but will we applaud or question the curtain call? In the end, the band The Velvet Sundown seems to have really hit the right notes... of controversy. Iván Palomares Carrascosa is a leader, writer, speaker, and adviser in AI, machine learning, deep learning & LLMs. He trains and guides others in harnessing AI in the real world.More On This TopicUnveiling the Potential of CTGAN: Harnessing Generative AI for…Unveiling Midjourney 5.2: A Leap Forward in AI Image GenerationUnveiling the Power of Meta's Llama 2: A Leap Forward in Generative AI?Unveiling Unsupervised LearningUnveiling Neural Magic: A Dive into Activation FunctionsUnveiling Hidden Patterns: An Introduction to Hierarchical Clustering Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggets Privacy Policy Leave this field empty if you're human: <= Previous post Next post =></p>",
    "image": "/images/articles/65e3321c98684219c1d30a29db11dd0e.jpeg",
    "author": "Iván Palomares Carrascosa",
    "date": "2025-07-03",
    "tags": [
      "ai",
      "artificial intelligence",
      "image",
      "platform"
    ],
    "status": "Published",
    "originalId": "65e3321c98684219c1d30a29db11dd0e",
    "originalUrl": "https://www.kdnuggets.com/the-velvet-sundown-unveiling-ais-center-stage-act",
    "source": "KDnuggets",
    "qualityScore": 1,
    "wordCount": 1234
  },
  {
    "id": 8,
    "slug": "att-rolls-out-wireless-account-lock-protection-to-curb-the-sim-swap-scourge",
    "title": "AT&T rolls out Wireless Account Lock protection to curb the SIM-swap scourge",
    "description": "The technique, known as SIM swapping or port-out fraud, has been a scourge that has vexed wireless carriers and their millions of subscribers for years. This class of attack has existed for well over a decade, and it became more commonplace amid the irrational exuberance that drove up the price of bitcoin and other cryptocurrencies.",
    "content": "<p>Text settings Story text Size Small Standard Large Width * Standard Wide Links Standard Orange * Subscribers only Learn more Minimize to nav AT&T is rolling out a protection that prevents unauthorized changes to mobile accounts as the carrier attempts to fight a costly form of account hijacking that occurs when a scammer swaps out the SIM card belonging to the account holder. The technique, known as SIM swapping or port-out fraud, has been a scourge that has vexed wireless carriers and their millions of subscribers for years. An indictment filed last year by federal prosecutors alleged that a single SIM swap scheme netted $400 million in cryptocurrency. The stolen funds belonged to dozens of victims who had used their phones for two-factor authentication to cryptocurrency wallets. Wireless Account Lock debut A separate scam from 2022 gave unauthorized access to a T-Mobile management platform that subscription resellers, known as mobile virtual network operators, use to provision services to their customers. The threat actor gained access using a SIM swap of a T-Mobile employee, a phishing attack on another T-Mobile employee, and at least one compromise of an unknown origin. This class of attack has existed for well over a decade, and it became more commonplace amid the irrational exuberance that drove up the price of bitcoin and other cryptocurrencies. In some cases, scammers impersonate existing account holders who want a new phone number for their account. At other times, they simply bribe the carrier's employees to make unauthorized changes. People storing large sums of digital coin have been frequent targets. Once crooks take control of a phone number, they trigger password resets that work by clicking on links sent in text messages. The crooks then drain cryptocurrency and traditional bank accounts. On Tuesday, AT&T revealed Wireless Account Lock, a new protection designed to curb the SIM-swap scams. When activated, the lock prevents changes from being made to a SIM until it's turned off again. The on/off button for Wireless Account Lock is available in the settings of the myAT&T mobile app. AT&T's move comes years after T-Mobile and Verizon started offering similar protection services. The Federal Communications Commission in 2023 also implemented new rules it said would make unauthorized SIM swaps harder to carry out. Wireless Account Lock prevents changes from being made to other types of account information, including billing information and authorized users. Wireless Account Lock is also available for business subscribers, although it works slightly differently. More about the new protection is available here. Dan Goodin Senior Security Editor Dan Goodin Senior Security Editor Dan Goodin is Senior Security Editor at Ars Technica, where he oversees coverage of malware, computer espionage, botnets, hardware hacking, encryption, and passwords. In his spare time, he enjoys gardening, cooking, and following the independent music scene. Dan is based in San Francisco. Follow him at here on Mastodon and here on Bluesky. Contact him on Signal at DanArs.82. 61 Comments</p>",
    "image": "/images/fallbacks/ai-default-1.jpg",
    "author": "federal prosecutors alleged that a single SIM swap scheme netted $400 million in cryptocurrency. The stolen funds belonged to dozens of victims who had used their phones for two-factor authentication to cryptocurrency wallets. Wireless Account Lock debut A separate scam from 2022 gave unauthorized access to a T-Mobile management platform that subscription resellers",
    "date": "2025-07-02",
    "tags": [
      "vision",
      "platform",
      "mobile"
    ],
    "status": "Published",
    "originalId": "2362fbaaa3ef4b947f714af64c7aa55a",
    "originalUrl": "https://arstechnica.com/security/2025/07/att-rolls-out-wireless-account-lock-protection-to-curb-the-sim-swap-scourge/",
    "source": "Ars Technica AI",
    "qualityScore": 1,
    "wordCount": 488
  },
  {
    "id": 9,
    "slug": "7-mistakes-data-scientists-make-when-applying-for-jobs",
    "title": "7 Mistakes Data Scientists Make When Applying for Jobs",
    "description": "Don’t be that data scientist. Employers and recruiters are sometimes real a-holes who ghost you just when you thought you’d start negotiating your salary.",
    "content": "<p>7 Mistakes Data Scientists Make When Applying for Jobs Data scientists often make these mistakes in their job applications and interviews. Don’t be that data scientist. By Nate Rosidi, KDnuggets Market Trends & SQL Content Specialist on July 2, 2025 in Data Science Image by Author | Canva The data science job market is crowded. Employers and recruiters are sometimes real a-holes who ghost you just when you thought you’d start negotiating your salary. As if fighting your competition, recruiters, and employers is not enough, you also have to fight yourself. Sometimes, the lack of success at interviews really is on data scientists. Making mistakes is acceptable. Not learning from them is anything but! So, let’s dissect some common mistakes and see how not to make them when applying for a data science job. 1. Treating All Roles the Same Mistake: Sending the same resume and cover letter to each role you apply for, from research-heavy and client-facing positions, to being a cook or a Timothée Chalamet lookalike. Why it hurts: Because you want the job, not the “Best Overall Candidate For All the Positions We’re Not Hiring For” award. Companies want you to fit into the particular job. A role at a software startup might prioritize product analytics, while an insurance company is hiring for modeling in R. Not tailoring your CV and cover letter to present yourself as highly suitable for a position carries a risk of being overlooked even before the interview. A fix: Read the job description carefully. Tailor your CV and cover letter to the mentioned job requirements – skills, tools, and tasks. Don’t just list skills, but show your experience with relevant applications of those skills. 2. Too Generic Data Projects Mistake: Submitting a data project portfolio brimming with washed-out projects like Titanic, Iris datasets, MNIST, or house price prediction. Why it hurts: Because recruiters will fall asleep when they read your application. They’ve seen the same portfolios thousands of times. They’ll ignore you, as this portfolio only shows your lack of business thinking and creativity. A fix: Work with messy, real-world data. Source the projects and data from sites such as StrataScratch, Kaggle, DataSF, DataHub by NYC Open Data, Awesome Public Datasets, etc. Work on less common projects Choose projects that show your passions and solve practical business problems, ideally those that your employer might have. Explain tradeoffs and why your approach makes sense in a business context. 3. Underestimating SQL Mistake: Not practicing SQL enough, because “it’s easy compared to Python or machine learning”. Why it hurts: Because knowing Python and how to avoid overfitting doesn’t make you an SQL expert. Oh, yeah, SQL is also heavily tested, especially for analyst and mid-level data science roles. Interviews often focus more on SQL than Python. A fix: Practice complex SQL concepts: subqueries, CTEs, window functions, time series joins, pivoting, and recursive queries. Use platforms like StrataScratch and LeetCode to practice real-world SQL interview questions. 4. Ignoring Product Thinking Mistake: Focusing on model metrics instead of business value. Why it hurts: Because a model that predicts customer churn with 94% ROC-AUC, but mostly flags customers who don’t use the product anymore, has no business value. You can’t retain customers that are already gone. Your skills don’t exist in a vacuum; employers want you to use those skills to deliver value. A fix: Always ask how your model impacts the business (e.g., cost reduction, revenue increase, customer satisfaction, etc.) Demonstrate your understanding of tradeoffs when building machine learning models (e.g., speed vs. accuracy, interpretability vs. complexity) 5. Ignoring MLOps Mistake: Focusing only on building a model while ignoring its deployment, monitoring, fine-tuning, and how it runs in production. Why it hurts: Because you can stick your model you-know-where if it’s not usable in production. Most employers won’t consider you a serious candidate if you don’t know how your model gets deployed, retrained, or monitored. You won’t necessarily do all that by yourself. But you’ll have to show some knowledge, as you’ll work with machine learning engineers to make sure your model actually works. A fix: Understand the three main ways of data processing: batch, real-time, and hybrid processing. Understand machine learning pipelines, CI/CD, and machine learning model monitoring. Practice workflow design in your projects by including data ingestion, model training, versioning, and serving. Get familiar with machine learning orchestration tools, such as Prefect and Airflow (for orchestration), Kubeflow and ZenML (for pipeline abstraction), and MLflow and Weights & Biases (for tracking). 6. Being Unprepared for Behavioral Interview Questions Mistake: Brushing off questions like “Tell me about a challenge you faced” as non-important and not preparing for them. Why it hurts: These questions are not a part of the interview (only) because the interviewer is bored to death with her family life, so she’d rather sit there with you in a stuffy office asking stupid questions. Behavioral questions test how you think and communicate. A fix: Don’t use generic STAR answers. Use clear, actual stories that highlight your problem-solving and communication skills. Tie your answers to data and metrics (e.g., model performance metrics, business metrics, user impact, etc.). Choose challenges with ambiguity, conflict, or cross-departmental cooperation. 7. Using Buzzwords Without Context Mistake: Packing your CV with technical and business buzzwords, but no concrete examples. Why it hurts: Because “Leveraged cutting-edge big data synergies to streamline scalable data-driven AI solution for end-to-end generative intelligence in the cloud” doesn’t really mean anything. You might accidentally impress someone with that. (But don’t count on that.) More often, you’ll be asked to explain what you mean by that and risk admitting you’ve no idea what you’re talking about. Fix it: Avoid using buzzwords and communicate clearly. Know what you’re talking about. If you can’t avoid using buzzwords, then for every buzzword, include a sentence that shows how you used it and why. Don’t be vague. Instead of saying “I have experience with DL”, say “I used long short-term memory to forecast product demand and reduced stockouts by 24%”. Conclusion Avoiding these seven mistakes is not difficult. Making them can be costly, so don’t make them. The recruitment process in data science is complicated and gruesome enough. Try not to make your life even more complicated by succumbing to the same stupid mistakes as other data scientists. Nate Rosidi is a data scientist and in product strategy. He's also an adjunct professor teaching analytics, and is the founder of StrataScratch, a platform helping data scientists prepare for their interviews with real interview questions from top companies. Nate writes on the latest trends in the career market, gives interview advice, shares data science projects, and covers everything SQL.More On This TopicStop Applying for Jobs!Stop! Do This Before Applying for JobsApplying Descriptive and Inferential Statistics in PythonMistakes That Newbie Data Scientists Should Avoid3 Mistakes That Could Be Affecting the Accuracy of Your Data Analytics5 Mistakes I Made While Switching to Data Science Career Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggets Privacy Policy Leave this field empty if you're human: <= Previous post Next post =></p>",
    "image": "/images/articles/59ab702b853bc5e85c74045c3a37f9f9.png",
    "author": "Nate Rosidi",
    "date": "2025-07-02",
    "tags": [
      "data science",
      "research",
      "image"
    ],
    "status": "Published",
    "originalId": "59ab702b853bc5e85c74045c3a37f9f9",
    "originalUrl": "https://www.kdnuggets.com/7-mistakes-data-scientists-make-when-applying-for-jobs",
    "source": "KDnuggets",
    "qualityScore": 1,
    "wordCount": 1207
  },
  {
    "id": 10,
    "slug": "nvidia-rtx-ai-accelerates-flux1-kontext-now-available-for-download",
    "title": "NVIDIA RTX AI Accelerates FLUX.1 Kontext — Now Available for Download",
    "description": "Share Email0 Black Forest Labs, one of the world’s leading AI research labs, just changed the game for image generation. Now, with its new FLUX.",
    "content": "<p>Share Email0 Black Forest Labs, one of the world’s leading AI research labs, just changed the game for image generation. The lab’s FLUX.1 image models have earned global attention for delivering high-quality visuals with exceptional prompt adherence. Now, with its new FLUX.1 Kontext model, the lab is fundamentally changing how users can guide and refine the image generation process. To get their desired results, AI artists today often use a combination of models and ControlNets — AI models that help guide the outputs of an image generator. This commonly involves combining multiple ControlNets or using advanced techniques like the one used in the NVIDIA AI Blueprint for 3D-guided image generation, where a draft 3D scene is used to determine the composition of an image. The new FLUX.1 Kontext model simplifies this by providing a single model that can perform both image generation and editing, using natural language. NVIDIA has collaborated with Black Forest Labs to optimize FLUX.1 Kontext [dev] for NVIDIA RTX GPUs using the NVIDIA TensorRT software development kit and quantization to deliver faster inference with lower VRAM requirements. For creators and developers alike, TensorRT optimizations mean faster edits, smoother iteration and more control — right from their RTX-powered machines. The FLUX.1 Kontext [dev] Flex: In-Context Image Generation Black Forest Labs in May introduced the FLUX.1 Kontext family of image models which accept both text and image prompts. These models allow users to start from a reference image and guide edits with simple language, without the need for fine-tuning or complex workflows with multiple ControlNets. FLUX.1 Kontext is an open-weight generative model built for image editing using a guided, step-by-step generation process that makes it easier to control how an image evolves, whether refining small details or transforming an entire scene. Because the model accepts both text and image inputs, users can easily reference a visual concept and guide how it evolves in a natural and intuitive way. This enables coherent, high-quality image edits that stay true to the original concept. FLUX.1 Kontext’s key capabilities include: Character Consistency: Preserve unique traits across multiple scenes and angles. Localized Editing: Modify specific elements without altering the rest of the image. Style Transfer: Apply the look and feel of a reference image to new scenes. Real-Time Performance: Low-latency generation supports fast iteration and feedback. Black Forest Labs last week released FLUX.1 Kontext weights for download in Hugging Face, as well as the corresponding TensorRT-accelerated variants. Three side-by-side images of the same graphic of coffee and snacks on a table with flowers, showing an example of multi-turn editing possible with the FLUX.1 Kontext [dev] model. The original image (left); the first edit transforms it into a Bauhaus style image (middle) and the second edit changes the color style of the image with a pastel palette (right).Traditionally, advanced image editing required complex instructions and hard-to-create masks, depth maps or edge maps. FLUX.1 Kontext [dev] introduces a much more intuitive and flexible interface, blending step-by-step edits with cutting-edge optimization for diffusion model inference. The [dev] model emphasizes flexibility and control. It supports capabilities like character consistency, style preservation and localized image adjustments, with integrated ControlNet functionality for structured visual prompting. FLUX.1 Kontext [dev] is already available in ComfyUI and the Black Forest Labs Playground, with an NVIDIA NIM microservice version expected to release in August. Optimized for RTX With TensorRT Acceleration FLUX.1 Kontext [dev] accelerates creativity by simplifying complex workflows. To further streamline the work and broaden accessibility, NVIDIA and Black Forest Labs collaborated to quantize the model — reducing the VRAM requirements so more people can run it locally — and optimized it with TensorRT to double its performance. The quantization step enables the model size to be reduced from 24GB to 12GB for FP8 (Ada) and 7GB for FP4 (Blackwell). The FP8 checkpoint is optimized for GeForce RTX 40 Series GPUs, which have FP8 accelerators in their Tensor Cores. The FP4 checkpoint is optimized for GeForce RTX 50 Series GPUs for the same reason and uses a new method called SVDQuant, which preserves high image quality while reducing model size. TensorRT — a framework to access the Tensor Cores in NVIDIA RTX GPUs for maximum performance — provides over 2x acceleration compared with running the original BF16 model with PyTorch. Speedup compared with BF16 GPU (left, higher is better) and memory usage required to run FLUX.1 Kontext [dev] in different precisions (right, lower is better).Learn more about NVIDIA optimizations and how to get started with FLUX.1 Kontext [dev] on the NVIDIA Technical Blog. Get Started With FLUX.1 Kontext FLUX.1 Kontext [dev] is available on Hugging Face (Torch and TensorRT). AI enthusiasts interested in testing these models can download the Torch variants and use them in ComfyUI. Black Forest Labs has also made available an online playground for testing the model. For advanced users and developers, NVIDIA is working on sample code for easy integration of TensorRT pipelines into workflows. Check out the DemoDiffusion repository to come later this month. But Wait, There’s More Google last week announced the release of Gemma 3n, a new multimodal small language model ideal for running on NVIDIA GeForce RTX GPUs and the NVIDIA Jetson platform for edge AI and robotics. AI enthusiasts can use Gemma 3n models with RTX accelerations in Ollama and Llama.cpp with their favorite apps, such as AnythingLLM and LM Studio. Performance tested in June 2025 with Gemma 3n in Ollama, with 4 billion active parameters, 100 ISL, 200 OSL. Plus, developers can easily deploy Gemma 3n models using Ollama and benefit from RTX accelerations. Learn more about how to run Gemma 3n on Jetson and RTX. In addition, NVIDIA’s Plug and Play: Project G-Assist Plug-In Hackathon — running virtually through Wednesday, July 16 — invites developers to explore AI and build custom G-Assist plug-ins for a chance to win prizes. Save the date for the G-Assist Plug-In webinar on Wednesday, July 9, from 10-11 a.m. PT, to learn more about Project G-Assist capabilities and fundamentals, and to participate in a live Q&A session. Join NVIDIA’s Discord server to connect with community developers and AI enthusiasts for discussions on what’s possible with RTX AI. Each week, the RTX AI Garage blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building AI agents, creative workflows, digital humans, productivity apps and more on AI PCs and workstations. Plug in to NVIDIA AI PC on Facebook, Instagram, TikTok and X — and stay informed by subscribing to the RTX AI PC newsletter. Follow NVIDIA Workstation on LinkedIn and X. See notice regarding software product information.</p>",
    "image": "/images/articles/846fdc89f720853914c30bb97af96ab8.jpg",
    "author": "providing a single model that can perform both image generation and editing",
    "date": "2025-07-02",
    "tags": [
      "ai",
      "attention",
      "research",
      "text",
      "image"
    ],
    "status": "Published",
    "originalId": "846fdc89f720853914c30bb97af96ab8",
    "originalUrl": "https://blogs.nvidia.com/blog/rtx-ai-garage-flux-kontext-nim-tensorrt/",
    "source": "NVIDIA Blog",
    "qualityScore": 1,
    "wordCount": 1103
  },
  {
    "id": 11,
    "slug": "making-group-conversations-more-accessible-with-sound-localization",
    "title": "Making group conversations more accessible with sound localization",
    "description": "Solutions have been deployed, but are currently impractical to set up in mobile scenarios. This multi-microphone approach lowers computational costs, reduces latency, and enhances privacy preservation.",
    "content": "<p>Making group conversations more accessible with sound localization July 2, 2025Samuel Yang, Research Scientist, Google Research, and Sagar Savla, Product Manager, Google DeepMind We explore an approach that uses multi-microphone localization to enhance mobile captioning with speaker diarization and directional guidance. Quick links Paper Share Copy link × Speech-to-text capabilities on mobile devices, such as Live Transcribe, have become invaluable for hearing and speech accessibility, language translation, note-taking, and meeting transcripts. However, when multiple people participate in a conversation, existing mobile automatic speech recognition (ASR) apps typically concatenate all transcribed speech together, making it difficult to follow who is saying what. This limitation creates cognitive overload for users who need to simultaneously process the transcript, identify speakers, and participate in the conversation. Solutions have been deployed, but are currently impractical to set up in mobile scenarios. For example, audio-visual speech separation requires speakers to be visible to a camera and speaker embedding approaches require a model to determine and register the unique voiceprint of each speaker.In “SpeechCompass: Enhancing Mobile Captioning with Diarization and Directional Guidance via Multi-Microphone Localization”, recipient of a Best Paper Award at CHI 2025, we explore an approach that enhances mobile captioning with speaker diarization (separating speakers in an ASR transcript) and real-time localization of incoming sound. SpeechCompass creates user-friendly transcripts for group conversations by providing color-coded visual separation for each speaker and directional indicators (arrows) to help users determine the direction from which speech is coming. This multi-microphone approach lowers computational costs, reduces latency, and enhances privacy preservation. Left: Existing mobile transcription apps concatenate transcribed text together. Right: SpeechCompass indicates the direction of incoming speech, allowing visually separated transcripts with colors and directional indicators (such as arrows) in the user interface. Efficient real-time audio localization We implement SpeechCompass in two different forms: as a phone case prototype with four microphones connected to a low-power microcontroller and as software for existing phones with two microphones. The phone case design provides optimal microphone placement to enable 360-degree sound localization. The software implementation offers only 180-degree localization on devices with two or more microphones, such as the Pixel phone. In both implementations, the phone is used for speech recognition and transcripts are visualized using a mobile application. Implementation of the prototype phone-case and its internal electronics. A) Mobile application interface with a mounted prototype case. B) Prototype case with a flexible PCB microphone mount and a main PCB. C) Top and bottom view of the main PCB (STM32). Because sound has low frequency, it bounces around indoor environments causing reverberations and making audio, especially speech, difficult to localize precisely. To address this challenge, we apply a localization algorithm based on time-difference of arrival (TDOA). Audio signals arrive at each microphone at slightly different times, so the algorithm estimates the TDOA between microphone pairs with cross-correlation to predict the angle of arrival for the sound. Specifically, we use Generalized Cross Correlation with Phase Transform (GCC-PHAT) to improve noise robustness and increase compute speed. We then apply statistical estimations, such as kernel density estimation, to improve localizer precision. The use of two omnidirectional microphones will always have “front–back” confusion (i.e., when the signals in front or in the back of the array appear identical to the microphone array), thus allowing only 180 degree localization. This issue is solved by using three or more microphones, making 360 degree localization possible. SpeechCompass system diagram, including the phone case hardware and the phone application. Unlike ML approaches to single-source speaker diarization, the SpeechCompass multi-microphone approach offers several advantages:Lower computational and memory costs: Since there is no model nor weights, the algorithm can run on small microcontrollers with limited memory and compute.Reduced latency: SpeechCompass does not rely on capturing distinguishing voice characteristics. Instead, it extracts directional information from basic sound properties, allowing it to operate in real-time with minimal lag.Greater privacy preservation: SpeechCompass assumes that different speakers are physically in separate places and does not require video or any unique personally identifying information, like speaker embeddings (unique identity of individual’s voice).Language-agnostic operation: SpeechCompass looks at differences between the audio waveforms, without prior assumptions about the content and works for sounds beyond speech.Instant reconfiguration: SpeechCompass can be reconfigured instantly by moving the phone. User interface for visualizing speaker direction We used Android’s speech-to-text capabilities to develop a mobile application that augments speech transcripts with localization data sent from the prototype phone case’s microphones via USB. The Android application provides multiple visualization styles to indicate speaker direction:Colored text: Speakers are separated using different colored text.Directional glyphs: Arrows, dials in a circle and color highlights on the boxes around the text point to the location of each speaker.Minimap: A small radar-like display shows the current speaker's position.Edge indicators: Visual cues around the screen edges highlight speaker direction.Unwanted speech suppression: The user can click on the sides of the screen to suppress speech coming from those directions. This can be used to remove their own speech. Irrelevant nearby conversation can be removed from the transcript, which enhances the privacy of nearby speakers. Various visualization styles augment speech transcripts. Technical evaluation To evaluate the SpeechCompass software, we placed a phone case on a rotating platform with a stationary speaker playing speech or noise. The platform was rotated at 10 degree increments and the angle of arrival was measured for each angle. Our evaluation shows that SpeechCompass can accurately localize sound direction with an average error of 11°–22° for normal conversational loudness (60–65 dB). The accuracy is roughly comparable to human localization abilities. For example, if a person were asked where sound was heard behind them, their answer would typically have up to 20 degrees error. The SpeechCompass system performs well across different materials and under varying ambient noise conditions, see the paper for more details. Error in localization at different audio levels and source angles. For diarization, we used diarization error rate (DER), a standard metric for diarization that corresponds to correctness of color coded speaker diarization in the interface. Our tests showed the four-microphone configuration consistently outperformed the three-microphone setup, with relative DER improvements of 23%–35% across different signal-to-noise (SNR) conditions. Diarization error rate (DER) with 3- and 4-microphone configurations at different signal-to-noise ratios. User evaluation and feedback To understand the limitations of current mobile captioning technology, we conducted an online survey with 263 frequent users of captioning technology. The results show that current solutions struggle with a significant limitation — the inability to distinguish between speakers makes them challenging to use in group conversations. The results of the survey with frequent users of mobile captioning. Second, we demonstrated the prototype to eight frequent users of mobile speech-to-text and gathered feedback. The prototype was used to diarize and visualize a conversation between the researchers. We found that colored text and directional arrows were the most preferred visualization methods. All participants agreed on the value of directional guidance for group conversations. Results of a user study with the working prototype. A) Preferences for the different visualization techniques. B) Value of the directional feedback to the users. What's next? We imagine that multi-microphone localization for mobile transcription could have numerous practical applications. One example could be in the classroom setting, where students could more easily follow discussions between instructors and classmates. Similarly in business meetings, interviews or social gatherings, users could track speaker changes in multi-person conversations.SpeechCompass demonstrates significant improvements for mobile captioning in group conversations, and there are numerous possible directions for additional development:Integration with additional wearable form factors like smart glasses and smartwatchesEnhanced noise robustness through machine learning approachesFurther customization of visualization preferencesLongitudinal studies to understand adoption and behavior in everyday scenariosWe hope that this research inspires continued innovation in making communication more accessible and inclusive for everyone. Acknowledgments We thank Artem Dementyev, Alex Olwal, Mathieu Parvaix, Chiong Lai and Dimitri Kanevsky for their work on the SpeechCompass publication and research. Dmitrii Votintcev for ideas on prototypes and interaction designs. We are grateful to Pascal Getreuer, Richard Lyon, Alex Huang, Shao-Fu Shih, and Chet Gnegy for their help with algorithms. We also thank Shaun Kane, James Landay, Malcolm Slaney, and Meredith Morris for their feedback on this paper. We appreciate the contributions of Carson Lau for the phone case mechanical design and Ngan Nguyen for electronics assembly. Finally, we thank Mei Lu, Don Barnett, Ryan Geraghty, and Sanjay Batra for UX research and design. Labels: Human-Computer Interaction and Visualization Sound & Accoustics Speech Processing Quick links Paper Share Copy link × Other posts of interest June 3, 2025 Learning to clarify: Multi-turn conversations with Action-Based Contrastive Self-Training Generative AI · Human-Computer Interaction and Visualization · Natural Language Processing April 18, 2025 InstructPipe: Generating Visual Blocks pipelines with human instructions and LLMs Human-Computer Interaction and Visualization · Machine Intelligence March 21, 2025 Deciphering language processing in the human brain through LLM representations General Science · Health & Bioscience · Natural Language Processing · Speech Processing</p>",
    "image": "/images/articles/f9372040cf902662625cd587e8982cfb.jpg",
    "author": "providing color-coded visual separation for each speaker and directional indicators (arrows) to help users determine the direction from which speech is coming. This multi-microphone approach lowers computational costs",
    "date": "2025-07-02",
    "tags": [
      "machine learning",
      "research",
      "language",
      "speech",
      "text"
    ],
    "status": "Published",
    "originalId": "f9372040cf902662625cd587e8982cfb",
    "originalUrl": "https://research.google/blog/making-group-conversations-more-accessible-with-sound-localization/",
    "source": "Google Research Blog",
    "qualityScore": 1,
    "wordCount": 1475
  },
  {
    "id": 12,
    "slug": "how-ai-factories-can-help-relieve-grid-stress",
    "title": "How AI Factories Can Help Relieve Grid Stress",
    "description": "Share Email0 In many parts of the world, including major technology hubs in the U. , there’s a yearslong wait for AI factories to come online, pending the buildout of new energy infrastructure to power them.",
    "content": "<p>Share Email0 In many parts of the world, including major technology hubs in the U.S., there’s a yearslong wait for AI factories to come online, pending the buildout of new energy infrastructure to power them. Emerald AI, a startup based in Washington, D.C., is developing an AI solution that could enable the next generation of data centers to come online sooner by tapping existing energy resources in a more flexible and strategic way. “Traditionally, the power grid has treated data centers as inflexible — energy system operators assume that a 500-megawatt AI factory will always require access to that full amount of power,” said Varun Sivaram, founder and CEO of Emerald AI. “But in moments of need, when demands on the grid peak and supply is short, the workloads that drive AI factory energy use can now be flexible.” That flexibility is enabled by the startup’s Emerald Conductor platform, an AI-powered system that acts as a smart mediator between the grid and a data center. In a recent field test in Phoenix, Arizona, the company and its partners demonstrated that its software can reduce the power consumption of AI workloads running on a cluster of 256 NVIDIA GPUs by 25% over three hours during a grid stress event while preserving compute service quality. Emerald AI achieved this by orchestrating the host of different workloads that AI factories run. Some jobs can be paused or slowed, like the training or fine-tuning of a large language model for academic research. Others, like inference queries for an AI service used by thousands or even millions of people, can’t be rescheduled, but could be redirected to another data center where the local power grid is less stressed. Emerald Conductor coordinates these AI workloads across a network of data centers to meet power grid demands, ensuring full performance of time-sensitive workloads while dynamically reducing the throughput of flexible workloads within acceptable limits. Beyond helping AI factories come online using existing power systems, this ability to modulate power usage could help cities avoid rolling blackouts, protect communities from rising utility rates and make it easier for the grid to integrate clean energy. “Renewable energy, which is intermittent and variable, is easier to add to a grid if that grid has lots of shock absorbers that can shift with changes in power supply,” said Ayse Coskun, Emerald AI’s chief scientist and a professor at Boston University. “Data centers can become some of those shock absorbers.” A member of the NVIDIA Inception program for startups and an NVentures portfolio company, Emerald AI today announced more than $24 million in seed funding. Its Phoenix demonstration, part of EPRI’s DCFlex data center flexibility initiative, was executed in collaboration with NVIDIA, Oracle Cloud Infrastructure (OCI) and the regional power utility Salt River Project (SRP). “The Phoenix technology trial validates the vast potential of an essential element in data center flexibility,” said Anuja Ratnayake, who leads EPRI’s DCFlex Consortium. EPRI is also leading the Open Power AI Consortium, a group of energy companies, researchers and technology companies — including NVIDIA — working on AI applications for the energy sector. Using the Grid to Its Full Potential Electric grid capacity is typically underused except during peak events like hot summer days or cold winter storms, when there’s a high power demand for cooling and heating. That means, in many cases, there’s room on the existing grid for new data centers, as long as they can temporarily dial down energy usage during periods of peak demand. A recent Duke University study estimates that if new AI data centers could flex their electricity consumption by just 25% for two hours at a time, less than 200 hours a year, they could unlock 100 gigawatts of new capacity to connect data centers — equivalent to over $2 trillion in data center investment. Putting AI Factory Flexibility to the Test Emerald AI’s recent trial was conducted in the Oracle Cloud Phoenix Region on NVIDIA GPUs spread across a multi-rack cluster managed through Databricks MosaicML. “Rapid delivery of high-performance compute to AI customers is critical but is constrained by grid power availability,” said Pradeep Vincent, chief technical architect and senior vice president of Oracle Cloud Infrastructure, which supplied cluster power telemetry for the trial. “Compute infrastructure that is responsive to real-time grid conditions while meeting the performance demands unlocks a new model for scaling AI — faster, greener and more grid-aware.” Jonathan Frankle, chief AI scientist at Databricks, guided the trial’s selection of AI workloads and their flexibility thresholds. “There’s a certain level of latent flexibility in how AI workloads are typically run,” Frankle said. “Often, a small percentage of jobs are truly non-preemptible, whereas many jobs such as training, batch inference or fine-tuning have different priority levels depending on the user.” Because Arizona is among the top states for data center growth, SRP set challenging flexibility targets for the AI compute cluster — a 25% power consumption reduction compared with baseline load — in an effort to demonstrate how new data centers can provide meaningful relief to Phoenix’s power grid constraints. “This test was an opportunity to completely reimagine AI data centers as helpful resources to help us operate the power grid more effectively and reliably,” said David Rousseau, president of SRP. On May 3, a hot day in Phoenix with high air-conditioning demand, SRP’s system experienced peak demand at 6 p.m. During the test, the data center cluster reduced consumption gradually with a 15-minute ramp down, maintained the 25% power reduction over three hours, then ramped back up without exceeding its original baseline consumption. AI factory users can label their workloads to guide Emerald’s software on which jobs can be slowed, paused or rescheduled — or, Emerald’s AI agents can make these predictions automatically. (Left panel): AI GPU cluster power consumption during SRP grid peak demand on May 3, 2025; (Right panel): Performance of AI jobs by flexibility tier. Flex 1 allows up to 10% average throughput reduction, Flex 2 up to 25% and Flex 3 up to 50% over a six-hour period. Figure courtesy of Emerald AI. Orchestration decisions were guided by the Emerald Simulator, which accurately models system behavior to optimize trade-offs between energy usage and AI performance. Historical grid demand from data provider Amperon confirmed that the AI cluster performed correctly during the grid’s peak period. Comparison of Emerald Simulator prediction of AI GPU cluster power with real-world measured power consumption. Figure courtesy of Emerald AI. Forging an Energy-Resilient Future The International Energy Agency projects that electricity demand from data centers globally could more than double by 2030. In light of the anticipated demand on the grid, the state of Texas passed a law that requires data centers to ramp down consumption or disconnect from the grid at utilities’ requests during load shed events. “In such situations, if data centers are able to dynamically reduce their energy consumption, they might be able to avoid getting kicked off the power supply entirely,” Sivaram said. Looking ahead, Emerald AI is expanding its technology trials in Arizona and beyond — and it plans to continue working with NVIDIA to test its technology on AI factories. “We can make data centers controllable while assuring acceptable AI performance,” Sivaram said. “AI factories can flex when the grid is tight — and sprint when users need them to.” Learn more about NVIDIA Inception and explore AI platforms designed for power and utilities.</p>",
    "image": "/images/articles/20a51d04d27499a3ede70cc908f60c29.png",
    "author": "tapping existing energy resources in a more flexible and strategic way. “Traditionally",
    "date": "2025-07-01",
    "tags": [
      "ai",
      "news"
    ],
    "status": "Published",
    "originalId": "20a51d04d27499a3ede70cc908f60c29",
    "originalUrl": "https://blogs.nvidia.com/blog/ai-factories-flexible-power-use/",
    "source": "NVIDIA Blog",
    "qualityScore": 0.9,
    "wordCount": 1233
  },
  {
    "id": 13,
    "slug": "how-we-created-hov-specific-etas-in-google-maps",
    "title": "How we created HOV-specific ETAs in Google Maps",
    "description": "The shift to sustainable travel modes like electric vehicles (EVs), carpooling, and public transit, has made travel times more varied. This is largely due to the availability of dedicated lanes, such as carpool lanes, also called high-occupancy vehicle (HOV) lanes, which are reserved for vehicles with multiple passengers and are designed to move traffic more efficiently during peak hours.",
    "content": "<p>How we created HOV-specific ETAs in Google Maps June 30, 2025Sara Ahmadian, Research Scientist, Google Research Through a novel classification approach we added a new feature of HOV routing and ETAs. The shift to sustainable travel modes like electric vehicles (EVs), carpooling, and public transit, has made travel times more varied. This is largely due to the availability of dedicated lanes, such as carpool lanes, also called high-occupancy vehicle (HOV) lanes, which are reserved for vehicles with multiple passengers and are designed to move traffic more efficiently during peak hours. As a result, HOV lanes are typically faster than general lanes during rush hour. For example, in Utah’s Salt Lake Valley, the average speed in HOV lanes was recorded at 68.18 mph compared to 58.60 mph in general lanes, a difference of about 16%.Accurate estimated time of arrival (ETA) predictions and optimized routing are key to improving the commuting experience. With precise ETAs, travelers can make better decisions, save time, and even contribute to reducing congestion and emissions. With this in mind, Google Maps recently introduced a feature that lets drivers select routes that include HOV lanes and see that route’s ETA. In this blog post, we explain how we developed this feature and developed a classification system to determine HOV trips from non-HOV trips, which led to the launch of HOV specific ETAs in Google Maps. HOV-aware route options in Google Maps. Developing HOV-specific ETAsTo estimate HOV travel times, we first infer past HOV travel times by analyzing aggregated and anonymized traffic trends. We then use these inferred times to train our ETA prediction models specifically for HOV lanes.However, identifying HOV trips isn't straightforward. Simple data, like speed, can be similar for both HOV and non-HOV users, especially when traffic is light. Yet HOV travel patterns also have several distinct and useful constraints, including limitations on availability based on location, time of day, and exceptional events.To address these issues, we develop an unsupervised learning approach, performing a classification without initial labels (HOV vs. non-HOV). We perform a classification task of the trip parts lying on individual road segments that have HOV availability. The overall trip classification is then determined by combining these segment-level classifications. Segment-level classificationFor each individual segment, we process a collection of trip parts from different trips that lie on this segment in a short window of time, e.g., 15 minutes. Our goal is to classify these trip parts using the information from both the trip itself and also the other trips that happened in the same window of time. Each trip is composed of several observations, or “trip points”, recorded during the travel time. These points include information such as speed, lateral distance from center of the road, and the time of the observation.The most prominent feature in this classification is the speed information. In fact, our classification task is more valuable when travel times on HOV lanes differ from general lanes. In such scenarios, we often observe a bimodal speed distribution, two distinct traffic patterns emerging at the same time. For example, in the figure below, anonymized aggregated speed data collected between 4:00 to 4:30 pm on Seattle’s I5 shows this clearly, with faster speeds likely corresponding to vehicles using HOV lanes. We call this Scenario A. Scenario A: Distribution of Speed on a segment with HOV availability during peak hours when HOV speeds are much faster than general lanes. By analyzing speed data, we can differentiate between HOV and non-HOV travelers when their travel times show a significant difference. However it is possible that there is a noticeable difference between travel times but it is not necessarily significant. For example, the following plot shows the speed distribution for the same segment during peak hours but when the HOV is traveling just a bit faster. We call this Scenario B. Scenario B: Distribution of Speed on a segment with HOV availability during peak hours when HOV speeds are faster than general lanes but it is not significantly faster. Beyond speed: Incorporating estimated lateral distanceWhile speed is a strong signal, we also explored other factors to improve classification. Estimated lateral distance to the center of the road, though inherently noisy due to the inherent imprecision of GPS, turned out to be useful when combined with speed. In fact, even with some imprecision, distance information helps highlight lane-specific behaviors, especially when distinguishing from adjacent general lanes.The two figures below illustrate the two speed distributions discussed previously. They depict anonymized rush-hour traffic data in the Seattle metro area on a highway segment with five lanes, where the two HOV lanes are located on the left. Speed and estimated relative distance to the center of the road from this segment are shown, with data points color-coded green and blue to indicate whether they fall within an HOV lane. The first figure highlights data in Scenario A where HOV lanes experience significantly higher speeds compared to general lanes, with average speeds of 65 mph versus 25 mph. Scenario A — Left: Recorded speed versus lateral distance from the center of the road for different anonymized trips on an individual segment. Right: The distribution of speeds separated by whether the data was located in an HOV lane or not. The second figure highlights the same data collected during the time that the the difference in speed is less (Scenario B) with average speeds of 67 mph versus 55 mph. Scenario B — Left: Recorded speed versus lateral distance from the center of the road for different anonymized trips on an individual segment. Right: The distribution of speeds separated by whether the data was located in an HOV lane or not. Temporal clustering and soft assignmentsAs the figure above suggests, clustering can be an effective method for generating initial labels for these observations. However our approach goes beyond basic clustering by incorporating an additional dimension: time. When classifying a trip observation, the timing of other observations plays a significant role. While it is important to have enough data within each time interval for statistical reliability, we also prioritize more recent data points during processing. Therefore, we use weighted median approaches that account for the timing of events.Another factor that helps our classification is a shift toward soft clustering techniques. Rather than assigning each data point definitively to a single cluster (HOV or non-HOV), we calculate the probability of each point belonging to each cluster. This is especially helpful for borderline data points. Soft clustering also gives us more flexibility when aggregating these classifications to make a final determination for the entire trip. Final aggregation and classificationA trip spans multiple road segments, and we classify each trip by aggregating classification outcomes from each segment. Special attention is given to segments that fall within HOV-eligible stretches of the road. We compute the proportion of the trip that was likely spent in HOV lanes and use this as a key factor in the final classification.To further refine our results, we implement a mixture of experts (MoE) approach. This framework uses multiple classifiers, each with different parameter settings for segment-level classification models. The final trip classification is then determined through a majority voting mechanism across these classifiers, resulting in more reliable outcomes. EvaluationTo evaluate our ETA accuracy, we conducted a series of experiments comparing the ETA calculated using our new HOV-based estimates with that of our legacy system. We partitioned trips across road stretches of varying lengths and analyzed the distribution of travel times for each stretch.For each stretch, we modeled the bimodal distribution of trips’ travel times using two normal distributions — one representing general lane travel and the other representing HOV-lane travel. Based on this, we computed the z-score for each trip to assess how closely it aligned with either distribution. This allowed us to confidently label trips as either HOV or not when their z-scores fell within high-certainty thresholds. We then evaluated our algorithm’s performance against these high-confidence labels. ResultsWe now present HOV-specific ETAs. With the launch of this feature, we've improved overall ETA accuracy for drivers using this feature by 75%, making our accuracy metrics for HOV users comparable to those of drivers taking routes without HOV lanes. Our final classification method yielded an 18% improvement in ETA accuracy over the initial method, which only compares travel speeds. ConclusionBy analyzing lane placement with speed analysis and applying a mixture-of-experts approach to trip classification, we developed a powerful method to address the scarcity of labeled HOV data. This framework offers a novel way to interpret dynamic traffic conditions and address key challenges in traffic modeling. Beyond HOV travel, similar principles can be extended to other transportation modes exhibiting similar usage patterns. For instance, in regions with significant two-wheeled traffic, these concepts could also be applicable to two-wheeled travelers.We believe this approach holds strong potential for advancing the field of traffic data analysis and has practical implications for enhancing real-world applications such as Google Maps. By improving the accuracy and efficiency of HOV lane utilization, our model could help users plan more efficient routes, reduce travel times, and contribute to smarter, greener commuting. AcknowledgementsThese technological advances were enabled by the tireless work of our collaborators in Google Maps: Daniel Delling, Amruta Gulanikar, Cameron Jones, Oliver Lange, Ramesh Namburi, Pooja Patel, Lorenzo Prelli, Stella Stylianidou, and Qian Zheng. Special thanks to Corinna Cortes, Sreenivas Gollapudi, Ravi Kumar, and Andrew Tomkins for their support during this project. We thank Cameron Jones, Sreenivas Gollapudi, and Ravi Kumar for their valuable contribution to this blog post. Labels: Algorithms & Theory Data Mining & Modeling Machine Intelligence Other posts of interest July 10, 2025 Graph foundation models for relational data Algorithms & Theory · Machine Intelligence July 9, 2025 MedGemma: Our most capable open models for health AI development Generative AI · Health & Bioscience · Machine Intelligence June 27, 2025 REGEN: Empowering personalized recommendations with natural language Data Mining & Modeling · General Science · Machine Intelligence</p>",
    "image": "/images/articles/b83613a895cfe21dce728ba1516ef1d0.png",
    "author": "analyzing aggregated and anonymized traffic trends. We then use these inferred times to train our ETA prediction models specifically for HOV lanes.However",
    "date": "2025-06-30",
    "tags": [
      "ai",
      "research",
      "classification"
    ],
    "status": "Published",
    "originalId": "b83613a895cfe21dce728ba1516ef1d0",
    "originalUrl": "https://research.google/blog/how-we-created-hov-specific-etas-in-google-maps/",
    "source": "Google Research Blog",
    "qualityScore": 1,
    "wordCount": 1649
  },
  {
    "id": 14,
    "slug": "regen-empowering-personalized-recommendations-with-natural-language",
    "title": "REGEN: Empowering personalized recommendations with natural language",
    "description": "Quick links REGEN Paper REGEN Dataset FLARE Paper Share Copy link × Large language models (LLMs) are reshaping how recommender systems interact with users. However, no datasets currently exist to explore these new capabilities.",
    "content": "<p>REGEN: Empowering personalized recommendations with natural language June 27, 2025Krishna Sayana, Software Engineer, and Hubert Pham, Research Scientist, Google Research We present a new benchmark dataset to help LLMs provide more contextualized recommendations through natural language interactions. Quick links REGEN Paper REGEN Dataset FLARE Paper Share Copy link × Large language models (LLMs) are reshaping how recommender systems interact with users. Traditional recommendation pipelines focus on predicting the next item a user might like — books, shoes, office supplies, etc. — based on past interactions. But the real goal goes further: we want systems that interact with users, understand their needs, adapt through natural language feedback, and explain why a recommendation makes sense. However, no datasets currently exist to explore these new capabilities.To address this gap we developed Reviews Enhanced with GEnerative Narratives (REGEN), a new benchmark dataset that incorporates item recommendations, natural language features composed of synthetic user critiques, and personalized narratives comprising purchase reasons and product endorsements. Rather than start from scratch, we augmented the widely-used Amazon Product Reviews dataset by synthesizing missing conversational elements with the help of Gemini 1.5 Flash. This dataset allows us to explore and benchmark new recommender architectures that incorporate both user feedback (e.g., FLARE) as well those that output natural language consistent with the recommendations (e.g., LUMEN). Our results show that LLMs trained on our dataset effectively generate both recommendations and contextual narratives, achieving performance comparable to state-of-the-art recommenders and language models. Building the REGEN dataset Existing datasets for training conversational recommenders often fall short in capturing the nuances of real-world conversations. They may focus on sequential item prediction, short dialog snippets, or lack explicit user feedback. We chose the Amazon Product Reviews dataset because of its specific utility for large vocabularies, potentially unfamiliar to an LLM.REGEN enriches the Amazon Reviews dataset with two key components: Critiques Critiques are a crucial aspect of conversational recommendation, allowing users to express their preferences and guide the system. In REGEN, critiques are generated to steer the recommender from a current item to a similar, desired item. For example, a user might critique a \"red ball-point pen\" by saying, \"I'd prefer a black one\".To ensure the relevance of critiques, we generate them only for adjacent item pairs that are sufficiently similar, using the Amazon Reviews–provided hierarchical item categories as a proxy for similarity. The Gemini 1.5 Flash model generates several critique options for each pair, from which we select one at random to include in the dataset. Narratives Narratives provide rich contextual information about recommended items, enhancing the user experience. REGEN includes diverse narratives, such as:Purchase reasons: Explanations for why an item might be suitable for a user.Product endorsements: Descriptions highlighting the benefits and features of an item.User summaries: Concise profiles of user preferences and purchase history.These narratives vary in contextualization and length, providing a rich dataset for training conversational recommenders. Experiments To evaluate REGEN effectively, we didn’t just want to test if models could recommend the right item, we wanted to see if they could communicate their reasoning, adapt to feedback, and generate language that feels tailored to the user. So we framed a new kind of task: conversational recommendation that’s jointly generative. The idea is simple but powerful — given a provided purchase history, and optionally a natural language critique (e.g., “I need something with more storage”), a model must recommend the next item and generate a contextual narrative about it.This task reflects how users naturally interact with recommendation systems when given the opportunity to express preferences in their own words. It also moves away from disjointed modeling, where recommendation and language generation are handled separately. Instead, we treat both as part of a unified, end-to-end objective.To explore different modeling approaches, we developed and implemented two baseline architectures. The first is a hybrid system, where a sequential recommender (FLARE) predicts the next item based on collaborative filtering and content signals. That output is then fed into a lightweight LLM (Gemma 2B), which is responsible for generating the narrative. This setup reflects a common architecture in production systems, where different components specialize in different stages of the pipeline. play silent looping video pause silent looping video The second architecture is LUMEN (LLM-based Unified Multi-task Model with Critiques, Recommendations, and Narratives). LUMEN does everything inside a single LLM. It’s trained end-to-end to handle critiques, generate recommendations, and produce narratives in a coherent way. During decoding, the model decides when to emit an item ID and when to continue generating natural language. We modified the vocabulary and embedding layers to support both types of outputs — item tokens and text tokens — which allowed the model to treat item recommendation as just another part of the generative process. play silent looping video pause silent looping video This dual approach — hybrid versus fully generative — lets us benchmark the trade-offs between modularity and integration, and provides a solid foundation for measuring how well models can tackle this more holistic conversational task. Results Our experiments show that REGEN can meaningfully challenge and differentiate models across both recommendation and generation tasks. In the Amazon Product Reviews dataset's Office domain, we observed that incorporating user critiques into the input consistently improved recommendation metrics across both architectures. For example, the FLARE hybrid model’s already state-of-the-art performance (0.124) on a metric which measures how often a desired item appears in the top 10 predicted results (known as Recall@10) increased to 0.1402 when critiques were included in the Office dataset, a notable bump that underscores the value of language-guided refinement.LUMEN’s performance was competitive, albeit slightly lower on traditional recommendation metrics. That’s not surprising, given the increased difficulty of generating the item and narrative jointly in a single pass. However, its real strength lies in its ability to maintain coherence between the item and the text it produces. Unlike modular pipelines, where disconnects between components can lead to awkward or generic explanations, LUMEN’s narratives tend to align more naturally with the user’s history and critique context.On the generation side, we evaluated outputs using BLEU, ROUGE, and semantic similarity. The hybrid model generally scored higher on BLEU and ROUGE, especially for product endorsements and purchase reasons, likely because the LLM was given the correct item as a prompt. LUMEN, by contrast, had slightly lower n-gram overlap but maintained strong semantic alignment, particularly for user summaries that relied more on long-term user behavior than on the specific item (see paper for details). These results highlight a few interesting dynamics. Narratives that depend primarily on the user, like summaries of their preferences, are easier for both models to generate consistently. But when the narrative is tightly coupled with the item context, like a product endorsement, performance hinges more on recommendation accuracy. If the model recommends the wrong item, it can throw off the entire narrative. This effect is more pronounced in LUMEN, where both the item and the narrative are co-generated, making it a stricter test of end-to-end alignment.We also evaluated performance on a much larger item space using the Clothing domain, which has over 370,000 unique items (5x–60x larger than any other product category). No one else we’re aware of performs evaluations on this much larger Clothing dataset, a key distinction of FLARE and REGEN. Even in this more complex setting, the hybrid system held up well, and again we saw clear gains in Recall@10 from 0.1264 to 0.1355 when critiques were included, validating the design of REGEN as a benchmark that rewards nuanced, user-guided reasoning. Conclusion REGEN provides a dataset with consistent user preferences, recommendations, and generated narratives, enabling the study of LLM capabilities in conversational recommendation. We evaluated REGEN using LUMEN, an LLM-based model for joint recommendation and narrative generation, demonstrating its utility, along with sequential recommender models. We believe REGEN serves as a fundamental resource for studying the capabilities of conversational recommender models, a crucial step towards personalized multi-turn systems.REGEN advances conversational recommendation by integrating language as a fundamental element, enhancing how recommenders interpret and respond to user preferences. This approach fosters research into multi-turn interactions, where systems can engage in extended dialogues to refine recommendations based on evolving user feedback.The dataset also encourages the development of more sophisticated models and training methodologies. It supports exploration into scaling model capacity, utilizing advanced training techniques, and adapting the methodology across different domains beyond Amazon reviews, such as travel, education, and music.Ultimately, REGEN sets a new direction for recommender systems, emphasizing comprehension and interaction, which paves the way for more intuitive, supportive, and human-like recommendation experiences. Acknowledgements We would like to thank our co-authors of the FLARE and REGEN papers without whom this work would not be possible: Liam Hebert from University of Waterloo and Kun Su, James Pine, Marialena Kyriakidi, Yuri Vasilevski, Raghavendra Vasudeva, Ambarish Jash, Sukhdeep Sodhi, Anushya Subbiah, from Google Research. Additionally, we are grateful for the support and guidance of our leadership Vikram Aggarwal, John Anderson, Dima Kuzmin, Emil Praun and Sarvjeet Singh. We also are grateful to Kimberly Schwede, Mark Simborg and the Google Research Blog editorial staff for helping us present our work to a larger audience. Finally we appreciate the authors of “Justifying Recommendations Using Distantly-Labeled Reviews and Fine-Grained Aspects” for releasing the Amazon Product Reviews dataset used in our work. Labels: Data Mining & Modeling General Science Machine Intelligence Quick links REGEN Paper REGEN Dataset FLARE Paper Share Copy link × Other posts of interest July 10, 2025 Graph foundation models for relational data Algorithms & Theory · Machine Intelligence July 9, 2025 MedGemma: Our most capable open models for health AI development Generative AI · Health & Bioscience · Machine Intelligence June 30, 2025 How we created HOV-specific ETAs in Google Maps Algorithms & Theory · Data Mining & Modeling · Machine Intelligence</p>",
    "image": "/images/articles/313fa56c5ef8ca6ed6185da41a5c4759.png",
    "author": "synthesizing missing conversational elements with the help of Gemini 1.5 Flash. This dataset allows us to explore and benchmark new recommender architectures that incorporate both user feedback (e.g.",
    "date": "2025-06-27",
    "tags": [
      "ai",
      "llm",
      "bert",
      "research",
      "api"
    ],
    "status": "Published",
    "originalId": "313fa56c5ef8ca6ed6185da41a5c4759",
    "originalUrl": "https://research.google/blog/regen-empowering-personalized-recommendations-with-natural-language/",
    "source": "Google Research Blog",
    "qualityScore": 1,
    "wordCount": 1617
  },
  {
    "id": 15,
    "slug": "actively-exploited-vulnerability-gives-extraordinary-control-over-server-fleets",
    "title": "Actively exploited vulnerability gives extraordinary control over server fleets",
    "description": "Text settings Story text Size Small Standard Large Width * Standard Wide Links Standard Orange * Subscribers only Learn more Minimize to nav Hackers are exploiting a maximum-severity vulnerability that has the potential to give them complete control over thousands of servers, many of which handle mission-critical tasks inside data centers, the US Cybersecurity and Infrastructure Security Agency is warning. The vulnerability, carrying a severity rating of 10 out of a possible 10, resides in the AMI MegaRAC, a widely used firmware package that allows large fleets of servers to be remotely accessed and managed even when power is unavailable or the operating system isn't functioning.",
    "content": "<p>Text settings Story text Size Small Standard Large Width * Standard Wide Links Standard Orange * Subscribers only Learn more Minimize to nav Hackers are exploiting a maximum-severity vulnerability that has the potential to give them complete control over thousands of servers, many of which handle mission-critical tasks inside data centers, the US Cybersecurity and Infrastructure Security Agency is warning. The vulnerability, carrying a severity rating of 10 out of a possible 10, resides in the AMI MegaRAC, a widely used firmware package that allows large fleets of servers to be remotely accessed and managed even when power is unavailable or the operating system isn't functioning. These motherboard-attached microcontrollers, known as baseboard management controllers (BMCs), give extraordinary control over servers inside data centers. Administrators use BMCs to reinstall operating systems, install or modify apps, and make configuration changes to large numbers of servers without physically being on premises and, in many cases, without the servers being turned on. Successful compromise of a single BMC can be used to pivot into internal networks and compromise all other BMCs. We don’t need no stinkin’ credentials CVE-2024-54085, as the vulnerability is tracked, allows for authentication bypasses by making a simple web request to a vulnerable BMC device over HTTP. The vulnerability was discovered by security firm Eclypsium and disclosed in March. The disclosure included proof-of-concept exploit code allowing a remote attacker to create an admin account without providing any authentication. At the time of the disclosure, there were no known reports of the vulnerability being actively exploited. On Wednesday, CISA added CVE-2024-54085 to its list of vulnerabilities known to be exploited in the wild. The notice provided no further details. In an email on Thursday, Eclypsium researchers said the scope of the exploits has the potential to be broad: Attackers could chain multiple BMC exploits to implant malicious code directly into the BMC’s firmware, making their presence extremely difficult to detect and allowing them to survive OS reinstalls or even disk replacements. By operating below the OS, attackers can evade endpoint protection, logging, and most traditional security tools. With BMC access, attackers can remotely power on or off, reboot, or reimage the server, regardless of the primary operating system's state. Attackers can scrape credentials stored on the system, including those used for remote management, and use the BMC as a launchpad to move laterally within the network BMCs often have access to system memory and network interfaces, enabling attackers to sniff sensitive data or exfiltrate information without detection Attackers with BMC access can intentionally corrupt firmware, rendering servers unbootable and causing significant operational disruption With no publicly known details of the ongoing attacks, it's unclear which groups may be behind them. Eclypsium said the most likely culprits would be espionage groups working on behalf of the Chinese government. All five of the specific APT groups Eclypsium named have a history of exploiting firmware vulnerabilities or gaining persistent access to high-value targets. Eclypsium said the line of vulnerable AMI MegaRAC devices uses an interface known as Redfish. Server makers known to use these products include AMD, Ampere Computing, ASRock, ARM, Fujitsu, Gigabyte, Huawei, Nvidia, and Qualcomm. Some, but not all, of these vendors have released patches for their wares. Given the damage possible from exploitation of this vulnerability, admins should examine all BMCs in their fleets to ensure they aren't vulnerable. With products from so many different server makers affected, admins should consult with their manufacturer when unsure if their networks are exposed. Dan Goodin Senior Security Editor Dan Goodin Senior Security Editor Dan Goodin is Senior Security Editor at Ars Technica, where he oversees coverage of malware, computer espionage, botnets, hardware hacking, encryption, and passwords. In his spare time, he enjoys gardening, cooking, and following the independent music scene. Dan is based in San Francisco. Follow him at here on Mastodon and here on Bluesky. Contact him on Signal at DanArs.82. 46 Comments</p>",
    "image": "/images/articles/549d527b3b1d79c60b013e34cc2c1900.jpg",
    "author": "making a simple web request to a vulnerable BMC device over HTTP. The vulnerability was discovered by security firm Eclypsium and disclosed in March. The disclosure included proof-of-concept exploit code allowing a remote attacker to create an admin account without providing any authentication. At the time of the disclosure",
    "date": "2025-06-26",
    "tags": [
      "ai",
      "security"
    ],
    "status": "Published",
    "originalId": "549d527b3b1d79c60b013e34cc2c1900",
    "originalUrl": "https://arstechnica.com/security/2025/06/active-exploitation-of-ami-management-tool-imperils-thousands-of-servers/",
    "source": "Ars Technica AI",
    "qualityScore": 1,
    "wordCount": 650
  },
  {
    "id": 16,
    "slug": "anthropic-summons-the-spirit-of-flash-games-for-the-ai-age",
    "title": "Anthropic summons the spirit of Flash games for the AI age",
    "description": "Using plain English dialogue, users can build and share interactive applications directly within Claude's chatbot interface using a new API capability that lets artifacts interact with Claude itself. Claude is an AI assistant similar to ChatGPT.",
    "content": "<p>Text settings Story text Size Small Standard Large Width * Standard Wide Links Standard Orange * Subscribers only Learn more Minimize to nav On Wednesday, Anthropic announced a new feature that expands its Artifacts document management system into the basis of a personal AI app gallery resembling something from the Flash game era of the early 2000s—though these apps run on modern web code rather than Adobe's defunct plugin. Using plain English dialogue, users can build and share interactive applications directly within Claude's chatbot interface using a new API capability that lets artifacts interact with Claude itself. Claude is an AI assistant similar to ChatGPT. Claude has been capable of building web apps for some time, but Anthropic has put renewed focus on the feature that many have overlooked. \"I'm amused that Anthropic turned 'we added a window.claude.complete() function to Artifacts' into what looks like a major new product launch,\" wrote independent AI researcher Simon Willison in a blog post, \"but I can't say it's bad marketing for them to do that!\" A screenshot of an example AI-coded game within Claude Artifacts. Benj Edwards / Anthropic A screenshot of an example AI-coded game within Claude Artifacts. Benj Edwards / Anthropic A screenshot of an example AI-coded game within Claude Artifacts. Benj Edwards / Anthropic A screenshot of an example AI-coded game within Claude Artifacts. Benj Edwards / Anthropic A screenshot of an example AI-coded game within Claude Artifacts. Benj Edwards / Anthropic A screenshot of an example AI-coded game within Claude Artifacts. Benj Edwards / Anthropic A screenshot of an example AI-coded game within Claude Artifacts. Benj Edwards / Anthropic A screenshot of an example AI-coded game within Claude Artifacts. Benj Edwards / Anthropic A screenshot of an example AI-coded game within Claude Artifacts. Benj Edwards / Anthropic A screenshot of an example AI-coded game within Claude Artifacts. Benj Edwards / Anthropic A screenshot of an example AI-coded game within Claude Artifacts. Benj Edwards / Anthropic A screenshot of an example AI-coded game within Claude Artifacts. Benj Edwards / Anthropic On the Anthropic gallery site, example artifact apps come organized into categories like \"Learn something,\" \"Life hacks,\" and \"Be creative.\" Featured artifacts at launch include an interactive writing editor, a bedtime story generator, a molecule visualizer, and a 3D first-person \"Anthropic office simulator\" where you can walk around and interact with simple representations of real Anthropic employees. Users can examine the prompts and the chats that made those examples possible and even modify them for their own purposes. The beta Artifacts gallery feature is currently available to users on Claude's Free, Pro, and Max plans, and it's accessible through the Claude app's sidebar. How it works: AI as the coder When users ask Claude to create an artifact, the AI model writes the HTML, CSS, and JavaScript codef, typically using React (a JavaScript library for web interfaces) for interactive components. Anthropic provided a demo video that showcases the process. Anthropic's \"Build a Claude-powered app\" video. The key addition in the latest update is a \"window.claude.complete()\" function that AI-generated apps can use to make their own requests back to Claude, enabling in-app conversational chatbot features like dynamic NPCs or tutors that users can talk to. Inspired by a demo created by Anthropic, we created a simple 2D simulation where the users move around an office and chat with some members of Ars Technica staff as if they were chatbot characters themselves. It's worth noting that the experience is entirely sandboxed for now. Unlike traditional web development, where developers manually integrate APIs and services, Claude creates self-contained applications that can only communicate with Claude itself—no external API calls (\"yet,\" as Anthropic notes), no database connections, and no local browser storage. A screenshot of the interface for building apps within Claude. You see a live preview of the app on the right side of the window. Credit: Benj Edwards / Anthropic All state management happens in-memory through React components or JavaScript variables that Claude implements, creating a simplified environment where users describe their ideas and Claude handles both the interface code and the AI logic. In a way, it's a form of vibe coding but self-contained entirely within its own web environment. Web portal throwback Perhaps unintentionally, Anthropic's artifact gallery interface reminds us of classic Flash gaming portals, with each tile in the gallery showing a snapshot of the interactive experience waiting inside—similar to how Flash portals teased players with game screenshots back in the early 2000s. For those who missed the Flash era, these in-browser apps feel somewhat like the vintage apps that defined a generation of Internet culture from the late 1990s through the 2000s when it first became possible to create complex in-browser experiences. Adobe Flash (originally Macromedia Flash) began as animation software for designers but quickly became the backbone of interactive web content when it gained its own programming language, ActionScript, in 2000. But unlike Flash games, where hosting costs fell on portal operators, Anthropic has crafted a system where users pay for their own fun through their existing Claude subscriptions. \"When someone uses your Claude-powered app, they authenticate with their existing Claude account,\" Anthropic explained in its announcement. \"Their API usage counts against their subscription, not yours. You pay nothing for their usage.\" A view of the Anthropic Artifacts gallery in the \"Play a Game\" section. Benj Edwards / Anthropic A view of the Anthropic Artifacts gallery in the \"Play a Game\" section. Benj Edwards / Anthropic A view of the Anthropic Artifacts gallery in the \"learn something\" section. Benj Edwards / Anthropic A view of the Anthropic Artifacts gallery in the \"learn something\" section. Benj Edwards / Anthropic A view of the Anthropic Artifacts gallery in the ironically named \"touch grass\" section. Benj Edwards / Anthropic A view of the Anthropic Artifacts gallery in the ironically named \"touch grass\" section. Benj Edwards / Anthropic A view of the Anthropic Artifacts gallery in the \"learn something\" section. Benj Edwards / Anthropic A view of the Anthropic Artifacts gallery in the ironically named \"touch grass\" section. Benj Edwards / Anthropic A view of the Anthropic Artifacts gallery in the \"be creative\" section. Benj Edwards / Anthropic A view of the Anthropic Artifacts gallery in the \"life hacks\" section. Benj Edwards / Anthropic Like the Flash games of yesteryear, any Claude-powered apps you build run in the browser and can be shared with anyone who has a Claude account. They're interactive experiences shared with a simple link, no installation required, created by other people for the sake of creating, except now they're powered by JavaScript instead of ActionScript. While you can share these apps with others individually, right now Anthropic's Artifact gallery only shows examples made by Anthropic and your own personal Artifacts. (If Anthropic expanded it into the future, it might end up feeling a bit like Scratch meets Newgrounds, but with AI doing the coding.) Ultimately, humans are still behind the wheel, describing what kinds of apps they want the AI model to build and guiding the process when it inevitably makes mistakes. Speaking of mistakes, don't expect perfect results at first. Usually, building an app with Claude is an interactive experience that requires some guidance to achieve your desired results. But with a little patience and a lot of tokens, you'll be vibe coding in no time. Benj Edwards Senior AI Reporter Benj Edwards Senior AI Reporter Benj Edwards is Ars Technica's Senior AI Reporter and founder of the site's dedicated AI beat in 2022. He's also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC. 23 Comments</p>",
    "image": "/images/fallbacks/ai-neural-network.jpg",
    "author": "a demo created by Anthropic",
    "date": "2025-06-26",
    "tags": [
      "ai",
      "gpt",
      "chatbot",
      "api"
    ],
    "status": "Published",
    "originalId": "b67630835be1a3ab6ee8176c75988aec",
    "originalUrl": "https://arstechnica.com/ai/2025/06/anthropic-summons-the-spirit-of-flash-games-for-the-ai-age/",
    "source": "Ars Technica AI",
    "qualityScore": 0.9,
    "wordCount": 1281
  },
  {
    "id": 17,
    "slug": "vmware-perpetual-license-holder-receives-audit-letter-from-broadcom",
    "title": "VMware perpetual license holder receives audit letter from Broadcom",
    "description": "Broadcom stopped selling VMware perpetual licenses in November 2023 in favor of pushing a small number of VMware SKUs that feature multiple VMware offerings. Since Broadcom is forcefully bundling VMware products, the costs associated with running VMware have skyrocketed, with customers frequently citing 300 percent price hikes and some firms claiming even larger increases.",
    "content": "<p>Text settings Story text Size Small Standard Large Width * Standard Wide Links Standard Orange * Subscribers only Learn more Minimize to nav After sending cease-and-desist letters to VMware users whose support contracts had expired and who subsequently declined to subscribe to one of Broadcom’s VMware bundles, Broadcom has started the process of conducting audits on former VMware customers. Broadcom stopped selling VMware perpetual licenses in November 2023 in favor of pushing a small number of VMware SKUs that feature multiple VMware offerings. Since Broadcom is forcefully bundling VMware products, the costs associated with running VMware have skyrocketed, with customers frequently citing 300 percent price hikes and some firms claiming even larger increases. As a result, some VMware users have opted to keep using VMware perpetual licenses, even though Broadcom refuses to renew most of those clients’ support services. This year, Broadcom started sending such VMware users cease-and-desist letters [PDF], telling organizations to stop using any maintenance releases/updates, minor releases, major releases/upgrades extensions, enhancements, patches, bug fixes, or security patches (except for zero-day security patches) that VMware issued since the user’s support contract ended. The letters also warned of potential audits, which appear to be underway now. Broadcom starts auditing Ars Technica reviewed a letter that a software provider and VMware user in the Netherlands received that is dated June 20 and informs the firm that it “has been selected for a formal audit of its use of VMware software and support services” [PDF]. The security professional who provided Ars with the letter asked to keep their name and their employers’ name anonymous out of privacy concerns. The anonymous employee told Ars that their company had been a VMware customer for “about” a decade before deciding not to sign up for a new contract with Broadcom’s VMware a year ago. The company had been using VMware Cloud Foundation and vSphere. “Our CEO decided to not extend the support contract because of the costs,” the employee said. “This already impacts us security-wise because we can no longer get updates (unless the CVSS score is critical).” The letter notes that an auditing firm, Connor Consulting, which is headquartered in San Francisco and has offices around the globe, will perform a review of the company’s “VMware deployment and entitlements, which may include fieldwork or remote testing and meetings with members of your accounting, licensing, and management information systems functions.” The letter informs its recipient that someone from Connor will reach out and that the VMware user should respond within three business days. The letter, signed by Aiden Fitzgerald, director of global sales operations at Broadcom, claims that Broadcom will use its time “as efficiently and productively as possible to minimize disruption.” Still, the security worker that Ars spoke with is concerned about the implications of the audit and said they “expect a big financial impact” for their employer. They added: Because we are focusing on saving costs and are on a pretty tight financial budget, this will likely have impact on the salary negotiations or even layoffs of employees. Currently, we have some very stressed IT managers [and] legal department [employees] … The employee noted that they are unsure if their employer exceeded its license limits. If the firm did, it could face “big” financial repercussions, the worker noted. Users deny wrongdoing As Broadcom works to ensure that people aren’t using VMware outside its terms, some suggest that the semiconductor giant is wasting some time by investigating organizations that aren’t violating agreements. After Broadcom started sending cease-and-desist letters, at least one firm claimed that it got a letter from Broadcom despite no longer using VMware at all. Additionally, various companies claimed that they received a cease-and-desist from Broadcom despite not implementing any updates after their VMware support contract expired. The employee at the Dutch firm that received an audit notice this month claimed that the only update that their employer has issued to the VMware offerings it uses since support ended was a “critical security patch.” That employee also claimed to Ars that their company didn’t receive a cease-and-desist letter from Broadcom before being informed of an audit. Broadcom didn’t respond to Ars' request for comment ahead of publication, so we’re unable to confirm if the company is sending audit letters without sending cease-and-desist letters first. Ars also reached out to Connor Consulting but didn’t hear back. “When we saw the news that they were going to send cease-and-desist letters and audits, our management thought it was a bluff and that they would never do that,” the anonymous security worker said. Broadcom’s litigious techniques to ensure VMware agreements are followed have soured its image among some current and former customers. Broadcom’s $69 billion VMware acquisition has proven lucrative, but as Broadcom approaches two years of VMware ownership, there are still calls for regulation of its practices, which some customers and partners believe are “legally and ethically flawed.” Scharon Harding Senior Technology Reporter Scharon Harding Senior Technology Reporter Scharon is a Senior Technology Reporter at Ars Technica writing news, reviews, and analysis on consumer gadgets and services. She's been reporting on technology for over 10 years, with bylines at Tom’s Hardware, Channelnomics, and CRN UK. 163 Comments</p>",
    "image": "/images/fallbacks/ai-brain.jpg",
    "author": "Aiden Fitzgerald",
    "date": "2025-06-26",
    "tags": [
      "ai",
      "news"
    ],
    "status": "Published",
    "originalId": "6b7c0b0b36c0074aae762597b99d7d4e",
    "originalUrl": "https://arstechnica.com/information-technology/2025/06/vmware-perpetual-license-holder-receives-audit-letter-from-broadcom/",
    "source": "Ars Technica AI",
    "qualityScore": 1,
    "wordCount": 863
  },
  {
    "id": 18,
    "slug": "game-on-with-geforce-now-the-membership-that-keeps-on-delivering",
    "title": "Game On With GeForce NOW, the Membership That Keeps on Delivering",
    "description": "Share Email0 This GFN Thursday rolls out a new reward and games for GeForce NOW members. Whether hunting for hot new releases or rediscovering timeless classics, members can always find more ways to play, games to stream and perks to enjoy.",
    "content": "<p>Share Email0 This GFN Thursday rolls out a new reward and games for GeForce NOW members. Whether hunting for hot new releases or rediscovering timeless classics, members can always find more ways to play, games to stream and perks to enjoy. Gamers can score major discounts on the titles they’ve been eyeing — perfect for streaming in the cloud — during the Steam Summer Sale, running until Thursday, July 10, at 10 a.m. PT. This week also brings unforgettable adventures to the cloud: We Happy Few and Broken Age are part of the five additions to the GeForce NOW library this week. The fun doesn’t stop there. A new in-game reward for Elder Scrolls Online is now available for members to claim. And SteelSeries has launched a new mobile controller that transforms phones into cloud gaming devices with GeForce NOW. Add it to the roster of on-the-go gaming devices — including the recently launched GeForce NOW app on Steam Deck for seamless 4K streaming. Scroll Into Power GeForce NOW Premium members receive exclusive 24-hour early access to a new mythical reward in The Elder Scrolls Online — Bethesda’s award-winning role-playing game — before it opens to all members. Sharpen the sword, ready the staff and chase glory across the vast, immersive world of Tamriel. Fortune favors the bold. Claim the mythical Grand Gold Coast Experience Scrolls reward, a rare item that grants a bonus of 150% Experience Points from all sources for one hour. The scroll’s effect pauses while players are offline and resumes upon return, ensuring every minute counts. Whether tackling dungeon runs, completing epic quests or leveling a new character, the scrolls provide a powerful edge. Claim the reward, harness its power and scroll into the next adventure. Members who’ve opted into the GeForce NOW Rewards program can check their emails for redemption instructions. The offer runs through Saturday, July 26, while supplies last. Don’t miss this opportunity to become a legend in Tamriel. Steam Up Summer The Steam Summer Sale is in full swing. Snag games at discounted prices and stream them instantly from the cloud — no downloads, no waiting, just pure gaming bliss. Treat yourself. Check out the “Steam Summer Sale” row in the GeForce NOW app to find deals on the next adventure. With GeForce NOW, gaming favorites are always just a click away. While picking up discounted games, don’t miss the chance to get a GeForce NOW six-month Performance membership at 40% off. This is also the last opportunity to take advantage of the Performance Day Pass sale, ending Friday, June 27 — which lets gamers access cloud gaming for 24 hours — before diving into the 6-month Performance membership. Find Adventure Two distinct worlds — where secrets simmer and imagination runs wild — are streaming onto the cloud this week. Keep calm and blend in (or else). Step into the surreal, retro-futuristic streets of We Happy Few, where a society obsessed with happiness hides its secrets behind a mask of forced cheer and a haze of “Joy.” This darkly whimsical adventure invites players to blend in, break out and uncover the truth lurking beneath the surface of Wellington Wells. Two worlds, one wild destiny. Broken Age spins a charming, hand-painted tale of two teenagers leading parallel lives in worlds at once strange and familiar. One of the teens yearns to escape a stifling spaceship, and the other is destined to challenge ancient traditions. With witty dialogue and heartfelt moments, Broken Age is a storybook come to life, brimming with quirky characters and clever puzzles. Each of these unforgettable adventures brings its own flavor — be it dark satire, whimsical wonder or pulse-pounding suspense — offering a taste of gaming at its imaginative peaks. Stream these captivating worlds straight from the cloud and enjoy seamless gameplay, no downloads or high-end hardware required. An Ultimate Controller Elevated gaming. Get ready for the SteelSeries Nimbus Cloud, a new dual-mode cloud controller. When paired with GeForce NOW, this new controller reaches new heights. Designed for versatility and comfort, and crafted specifically for cloud gaming, the SteelSeries Nimbus Cloud effortlessly shifts from a mobile device controller to a full-sized wireless controller, delivering top-notch performance and broad compatibility across devices. The Nimbus Cloud enables gamers to play wherever they are, as it easily adapts to fit iPhones and Android phones. Or collapse and connect the controller via Bluetooth to a gaming rig or smart TV. Transform any space into a personal gaming station with GeForce NOW and the Nimbus Cloud, part of the list of recommended products for an elevated cloud gaming experience. Gaming Never Sleeps “System Shock 2” — now with 100% more existential dread. System Shock 2: 25th Anniversary Remaster is an overhaul of the acclaimed sci-fi horror classic, rebuilt by Nightdive Studios with enhanced visuals, refined gameplay and features such as cross-play co-op multiplayer. Face the sinister AI SHODAN and her mutant army aboard the starship Von Braun as a cybernetically enhanced soldier with upgradable skills, powerful weapons and psionic abilities. Stream the title from the cloud with GeForce NOW for ultimate flexibility and performance. Look for the following games available to stream in the cloud this week: System Shock 2: 25th Anniversary Remaster (New release on Steam, June 26) Broken Age (Steam) Easy Red 2 (Steam) Sandwich Simulator (Steam) We Happy Few (Steam) What are you planning to play this weekend? Let us know on X or in the comments below. The official GFN summer bucket list ☁️Play anywhere 💻Stream on every screen you own 🎮 Finally crush that backlog 🚫 Skip every single download bar Drop the emoji for the one you’re tackling right now 👇 — 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) June 25, 2025</p>",
    "image": "/images/articles/c44803353cc1367a9c655ca1f1e0a220.jpg",
    "author": "Nightdive Studios with enhanced visuals",
    "date": "2025-06-26",
    "tags": [
      "ai",
      "cloud",
      "mobile"
    ],
    "status": "Published",
    "originalId": "c44803353cc1367a9c655ca1f1e0a220",
    "originalUrl": "https://blogs.nvidia.com/blog/geforce-now-thursday-elder-scrolls-online-member-reward/",
    "source": "NVIDIA Blog",
    "qualityScore": 1,
    "wordCount": 952
  },
  {
    "id": 19,
    "slug": "startup-uses-nvidia-rtx-powered-generative-ai-to-make-coolers-cooler",
    "title": "Startup Uses NVIDIA RTX-Powered Generative AI to Make Coolers, Cooler",
    "description": "Share Email0 Mark Theriault founded the startup FITY envisioning a line of clever cooling products: cold drink holders that come with freezable pucks to keep beverages cold for longer without the mess of ice. The entrepreneur started with 3D prints of products in his basement, building one unit at a time, before eventually scaling to mass production.",
    "content": "<p>Share Email0 Mark Theriault founded the startup FITY envisioning a line of clever cooling products: cold drink holders that come with freezable pucks to keep beverages cold for longer without the mess of ice. The entrepreneur started with 3D prints of products in his basement, building one unit at a time, before eventually scaling to mass production. Founding a consumer product company from scratch was a tall order for a single person. Going from preliminary sketches to production-ready designs was a major challenge. To bring his creative vision to life, Theriault relied on AI and his NVIDIA GeForce RTX-equipped system. For him, AI isn’t just a tool — it’s an entire pipeline to help him accomplish his goals. Read more about his workflow below. Plus, GeForce RTX 5050 laptops start arriving today at retailers worldwide, from $999. GeForce RTX 5050 Laptop GPUs feature 2,560 NVIDIA Blackwell CUDA cores, fifth-generation AI Tensor Cores, fourth-generation RT Cores, a ninth-generation NVENC encoder and a sixth-generation NVDEC decoder. In addition, NVIDIA’s Plug and Play: Project G-Assist Plug-In Hackathon — running virtually through Wednesday, July 16 — invites developers to explore AI and build custom G-Assist plug-ins for a chance to win prizes. Save the date for the G-Assist Plug-In webinar on Wednesday, July 9, from 10-11 a.m. PT, to learn more about Project G-Assist capabilities and fundamentals, and to participate in a live Q&A session. From Concept to Completion To create his standout products, Theriault tinkers with potential FITY Flex cooler designs with traditional methods, from sketch to computer-aided design to rapid prototyping, until he finds the right vision. A unique aspect of the FITY Flex design is that it can be customized with fun, popular shoe charms. For packaging design inspiration, Theriault uses his preferred text-to-image generative AI model for prototyping, Stable Diffusion XL — which runs 60% faster with the NVIDIA TensorRT software development kit — using the modular, node-based interface ComfyUI. ComfyUI gives users granular control over every step of the generation process — prompting, sampling, model loading, image conditioning and post-processing. It’s ideal for advanced users like Theriault who want to customize how images are generated. Theriault’s uses of AI result in a complete computer graphics-based ad campaign. Image courtesy of FITY. NVIDIA and GeForce RTX GPUs based on the NVIDIA Blackwell architecture include fifth-generation Tensor Cores designed to accelerate AI and deep learning workloads. These GPUs work with CUDA optimizations in PyTorch to seamlessly accelerate ComfyUI, reducing generation time on FLUX.1-dev, an image generation model from Black Forest Labs, from two minutes per image on the Mac M3 Ultra to about four seconds on the GeForce RTX 5090 desktop GPU. ComfyUI can also add ControlNets — AI models that help control image generation — that Theriault uses for tasks like guiding human poses, setting compositions via depth mapping and converting scribbles to images. Theriault even creates his own fine-tuned models to keep his style consistent. He used low-rank adaptation (LoRA) models — small, efficient adapters into specific layers of the network — enabling hyper-customized generation with minimal compute cost. LoRA models allow Theriault to ideate on visuals quickly. Image courtesy of FITY. “Over the last few months, I’ve been shifting from AI-assisted computer graphics renders to fully AI-generated product imagery using a custom Flux LoRA I trained in house. My RTX 4080 SUPER GPU has been essential for getting the performance I need to train and iterate quickly.” – Mark Theriault, founder of FITY Theriault also taps into generative AI to create marketing assets like FITY Flex product packaging. He uses FLUX.1, which excels at generating legible text within images, addressing a common challenge in text-to-image models. Though FLUX.1 models can typically consume over 23GB of VRAM, NVIDIA has collaborated with Black Forest Labs to help reduce the size of these models using quantization — a technique that reduces model size while maintaining quality. The models were then accelerated with TensorRT, which provides an up to 2x speedup over PyTorch. To simplify using these models in ComfyUI, NVIDIA created the FLUX.1 NIM microservice, a containerized version of FLUX.1 that can be loaded in ComfyUI and enables FP4 quantization and TensorRT support. Combined, the models come down to just over 11GB of VRAM, and performance improves by 2.5x. Theriault uses the Blender Cycles app to render out final files. For 3D workflows, NVIDIA offers the AI Blueprint for 3D-guided generative AI to ease the positioning and composition of 3D images, so anyone interested in this method can quickly get started. Photorealistic renders. Image courtesy of FITY. Finally, Theriault uses large language models to generate marketing copy — tailored for search engine optimization, tone and storytelling — as well as to complete his patent and provisional applications, work that usually costs thousands of dollars in legal fees and considerable time. Generative AI helps Theriault create promotional materials like the above. Image courtesy of FITY. “As a one-man band with a ton of content to generate, having on-the-fly generation capabilities for my product designs really helps speed things up.” – Mark Theriault, founder of FITY Every texture, every word, every photo, every accessory was a micro-decision, Theriault said. AI helped him survive the “death by a thousand cuts” that can stall solo startup founders, he added. Each week, the RTX AI Garage blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building AI agents, creative workflows, digital humans, productivity apps and more on AI PCs and workstations. Plug in to NVIDIA AI PC on Facebook, Instagram, TikTok and X — and stay informed by subscribing to the RTX AI PC newsletter. Follow NVIDIA Workstation on LinkedIn and X. See notice regarding software product information.</p>",
    "image": "/images/articles/9148b2c4eac26e32dfb3a31738520ca9.jpg",
    "author": "2.5x. Theriault uses the Blender Cycles app to render out final files. For 3D workflows",
    "date": "2025-06-26",
    "tags": [
      "ai",
      "vision"
    ],
    "status": "Published",
    "originalId": "9148b2c4eac26e32dfb3a31738520ca9",
    "originalUrl": "https://blogs.nvidia.com/blog/rtx-ai-garage-fity-flex-flux-comfyui-stable-diffusion/",
    "source": "NVIDIA Blog",
    "qualityScore": 1,
    "wordCount": 957
  },
  {
    "id": 20,
    "slug": "into-the-omniverse-world-foundation-models-advance-autonomous-vehicle-simulation-and-safety",
    "title": "Into the Omniverse: World Foundation Models Advance Autonomous Vehicle Simulation and Safety",
    "description": "Simulated driving environments enable engineers to safely and efficiently train, test and validate autonomous vehicles (AVs) across countless real-world and edge-case scenarios without the risks and costs of physical testing. WFMs can be used to generate synthetic datasets for enhanced AV simulation.",
    "content": "<p>Share Email0 Editor’s note: This blog is a part of Into the Omniverse, a series focused on how developers, 3D practitioners and enterprises can transform their workflows using the latest advances in OpenUSD and NVIDIA Omniverse. Simulated driving environments enable engineers to safely and efficiently train, test and validate autonomous vehicles (AVs) across countless real-world and edge-case scenarios without the risks and costs of physical testing. These simulated environments can be created through neural reconstruction of real-world data from AV fleets or generated with world foundation models (WFMs) — neural networks that understand physics and real-world properties. WFMs can be used to generate synthetic datasets for enhanced AV simulation. To help physical AI developers build such simulated environments, NVIDIA unveiled major advances in WFMs at the GTC Paris and CVPR conferences earlier this month. These new capabilities enhance NVIDIA Cosmos — a platform of generative WFMs, advanced tokenizers, guardrails and accelerated data processing tools. Key innovations like Cosmos Predict-2, the Cosmos Transfer-1 NVIDIA preview NIM microservice and Cosmos Reason are improving how AV developers generate synthetic data, build realistic simulated environments and validate safety systems at unprecedented scale. Universal Scene Description (OpenUSD), a unified data framework and standard for physical AI applications, enables seamless integration and interoperability of simulation assets across the development pipeline. OpenUSD standardization plays a critical role in ensuring 3D pipelines are built to scale. NVIDIA Omniverse, a platform of application programming interfaces, software development kits and services for building OpenUSD-based physical AI applications, enables simulations from WFMs and neural reconstruction at world scale. Leading AV organizations — including Foretellix, Mcity, Oxa, Parallel Domain, Plus AI and Uber — are among the first to adopt Cosmos models. Foundations for Scalable, Realistic Simulation Cosmos Predict-2, NVIDIA’s latest WFM, generates high-quality synthetic data by predicting future world states from multimodal inputs like text, images and video. This capability is critical for creating temporally consistent, realistic scenarios that accelerate training and validation of AVs and robots. In addition, Cosmos Transfer, a control model that adds variations in weather, lighting and terrain to existing scenarios, will soon be available to 150,000 developers on CARLA, a leading open-source AV simulator. This greatly expands the broad AV developer community’s access to advanced AI-powered simulation tools. Developers can start integrating synthetic data into their own pipelines using the NVIDIA Physical AI Dataset. The latest release includes 40,000 clips generated using Cosmos. Building on these foundations, the Omniverse Blueprint for AV simulation provides a standardized, API-driven workflow for constructing rich digital twins, replaying real-world sensor data and generating new ground-truth data for closed-loop testing. The blueprint taps into OpenUSD’s layer-stacking and composition arcs, which enable developers to collaborate asynchronously and modify scenes nondestructively. This helps create modular, reusable scenario variants to efficiently generate different weather conditions, traffic patterns and edge cases. Driving the Future of AV Safety To bolster the operational safety of AV systems, NVIDIA earlier this year introduced NVIDIA Halos — a comprehensive safety platform that integrates the company’s full automotive hardware and software stack with AI research focused on AV safety. The new Cosmos models — Cosmos Predict- 2, Cosmos Transfer- 1 NIM and Cosmos Reason — deliver further safety enhancements to the Halos platform, enabling developers to create diverse, controllable and realistic scenarios for training and validating AV systems. These models, trained on massive multimodal datasets including driving data, amplify the breadth and depth of simulation, allowing for robust scenario coverage — including rare and safety-critical events — while supporting post-training customization for specialized AV tasks. At CVPR, NVIDIA was recognized as an Autonomous Grand Challenge winner, highlighting its leadership in advancing end-to-end AV workflows. The challenge used OpenUSD’s robust metadata and interoperability to simulate sensor inputs and vehicle trajectories in semi-reactive environments, achieving state-of-the-art results in safety and compliance. Learn more about how developers are leveraging tools like CARLA, Cosmos, and Omniverse to advance AV simulation in this livestream replay: Hear NVIDIA Director of Autonomous Vehicle Research Marco Pavone on the NVIDIA AI Podcast share how digital twins and high-fidelity simulation are improving vehicle testing, accelerating development and reducing real-world risks. Get Plugged Into the World of OpenUSD Learn more about what’s next for AV simulation with OpenUSD by watching the replay of NVIDIA founder and CEO Jensen Huang’s GTC Paris keynote. Looking for more live opportunities to learn more about OpenUSD? Don’t miss sessions and labs happening at SIGGRAPH 2025, August 10–14. Discover why developers and 3D practitioners are using OpenUSD and learn how to optimize 3D workflows with the self-paced “Learn OpenUSD” curriculum for 3D developers and practitioners, available for free through the NVIDIA Deep Learning Institute. Explore the Alliance for OpenUSD forum and the AOUSD website. Stay up to date by subscribing to NVIDIA Omniverse news, joining the community and following NVIDIA Omniverse on Instagram, LinkedIn, Medium and X.</p>",
    "image": "/images/articles/b0fa65ca5f512f6d392769f6dd04b615.jpg",
    "author": "predicting future world states from multimodal inputs like text",
    "date": "2025-06-26",
    "tags": [
      "ai",
      "neural network",
      "dataset",
      "autonomous",
      "edge"
    ],
    "status": "Published",
    "originalId": "b0fa65ca5f512f6d392769f6dd04b615",
    "originalUrl": "https://blogs.nvidia.com/blog/wfm-advance-av-sim-safety/",
    "source": "NVIDIA Blog",
    "qualityScore": 1,
    "wordCount": 802
  },
  {
    "id": 21,
    "slug": "muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search",
    "title": "MUVERA: Making multi-vector retrieval as fast as single-vector search",
    "description": "Given a query from a user (e. , “How tall is Mt Everest.",
    "content": "<p>MUVERA: Making multi-vector retrieval as fast as single-vector search June 25, 2025Rajesh Jayaram and Laxman Dhulipala, Research Scientists, Google Research We introduce MUVERA, a state-of-the-art retrieval algorithm that reduces complex multi-vector retrieval back to single-vector maximum inner product search. Quick links Paper GitHub Share Copy link × Neural embedding models have become a cornerstone of modern information retrieval (IR). Given a query from a user (e.g., “How tall is Mt Everest?”), the goal of IR is to find information relevant to the query from a very large collection of data (e.g., the billions of documents, images, or videos on the Web). Embedding models transform each datapoint into a single-vector “embedding”, such that semantically similar datapoints are transformed into mathematically similar vectors. The embeddings are generally compared via the inner-product similarity, enabling efficient retrieval through optimized maximum inner product search (MIPS) algorithms. However, recent advances, particularly the introduction of multi-vector models like ColBERT, have demonstrated significantly improved performance in IR tasks.Unlike single-vector embeddings, multi-vector models represent each data point with a set of embeddings, and leverage more sophisticated similarity functions that can capture richer relationships between datapoints. For example, the popular Chamfer similarity measure used in state-of-the-art multi-vector models captures when the information in one multi-vector embedding is contained within another multi-vector embedding. While this multi-vector approach boosts accuracy and enables retrieving more relevant documents, it introduces substantial computational challenges. In particular, the increased number of embeddings and the complexity of multi-vector similarity scoring make retrieval significantly more expensive.In “MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings”, we introduce a novel multi-vector retrieval algorithm designed to bridge the efficiency gap between single- and multi-vector retrieval. We transform multi-vector retrieval into a simpler problem by constructing fixed dimensional encodings (FDEs) of queries and documents, which are single vectors whose inner product approximates multi-vector similarity, thus reducing complex multi-vector retrieval back to single-vector maximum inner product search (MIPS). This new approach allows us to leverage the highly-optimized MIPS algorithms to retrieve an initial set of candidates that can then be re-ranked with the exact multi-vector similarity, thereby enabling efficient multi-vector retrieval without sacrificing accuracy. We have provided an open-source implementation of our FDE construction algorithm on GitHub. The challenge of multi-vector retrievalMulti-vector models generate multiple embeddings per query or document, often one embedding per token. One typically calculates the similarity between a query and a document using Chamfer matching, which measures the maximum similarity between each query embedding and the closest document embedding, and then adds these similarities up across all query vectors (the standard method of computing multi-vector similarity). The Chamfer similarity, therefore, provides a \"holistic\" measure of how each part of the query relates to some part of the document.While multi-vector representations offer advantages like improved interpretability and generalization, they pose significant retrieval challenges:Increased embedding volume: Generating embeddings per token drastically increases the number of embeddings to be processed.Complex and compute-intensive similarity scoring: Chamfer matching is a non-linear operation requiring a matrix product, which is more expensive than a single vector dot-product.Lack of efficient sublinear search methods: Single-vector retrieval benefits from highly optimized algorithms (e.g., based on space partitioning) that simultaneously achieve high accuracy and sublinear search times, avoiding exhaustive comparisons. The complex nature of multi-vector similarity prevents the direct application of these fast geometric techniques, hindering efficient retrieval at scale.Unfortunately, traditional single-vector MIPS algorithms cannot be directly applied to multi-vector retrieval — for example, a document might have a token with high similarity to a single query token, but overall, the document might not be very relevant. This problem necessitates more complex and computationally intensive retrieval methods. MUVERA: A solution with fixed dimensional encodingsMUVERA offers an elegant solution by reducing multi-vector similarity search to single-vector MIPS to make retrieval over complex multi-vector data much faster. Imagine you have a large dataset of \"multi-vector sets\" (i.e., sets of vectors) where each set describes some datapoint, but searching through each of these sets is slow. MUVERA's trick is to take that whole group of multi-vectors and squeeze them into a single, easier-to-handle vector that we call a fixed dimensional encoding (FDE). A key part is that if you compare these simplified FDEs, their comparison closely matches what you'd get if you compared the original, more complex multi-vector sets. This lets us use much quicker search methods designed for single vectors.Here's a simplified breakdown of how MUVERA works:FDE generation: MUVERA employs mappings to convert query and document multi-vector sets into FDEs. These mappings are designed to capture the essential similarity information in a fixed-length vector.MIPS-based retrieval: The FDEs of documents are indexed using a standard MIPS solver. Given a query, its FDE is computed, and the MIPS solver efficiently retrieves the most similar document FDEs.Re-ranking: The initial candidates retrieved by MIPS are re-ranked using the original Chamfer similarity for improved accuracy.A key advantage of MUVERA is that the FDE transformation is data-oblivious. This means it doesn't depend on the specific dataset, making it both robust to changes in data distribution and suitable for streaming applications. Additionally, unlike single-vectors produced by a model, FDE’s are guaranteed to approximate the true Chamfer similarity to within a specified error. Thus, after the re-ranking stage, MUVERA is guaranteed to find the most similar multi-vector representations. play silent looping video pause silent looping video Illustration of the construction of query FDE's. Each token (shown as a word in this example) is mapped to a high-dimensional vector (2-D in the example for simplicity). The high-dimensional space is randomly partitioned by hyperplane cuts. Each piece of space is assigned a block of coordinates in the output FDE, which is set to the sum of the coordinates of the query vectors that land in that piece. play silent looping video pause silent looping video Illustration of the construction of document FDE's. The construction is the same as the query construction, except that the vectors falling in a given piece of the partitioned space are averaged together instead of summed, which accurately captures the asymmetric nature of the Chamfer similarity. Theoretical foundationsOur approach is inspired by techniques used in probabilistic tree embeddings, a powerful tool in the theory of geometric algorithms. However, we adapt these techniques to work with inner products and Chamfer similarity.The core idea behind FDE generation is to partition the embedding space into sections (illustrated in the figure above). If similar vectors from a query and a document fall into the same section, we can approximate their similarity efficiently. However, since we don't know the optimal matching between query and document vectors beforehand, we use a randomized partitioning scheme.We also provide theoretical guarantees for MUVERA, proving that FDEs offer a strong approximation of Chamfer similarity (you can read more in the paper). This is a significant result, as it provides a principled way to perform multi-vector retrieval using single-vector proxies with provable accuracy. Experimental resultsWe evaluated MUVERA on several information retrieval datasets from the BEIR benchmarks. Our experiments demonstrate that MUVERA consistently achieves high retrieval accuracy with significantly reduced latency compared to the previous state-of-the-art method known as PLAID.Our key findings include:Improved recall: MUVERA outperforms the single-vector heuristic, a common approach used in multi-vector retrieval (which PLAID also employs), achieving better recall while retrieving significantly fewer candidate documents (shown in the figure below). For instance, FDE’s retrieve 5–20x fewer candidates to achieve a fixed recall. Recall of fixed dimensional encodings (FDE) of varying dimensions vs. a single-vector heuristic (SV). Note 10240-dimensional FDE’s have nearly the same representation size as the original MV representation (used in SV heuristic), while requiring significantly fewer comparisons in the search (true even for 20k-dimensional FDE’s). Reduced latency: Compared to PLAID, a highly optimized multi-vector retrieval system based on the single-vector heuristic, MUVERA achieves an average of 10% higher recall with a remarkable 90% reduction in latency across the BEIR datasets (shown in the figure below). MUVERA vs. PLAID over BEIR benchmarks. Moreover, we found that MUVERA's FDEs can be effectively compressed using product quantization, reducing memory footprint by 32x with minimal impact on retrieval quality.These results highlight MUVERA's potential to significantly accelerate multi-vector retrieval, making it more practical for real-world applications. ConclusionWe have presented MUVERA, a novel and efficient multi-vector retrieval algorithm with provable guarantees on its approximation quality and good practical performance. By reducing multi-vector search to single-vector MIPS, MUVERA leverages existing optimized search techniques and achieves state-of-the-art performance with significantly improved efficiency. Interested readers can find an open-source implementation of our FDE construction algorithm on GitHub.Our work opens up new avenues for efficient multi-vector retrieval, which is crucial for various applications, including search engines, recommendation systems, and natural language processing. We believe that further research and optimization of MUVERA will lead to even greater performance gains and broader adoption of multi-vector retrieval techniques. AcknowledgementsThe work summarized in this blog post was done in collaboration with Majid Hadian, Jason Lee, and Vahab Mirrokni. Lastly, we thank Kimberly Schwede for their valuable help with making the animation in this blog post. Labels: Algorithms & Theory Machine Intelligence Quick links Paper GitHub Share Copy link × Other posts of interest July 10, 2025 Graph foundation models for relational data Algorithms & Theory · Machine Intelligence July 9, 2025 MedGemma: Our most capable open models for health AI development Generative AI · Health & Bioscience · Machine Intelligence June 30, 2025 How we created HOV-specific ETAs in Google Maps Algorithms & Theory · Data Mining & Modeling · Machine Intelligence</p>",
    "image": "/images/articles/74ddde2276c188b97c8e8687e7882708.png",
    "author": "constructing fixed dimensional encodings (FDEs) of queries and documents",
    "date": "2025-06-25",
    "tags": [
      "algorithm",
      "bert",
      "research",
      "image",
      "video"
    ],
    "status": "Published",
    "originalId": "74ddde2276c188b97c8e8687e7882708",
    "originalUrl": "https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/",
    "source": "Google Research Blog",
    "qualityScore": 0.9,
    "wordCount": 1563
  },
  {
    "id": 22,
    "slug": "from-research-to-climate-resilience",
    "title": "From research to climate resilience",
    "description": "Our research impacts products, businesses, scientific discovery, and society. Today, the opportunity to help solve seemingly impossible problems with breakthrough research is greater than ever, as is the opportunity to translate this research into real-world impact.",
    "content": "<p>From research to climate resilience June 24, 2025Yossi Matias, Vice President & Head of Google Research Google Research is driving AI breakthroughs to help communities bolster their resilience to climate-related threats. At Google Research, we're driven by exploring the art of the possible. Our research impacts products, businesses, scientific discovery, and society. Today, the opportunity to help solve seemingly impossible problems with breakthrough research is greater than ever, as is the opportunity to translate this research into real-world impact.I'm excited to share how we're advancing research and harnessing tech innovation to help build more resilience, tackling the urgent challenges of climate crises, such as wildfires, floods, extreme weather, and cyclones. Can we deliver timely, reliable predictions of these threats, helping people stay safe and communities build resilience?Our work with AI over the past years, advancing climate science and tackling these difficult problems, is already making a tangible difference. It clearly demonstrates AI’s potential to build towards better climate resilience. From impossible to global scale: AI-powered flood forecasting A few years ago, providing accurate, reliable flood predictions was widely considered an impossible challenge. Today, Google Research’s groundbreaking global hydrological AI model, published in Nature, enables us to accurately forecast riverine floods around the world up to seven days in advance. Our forecasts are available on Google’s Flood Hub platform and cover over 700 million people across more than 100 countries worldwide, empowering governments and local communities to protect lives and livelihoods.We’re also providing researchers and experts with expanded coverage via an API as well as an expert data layer on Flood Hub. We’ve employed AI to analyze historical data and create “virtual gauges” for locations where data is scarce and physical gauges are not available. This significantly expands our coverage to data-scarce regions across 150 countries, addressing the need, raised by our partners, for greater information in these regions, which are particularly vulnerable. We’re also working with the WMO and meteorological services in other countries, including the Czech Republic, Nigeria, Uruguay, and Vietnam, on efforts to scale flood forecasting globally, further enhancing community resilience worldwide. Watch the film Link to Youtube Video Improving the lead time and accuracy of cyclone forecasts Cyclones — also known as typhoons or hurricanes — devastate communities and endanger lives. In the last 50 years, they have caused 1.4 trillion dollars in economic losses. Advanced warning could make a tremendous difference, but it is difficult to predict whether oceanic storms will become dangerous cyclones, where they might make landfall, and what the impact will be. For decades, scientists have relied on supercomputers to simulate the laws of physics to estimate the future path, intensity and size of these storms. Now Google DeepMind and Google Research teams are exploring AI’s potential to improve how we predict and prepare for cyclones. We can currently predict existence, track, intensity, size, and structure, generating up to 50 possible scenarios as far as 15 days in advance. It’s still early days and we're working with experts around the world, including partners in academia, government agencies, and non-profit organizations, to refine and expand the impact of our work. Earlier this month we announced Weather Lab, an interactive website where we are sharing ongoing research, making our newest and most accurate weather models available to experts and the public. We also announced a partnership with the US National Hurricane Center, who will be leveraging these experimental models this summer throughout the Atlantic Hurricane Season. We hope this data can help improve NHC forecasts and provide earlier and more accurate warnings for hazards linked to tropical cyclones. Nowcasting: Improving access to reliable, real-time weather information We’re applying AI to improve forecasts for everyday weather, too. This addresses an acute challenge in regions with limited traditional infrastructure, including across Africa, where reliable forecasting is often scarce. We’re generating hyper-local, short-term weather predictions — known as nowcasting — and can now make available these global precipitation predictions with a 5km resolution, updated every 15 minutes, up to 12 hours ahead. This transformative capability stems from MetNet-3, our state-of-the-art AI neural weather model. To scale nowcasting globally, Google Research teams are leveraging globally available satellite observations in our models, allowing us to bring the power of AI-driven weather forecasts directly to people across Africa via Google Search. This innovative approach overcomes the critical gap where traditional weather forecasting infrastructure, like ground-based radar, is scarce. For users across Africa, this translates to more reliable, real-time weather information that improves their daily lives. The agricultural community tells us that nowcasting and other weather models can help farmers react to changing conditions, potentially improving yields, reducing waste, lowering operating costs, and enhancing economic resilience.We are collaborating with leading experts, including meteorologists at the University of Leeds and the UK Met office, exploring how they can use our models for various applications. Such partnerships with local scientific communities are crucial not only to continue to refine the accuracy of our forecasts, but also to advance the science of meteorology and improve weather predictions for everyone, everywhere. Leveraging AI to understand and mitigate the growing threat of wildfires Wildfires are increasing in frequency and intensity, putting lives, homes, and ecosystems at risk. For years we have been leveraging satellite imagery and AI to detect the boundaries of wildfires in near real time. We share this vital information with first responders and affected communities, enabling quicker and more effective responses. Wildfire boundary information is available on Google Search and Maps in 27 countries. Watch the film Link to Youtube Video During this ongoing work, we recognized the need for more accurate information on earlier, smaller fires that can rapidly escalate. We set out to fundamentally change how we detect wildfires with a dedicated satellite constellation, FireSat. In March, the first FireSat satellite was launched, representing a major leap forward in our ability to mitigate the threat of wildfires worldwide. FireSat consists of satellites that can detect and track wildfires as small as a 5x5 meter classroom, a significant improvement over current technology that can only spot fires the size of a football field. A constellation of 50 satellites will provide high-resolution imagery updated globally every 20 minutes, giving fire authorities time for quick action. Additionally, FireSat’s unprecedented global view of fires will allow scientists to better study fire propagation. This leap is the result of a partnership between Google Research and Google.org with the Earth Fire Alliance, Muon Space, the Moore Foundation, wildfire authorities, and many others. Geospatial Reasoning: Pioneering planetary insights Beyond prediction and forecasting, we’re pioneering new frontiers by enabling insights about our planet with Geospatial Reasoning. Our long-standing investment in applying AI to geospatial data is evident in our extensive portfolio of work — from AI models that power weather forecasts and help predict floods and wildfires to remote sensing and analyzing population dynamics. The Geospatial Reasoning framework brings Earth models together with generative AI to accelerate geospatial problem solving. It enables people to ask questions in natural language about, for example, which vulnerable communities should be evacuated first before a natural disaster and to receive comprehensive answers and visualizations that are grounded in robust geospatial data. This can help provide critical insights to governments, local agencies and businesses, creating new opportunities for building community resilience. Watch the film Link to Youtube Video We’re removing cost and expertise barriers by making sophisticated planetary analysis on Google Cloud Platform more accessible, and empowering experts and developers to integrate Google’s advanced geospatial Earth models with their data to unlock powerful insights. Since the launch of Geospatial Reasoning in April, we've seen interest from a wide range of sectors eager to harness its potential, from public health to climate resilience to commercial applications, such as insurance. We are now collaborating with partners like Maxar, Sust-Global, Airbus, Planet Labs and Carto to translate the potential into tangible solutions. Using AI to reduce transport-related emissions and improve air quality AI is paving the way for making air and ground transportation more sustainable, with reduced carbon emission and improved air quality in our cities. One third of aviation’s climate impact are the result of contrails, the condensation trails sometimes seen behind airplanes. In early real-world demonstrations with partners like American Airlines, our AI-powered forecasts helped pilots reduce contrails by 54%. We’re now making these AI-powered forecasts available across the aviation industry through our Contrails API and working with partners like EUROCONTROL to integrate the forecasts into their flight planning systems. Google.org is providing 3 million dollars to co-catalyze a new nonprofit with Breakthrough Energy, called contrails.org, which will focus on translating research into tangible tools for aviation. For ground transportation, we’re teaming up with cities to improve traffic flow. Project Green Light uses AI and Google Maps driving trends to propose adjustment to traffic light timing in order to reduce vehicle gas emissions, also resulting in improved air quality. Green Light has demonstrated potential to reduce stops at intersections by up to 30% and reduce emissions at intersections by an average of over 10%. Conclusion Our breakthrough research over the past years is already having a profound impact on natural disaster preparedness. Yet, this is just the beginning of what’s possible. We are confident that advancing AI and scientific research can play a key role in addressing the difficult problems of timely, reliable global predictions, working towards better climate resilience. I am incredibly optimistic that collectively we’ll ultimately get closer to a point where no one is surprised by a natural disaster coming their way. Labels: Climate & Sustainability General Science Generative AI Other posts of interest July 9, 2025 MedGemma: Our most capable open models for health AI development Generative AI · Health & Bioscience · Machine Intelligence June 27, 2025 REGEN: Empowering personalized recommendations with natural language Data Mining & Modeling · General Science · Machine Intelligence June 23, 2025 Unlocking rich genetic insights through multimodal AI with M-REGLE Generative AI · Health & Bioscience · Machine Intelligence · Open Source Models & Datasets</p>",
    "image": "/images/articles/7dd0fd0aba977d3a0bd8b5867c52b01d.png",
    "author": "exploring the art of the possible. Our research impacts products",
    "date": "2025-06-24",
    "tags": [
      "ai",
      "research"
    ],
    "status": "Published",
    "originalId": "7dd0fd0aba977d3a0bd8b5867c52b01d",
    "originalUrl": "https://research.google/blog/how-ai-is-helping-us-build-a-more-resilient-planet/",
    "source": "Google Research Blog",
    "qualityScore": 1,
    "wordCount": 1660
  },
  {
    "id": 23,
    "slug": "were-expanding-our-gemini-25-family-of-models",
    "title": "We’re expanding our Gemini 2.5 family of models",
    "description": "We’re expanding our Gemini 2. 5 family of models Jun 17, 2025 · Share Twitter Facebook LinkedIn Mail Copy link Gemini 2.",
    "content": "<p>We’re expanding our Gemini 2.5 family of models Jun 17, 2025 · Share Twitter Facebook LinkedIn Mail Copy link Gemini 2.5 Flash and Pro are now generally available, and we’re introducing 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet. Tulsee Doshi Senior Director, Product Management, on behalf of the Gemini team Share Twitter Facebook LinkedIn Mail Copy link We designed Gemini 2.5 to be a family of hybrid reasoning models that provide amazing performance, while also being at the Pareto Frontier of cost and speed. Today, we’re taking the next step with our 2.5 Pro and Flash models by releasing them as stable and generally available. And we’re bringing you 2.5 Flash-Lite in preview — our most cost-efficient and fastest 2.5 model yet.Making 2.5 Flash and 2.5 Pro generally availableThanks to all of your feedback, today we’re releasing stable versions of 2.5 Flash and Pro, so you can build production applications with confidence. Developers like Spline and Rooms and organizations like Snap and SmartBear have already been using the latest versions in-production for the last few weeks.Introducing Gemini 2.5 Flash-LiteWe’re also introducing a preview of the new Gemini 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet. You can start building with the preview version now, and we’re looking forward to your feedback.2.5 Flash-Lite has all-around higher quality than 2.0 Flash-Lite on coding, math, science, reasoning and multimodal benchmarks. It excels at high-volume, latency-sensitive tasks like translation and classification, with lower latency than 2.0 Flash-Lite and 2.0 Flash on a broad sample of prompts. It comes with the same capabilities that make Gemini 2.5 helpful, including the ability to turn thinking on at different budgets, connecting to tools like Google Search and code execution, multimodal input, and a 1 million-token context length.See more details about our 2.5 family of models in the latest Gemini technical report. The preview of Gemini 2.5 Flash-Lite is now available in Google AI Studio and Vertex AI, alongside the stable versions of 2.5 Flash and Pro. Both 2.5 Flash and Pro are also accessible in the Gemini app. We’ve also brought custom versions of 2.5 Flash-Lite and Flash to Search.We can’t wait to see what you continue to build with Gemini 2.5. POSTED IN:</p>",
    "image": "/images/articles/110cde3a95a297375278bde58a919b6e.jpg",
    "author": "releasing them as stable and generally available. And we’re bringing you 2.5 Flash-Lite in preview — our most cost-efficient and fastest 2.5 model yet.Making 2.5 Flash and 2.5 Pro generally availableThanks to all of your feedback",
    "date": "2025-06-17",
    "tags": [
      "ai",
      "news"
    ],
    "status": "Published",
    "originalId": "110cde3a95a297375278bde58a919b6e",
    "originalUrl": "https://deepmind.google/discover/blog/were-expanding-our-gemini-25-family-of-models/",
    "source": "DeepMind Blog",
    "qualityScore": 1,
    "wordCount": 372
  }
]